{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2af0b51",
   "metadata": {},
   "source": [
    "# CV in Transformer\n",
    "- **Learner** : shenhao\n",
    "- **Date** : 2021.10.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9fa6de-50cd-4ffb-9bc9-9b4e7878bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94bcdfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28cadf",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "与其他seq2seq模型类似，我们使用学习到的embedding将输入token和输出token转换为$d_{\\text{model}}$维的向量。我们还使用普通的线性变换和softmax函数将解码器输出转换为预测的下一个token的概率 在我们的模型中，两个嵌入层之间和pre-softmax线性变换共享相同的权重矩阵，类似于[(cite)](https://arxiv.org/abs/1608.05859)。在embedding层中，我们将这些权重乘以$\\sqrt{d_{\\text{model}}}$。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a27198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        类的初始化函数\n",
    "        d_model：指词嵌入的维度\n",
    "        vocab:指词表的大小\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        # 之后就是调用nn中的预定义层Embedding，获得一个词嵌入对象self.lut\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        # 最后就是将d_model传入类中\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Embedding层的前向传播逻辑\n",
    "        参数x：这里代表输入给模型的单词文本通过词表映射后的one-hot向量\n",
    "        将x传给self.lut并与根号下self.d_model相乘作为结果返回\n",
    "        \"\"\"\n",
    "        embedds = self.lut(x)\n",
    "        return embedds * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c52f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings(\n",
      "  (lut): Embedding(10, 16)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# embedding_size=16, input_vocab_size=10\n",
    "embedding_fc = Embeddings(16, 10).to(device)\n",
    "print(embedding_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18996f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4, 6, 5, 1, 2]]), torch.Size([1, 5]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence or batch_size=1,words=5\n",
    "input_X = torch.randint(0, 10, (1, 5))\n",
    "input_X, input_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc1c0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_x = embedding_fc(input_X)\n",
    "embeds_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb7b04",
   "metadata": {},
   "source": [
    "## Positional Encodding\n",
    "Positional Encodding位置编码的作用是为模型提供当前时间步的前后出现顺序的信息.因为Transformer不像RNN那样的循环结构有前后不同时间步输入间天然的先后顺序,所有的时间步是同时输入,并行推理的,因此在时间步的特征中融合进位置编码的信息是合理的.\n",
    "\n",
    "位置编码可以有很多选择,可以是固定的,也可以设置成可学习的参数.\n",
    "\n",
    "这里,我们使用固定的位置编码.具体地,使用不同频率的sin和cos函数来进行位置编码,如下所示:\n",
    "$$PE_{pos,2i}=sin(pos/10000^{2i/d_{model}})$$  \n",
    "$$PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}})$$  \n",
    "\n",
    "其中pos代表时间步的下标索引,向量$PE_{pos}$也就是第pos个时间步的位置编码,编码长度同Embedding层."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b346024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        \"\"\"\n",
    "        位置编码器类的初始化函数\n",
    "\n",
    "        共有三个参数，分别是\n",
    "        d_model：词嵌入维度\n",
    "        dropout: dropout触发比率\n",
    "        max_len：每个句子的最大长度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings\n",
    "        # 注意下面代码的计算方式与公式中给出的是不同的，但是是等价的，你可以尝试简单推导证明一下。\n",
    "        # 这样计算是为了避免中间的数值计算结果超出float的范围，\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04a6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size=15,dropout=0.1\n",
    "position_encoding_layer = PositionalEncoding(16, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47988e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_enc_x = position_encoding_layer(embeds_x)\n",
    "position_enc_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4d1a1",
   "metadata": {},
   "source": [
    "## 掩码及其作用\n",
    "掩码：掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有0和1；代表位置被遮掩或者不被遮掩。\n",
    "\n",
    "掩码的作用：在transformer中，掩码主要的作用有两个，一个是屏蔽掉无效的padding区域，一个是屏蔽掉来自“未来”的信息。Encoder中的掩码主要是起到第一个作用，Decoder中的掩码则同时发挥着两种作用。\n",
    "\n",
    "屏蔽掉无效的padding区域：我们训练需要组batch进行，就以机器翻译任务为例，一个batch中不同样本的输入长度很可能是不一样的，此时我们要设置一个最大句子长度，然后对空白区域进行padding填充，而填充的区域无论在Encoder还是Decoder的计算中都是没有意义的，因此需要用mask进行标识，屏蔽掉对应区域的响应。\n",
    "\n",
    "屏蔽掉来自未来的信息：我们已经学习了attention的计算流程，它是会综合所有时间步的计算的，那么在解码的时候，就有可能获取到未来的信息，这是不行的。因此，这种情况也需要我们使用mask进行屏蔽。现在还没介绍到Decoder，如果没完全理解，可以之后再回过头来思考下。\n",
    "\n",
    "mask的构造代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fdbb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    # 生成向后遮掩的掩码张量，参数size是掩码张量最后两个维度的大小，它最后两维形成一个方阵\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "\n",
    "    # 然后使用np.ones方法向这个形状中添加1元素，形成上三角阵\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "\n",
    "    # 最后将numpy类型转化为torch中的tensor，内部做一个1- 的操作。这个其实是做了一个三角阵的反转，subsequent_mask中的每个元素都会被1减。\n",
    "    # 如果是0，subsequent_mask中的该位置由0变成1\n",
    "    # 如果是1，subsequect_mask中的该位置由1变成0\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f0d191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_shape = (1, 10, 10)\n",
    "# np.triu(m, k=0)\n",
    "# k是指从主对角线开始保留\n",
    "# k=0\n",
    "# [[1. 1. 1. 1. 1.]\n",
    "#  [0. 1. 1. 1. 1.]\n",
    "#  [0. 0. 1. 1. 1.]\n",
    "#  [0. 0. 0. 1. 1.]\n",
    "#  [0. 0. 0. 0. 1.]]\n",
    "# k=1\n",
    "# [[0. 1. 1. 1. 1.]\n",
    "#  [0. 0. 1. 1. 1.]\n",
    "#  [0. 0. 0. 1. 1.]\n",
    "#  [0. 0. 0. 0. 1.]\n",
    "#  [0. 0. 0. 0. 0.]]\n",
    "# k=2\n",
    "# [[0. 0. 1. 1. 1.]\n",
    "#  [0. 0. 0. 1. 1.]\n",
    "#  [0. 0. 0. 0. 1.]\n",
    "#  [0. 0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0. 0.]]\n",
    "subseq_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "subseq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94cf9f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(subseq_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28f4ee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x205ceb0c2e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7394f",
   "metadata": {},
   "source": [
    "## 规范化层\n",
    "规范化层的作用：它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后输出可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常慢。因此都会在一定层后接规范化层进行数值的规范化，使其特征数值在合理范围内。\n",
    "\n",
    "Transformer中使用的normalization手段是layer norm，实现代码很简单，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f916b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, feature_size, eps=1e-6):\n",
    "        # 初始化函数有两个参数，一个是features,表示词嵌入的维度,另一个是eps它是一个足够小的数，在规范化公式的分母中出现,防止分母为0，默认是1e-6。\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 根据features的形状初始化两个参数张量a2，和b2，第一初始化为1张量，也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是0，这两个张量就是规范化层的参数。因为直接对上一层得到的结果做规范化公式计算，将改变结果的正常表征，因此就需要有参数作为调节因子，使其即能满足规范化要求，又能不改变针对目标的表征，最后使用nn.parameter封装，代表他们是模型的参数\n",
    "        self.a_2 = nn.Parameter(torch.ones(feature_size))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(feature_size))\n",
    "        # 把eps传到类中\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入参数x代表来自上一层的输出，在函数中，首先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致，接着再求最后一个维度的标准差，然后就是根据规范化公式，用x减去均值除以标准差获得规范化的结果。\n",
    "        # 最后对结果乘以我们的缩放参数，即a2,*号代表同型点乘，即对应位置进行乘法操作，加上位移参b2，返回即可\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc773004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0000e+00, -3.6160e+00, -3.0979e-01, -0.0000e+00, -6.0106e+00,\n",
       "           -7.8812e+00,  1.1963e+01,  3.3401e-01, -5.2540e+00, -3.0309e+00,\n",
       "            4.7534e+00,  8.1180e-01,  6.2579e-01,  7.4930e+00,  4.5049e+00,\n",
       "            1.8610e+00],\n",
       "          [-5.2908e-01, -0.0000e+00, -2.6326e-01, -1.1667e+00, -4.5975e+00,\n",
       "           -7.8027e+00, -1.0195e-02,  2.9557e+00,  5.6879e-01,  4.5661e+00,\n",
       "           -1.3931e+00, -3.0531e+00,  2.2437e+00,  2.2772e+00,  2.5196e+00,\n",
       "           -2.9457e-01],\n",
       "          [-1.9345e+00, -6.6504e+00, -4.5080e-01, -7.0320e-01,  1.5061e+00,\n",
       "            1.0495e+01, -0.0000e+00,  1.7928e+00,  2.1457e+00, -0.0000e+00,\n",
       "           -0.0000e+00, -7.5229e+00, -2.6902e+00,  1.7529e+00, -2.2068e+00,\n",
       "           -2.5872e+00],\n",
       "          [-1.1563e+00, -7.1341e+00,  3.6549e+00,  1.6422e+00, -8.7020e+00,\n",
       "            4.2414e+00,  2.2792e-01, -4.1304e+00, -2.0449e-01, -1.1205e+00,\n",
       "            1.9175e-01,  1.4125e+00,  1.6505e+00,  3.3348e+00,  5.8560e+00,\n",
       "           -2.7107e+00],\n",
       "          [-6.3058e-01, -0.0000e+00, -6.4573e-01, -3.2826e+00,  8.2511e+00,\n",
       "           -6.4506e-01, -6.7896e-01,  5.1568e+00, -2.6388e+00,  0.0000e+00,\n",
       "           -3.2014e+00,  0.0000e+00, -1.5666e+00, -2.1916e+00,  0.0000e+00,\n",
       "            1.7595e+00]]], grad_fn=<MulBackward0>),\n",
       " tensor([[[-7.6292e-02, -7.8315e-01, -1.3685e-01, -7.6292e-02, -1.2513e+00,\n",
       "           -1.6169e+00,  2.2622e+00, -1.0999e-02, -1.1033e+00, -6.6877e-01,\n",
       "            8.5291e-01,  8.2399e-02,  4.6039e-02,  1.3885e+00,  8.0433e-01,\n",
       "            2.8750e-01],\n",
       "          [-9.1594e-02,  8.1241e-02, -4.7592e-03, -2.9987e-01, -1.4206e+00,\n",
       "           -2.4676e+00,  7.7910e-02,  1.0468e+00,  2.6705e-01,  1.5728e+00,\n",
       "           -3.7384e-01, -9.1610e-01,  8.1417e-01,  8.2512e-01,  9.0432e-01,\n",
       "           -1.4987e-02],\n",
       "          [-3.7014e-01, -1.5388e+00, -2.4632e-03, -6.5011e-02,  4.8250e-01,\n",
       "            2.7100e+00,  1.0925e-01,  5.5353e-01,  6.4100e-01,  1.0925e-01,\n",
       "            1.0925e-01, -1.7550e+00, -5.5743e-01,  5.4364e-01, -4.3763e-01,\n",
       "           -5.3188e-01],\n",
       "          [-2.4436e-01, -1.7470e+00,  9.6504e-01,  4.5909e-01, -2.1411e+00,\n",
       "            1.1125e+00,  1.0358e-01, -9.9197e-01, -5.1123e-03, -2.3538e-01,\n",
       "            9.4490e-02,  4.0134e-01,  4.6118e-01,  8.8458e-01,  1.5183e+00,\n",
       "           -6.3511e-01],\n",
       "          [-2.0388e-01,  6.5466e-03, -2.0894e-01, -1.0889e+00,  2.7600e+00,\n",
       "           -2.0871e-01, -2.2002e-01,  1.7274e+00, -8.7403e-01,  6.5466e-03,\n",
       "           -1.0618e+00,  6.5466e-03, -5.1622e-01, -7.2481e-01,  6.5466e-03,\n",
       "            5.9370e-01]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_size=16\n",
    "layer_norm = LayerNorm(16)\n",
    "norm_x = layer_norm(position_enc_x)\n",
    "position_enc_x, norm_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcba998",
   "metadata": {},
   "source": [
    "## Attention\n",
    "Attention功能可以描述为将query和一组key-value映射到输出，其中query、key、value和输出都是向量。输出为value的加权和，其中每个value的权重通过query与相应key的计算得到。                                                                         \n",
    "我们将particular attention称之为“缩放的点积Attention”(Scaled Dot-Product Attention\")。其输入为query、key(维度是$d_k$)以及values(维度是$d_v$)。我们计算query和所有key的点积，然后对每个除以 $\\sqrt{d_k}$, 最后用softmax函数获得value的权重。         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3242872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    # query,key,value均为[batch_size,sentence_len,embedding_size]\n",
    "    # 首先取query的最后一维的大小，对应词嵌入维度\n",
    "    d_k = query.size(-1)\n",
    "    # 按照注意力公式，将query与key的转置相乘，这里面key是将最后两个维度进行转置，再除以缩放系数得到注意力得分张量scores\n",
    "    # src:scores.shape=[30,8,10,10] tgt:[30,8,9,9]\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # 接着判断是否使用掩码张量\n",
    "    if mask is not None:\n",
    "        # 使用tensor的masked_fill方法，将掩码张量和scores张量每个位置一一比较，如果掩码张量则对应的scores张量用-1e9这个置来替换\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # 对scores的最后一维进行softmax操作，使用F.softmax方法，这样获得最终的注意力张量\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 之后判断是否使用dropout进行随机置0\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    # 最后，根据公式将p_attn与value张量相乘获得最终的query注意力表示，同时返回注意力张量\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a66c648f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_enc_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "313a8729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 16]), torch.Size([1, 5, 5]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query的最后一维的大小为embedding_size,理论上q,k,v通过input_x线性转换后得来的.\n",
    "query, key, value = [torch.randn((1, 5, 16)) for i in range(3)]\n",
    "output_x = attention(query,key,value)\n",
    "output_x[0].shape,output_x[1].shape\n",
    "# p_attn\n",
    "# [[a11,a12,a13,a14,a15],\n",
    "#  [a21,a22,a23,a24,a25],\n",
    "#  [a31,a32,a33,a34,a35],\n",
    "#  [a41,a42,a43,a44,a45],\n",
    "#  [a51,a52,a53,a54,a55]]\n",
    "# 其中a(i,j)表示第i个单词对第j个单词的注意力程度."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43dd4e",
   "metadata": {},
   "source": [
    "## 多头注意力机制\n",
    "\n",
    "Transformer 的论文通过增加多头注意力机制（一组注意力称为一个 attention head），进一步完善了Self-Attention。这种机制从如下两个方面增强了attention层的能力：\n",
    "\n",
    "- **它扩展了模型关注不同位置的能力**。在上面的例子中，第一个位置的输出 $z_1$ 包含了句子中其他每个位置的很小一部分信息，但$z_1$​仅仅是单个向量，所以可能仅由第1个位置的信息主导了。而当我们翻译句子：`The animal didn’t cross the street because it was too tired`时，我们不仅希望模型关注到\"it\"本身，还希望模型关注到\"The\"和\"animal\"，甚至关注到\"tired\"。这时，多头注意力机制会有帮助。\n",
    "- **多头注意力机制赋予attention层多个“子表示空间”**。下面我们会看到，多头注意力机制会有多组 $W^Q, W^K W^V$ 的权重矩阵（在 Transformer 的论文中，使用了 8 组注意力),，因此可以将 $X$ 变换到更多种子空间进行表示。接下来我们也使用8组注意力头（attention heads））。每一组注意力的权重矩阵都是随机初始化的，但经过训练之后，每一组注意力的权重 $W^Q, W^K W^V$ 可以把输入的向量映射到一个对应的\"子表示空间\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36078435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12, 300])\n"
     ]
    }
   ],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    # n_heads：多头注意力的数量\n",
    "    # hid_dim：每个词输出的向量维度\n",
    "    def __init__(self, hid_dim, n_heads, dropout):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 强制 hid_dim 必须整除 h\n",
    "        assert hid_dim % n_heads == 0\n",
    "        # 定义 W_q 矩阵\n",
    "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
    "        # 定义 W_k 矩阵\n",
    "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
    "        # 定义 W_v 矩阵\n",
    "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        # 缩放\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # 注意 Q，K，V的在句子长度这一个维度的数值可以一样，可以不一样。\n",
    "        # K: [64,10,300], 假设batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维\n",
    "        # V: [64,10,300], 假设batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维\n",
    "        # Q: [64,12,300], 假设batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维\n",
    "        bsz = query.shape[0]\n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "        # 这里把 K Q V 矩阵拆分为多组注意力\n",
    "        # 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50\n",
    "        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度\n",
    "        # K: [64,10,300] 拆分多组注意力 -> [64,10,6,50] 转置得到 -> [64,6,10,50]\n",
    "        # V: [64,10,300] 拆分多组注意力 -> [64,10,6,50] 转置得到 -> [64,6,10,50]\n",
    "        # Q: [64,12,300] 拆分多组注意力 -> [64,12,6,50] 转置得到 -> [64,6,12,50]\n",
    "        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算\n",
    "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        K = K.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        V = V.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 第 1 步：Q 乘以 K的转置，除以scale\n",
    "        # [64,6,12,50] * [64,6,50,10] = [64,6,12,10]\n",
    "        # attention：[64,6,12,10]\n",
    "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # 如果 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10，这里用“0”来指示哪些位置的词向量不能被attention到，比如padding位置，当然也可以用“1”或者其他数字来指示，主要设计下面2行代码的改动。\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。\n",
    "        # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax\n",
    "        # attention: [64,6,12,10]\n",
    "        attention = self.do(torch.softmax(attention, dim=-1))\n",
    "\n",
    "        # 第三步，attention结果与V相乘，得到多头注意力的结果\n",
    "        # [64,6,12,10] * [64,6,10,50] = [64,6,12,50]\n",
    "        # x: [64,6,12,50]\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # 因为 query 有 12 个词，所以把 12 放到前面，把 50 和 6 放到后面，方便下面拼接多组的结果\n",
    "        # x: [64,6,12,50] 转置-> [64,12,6,50]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # 这里的矩阵转换就是：把多组注意力的结果拼接起来\n",
    "        # 最终结果就是 [64,12,300]\n",
    "        # x: [64,12,6,50] -> [64,12,300]\n",
    "        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维\n",
    "query = torch.rand(64, 12, 300)\n",
    "# batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维\n",
    "key = torch.rand(64, 12, 300)\n",
    "# batch_size 为 64，有 12 个词，每个词的 Value 向量是 300 维\n",
    "value = torch.rand(64, 12, 300)\n",
    "attention_fn = MultiheadAttention(hid_dim=300, n_heads=6, dropout=0.1)\n",
    "output = attention_fn(query, key, value)\n",
    "## output: torch.Size([64, 12, 300])\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121fc2e",
   "metadata": {},
   "source": [
    "### 简化代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "558fad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个clones函数，来更方便的将某个结构复制若干份\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7df55e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        # 在类的初始化时，会传入三个参数，h代表头数，d_model代表词嵌入的维度，dropout代表进行dropout操作时置0比率，默认是0.1\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # 在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，这是因为我们之后要给每个头分配等量的词特征，也就是embedding_dim/head个\n",
    "        assert d_model % h == 0\n",
    "        # 得到每个头获得的分割词向量维度d_k\n",
    "        self.d_k = d_model // h\n",
    "        # 传入头数h\n",
    "        self.h = h\n",
    "\n",
    "        # 创建linear层，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用，为什么是四个呢，这是因为在多头注意力中，Q,K,V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        # self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # 前向逻辑函数，它输入参数有四个，前三个就是注意力机制需要的Q,K,V，最后一个是注意力机制中可能需要的mask掩码张量，默认是None\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            # 使用unsqueeze扩展维度，代表多头中的第n头\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # 接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        # 首先利用zip将输入QKV与三个线性层组到一起，然后利用for循环，将输入QKV分别传到线性层中，做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结构进行维度重塑，多加了一个维度h代表头，这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，计算机会根据这种变换自动计算这里的值，然后对第二维和第三维进行转置操作，为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，从attention函数中可以看到，利用的是原始输入的倒数第一和第二维，这样我们就得到了每个头的输入\n",
    "        # src:query.shape=[30,10,512] tgt:query.shape=[30,9,512]\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        # 得到每个头的输入后，接下来就是将他们传入到attention中，这里直接调用我们之前实现的attention函数，同时也将mask和dropout传入其中\n",
    "        # src:query.shape=[30,8,10,64] tgt:query.shape=[30,8,9,64]\n",
    "        # 8*64 = 512\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        # 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法。这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，所以，下一步就是使用view重塑形状，变成和输入形状相同。\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        # 最后使用线性层列表中的最后一个线性变换得到最终的多头注意力结构的输出\n",
    "        # src:return.shape=[30,10,512] tgt:return.shape=[30,9,512]\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd344ad",
   "metadata": {},
   "source": [
    " ## 前馈全连接层\n",
    "除了attention子层之外，我们的编码器和解码器中的每个层都包含一个全连接的前馈网络，该网络在每个层的位置相同（都在每个encoder-layer或者decoder-layer的最后）。该前馈网络包括两个线性变换，并在两个线性变换中间有一个ReLU激活函数。\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$                                                                        \n",
    "\n",
    "尽管两层都是线性变换，但它们在层与层之间使用不同的参数。另一种描述方式是两个内核大小为1的卷积。 输入和输出的维度都是 $d_{\\text{model}}=512$, 内层维度是$d_{ff}=2048$。（也就是第一层输入512维,输出2048维；第二层输入2048维，输出512维）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbdf4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        #初始化函数有三个输入参数分别是d_model，d_ff，和dropout=0.1，第一个是线性层的输入维度也是第二个线性层的输出维度，因为我们希望输入通过前馈全连接层后输入和输出的维度不变，第二个参数d_ff就是第二个线性层的输入维度和第一个线性层的输出，最后一个是dropout置0比率。\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #输入参数为x，代表来自上一层的输出，首先经过第一个线性层，然后使用F中的relu函数进行激活，之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4cf4d",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "编码器由N = 6个完全相同的层组成。\n",
    "编码器的每层encoder包含Self Attention 子层和FFN子层，每个子层都使用了残差连接[(cite)](https://arxiv.org/abs/1512.03385)，和层标准化（layer-normalization） [(cite)](https://arxiv.org/abs/1607.06450)。\n",
    "\n",
    "我们称呼子层为：$\\mathrm{Sublayer}(x)$，每个子层的最终输出是$\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$。 dropout [(cite)](http://jmlr.org/papers/v15/srivastava14a.html)被加在Sublayer上。\n",
    "\n",
    "为了便于进行残差连接，模型中的所有子层以及embedding层产生的输出的维度都为 $d_{\\text{model}}=512$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "456324cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个clones函数，来更方便的将某个结构复制若干份\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "    The encoder is composed of a stack of N=6 identical layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        # 调用时会将编码器层传进来，我们简单克隆N分，叠加在一起，组成完整的Encoder\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978cf74",
   "metadata": {},
   "source": [
    "第一个子层包括一个**多头自注意力层**和**规范化层**以及一个**残差连接**\n",
    "\n",
    "第二个子层包括一个**前馈全连接层**和**规范化层**以及一个**残差连接**\n",
    "\n",
    "可以看到，两个子层的结构其实是一致的，只是中间核心层的实现不同.\n",
    "\n",
    "![image-20211014164210704](https://gitee.com/shenhao-stu/picgo/raw/master/Others/image-20211014164210704.png)\n",
    "\n",
    "下面的**SublayerConnection**类用来处理单个Sublayer的输出，该输出将继续被输入下一个Sublayer："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdcf110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\" \n",
    "    实现子层连接结构的类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "\n",
    "        # 原paper的方案\n",
    "        #sublayer_out = sublayer(x)\n",
    "        #x_norm = self.norm(x + self.dropout(sublayer_out))\n",
    "\n",
    "        # 稍加调整的版本\n",
    "        sublayer_out = sublayer(x)\n",
    "        sublayer_out = self.dropout(sublayer_out)\n",
    "        x_norm = x + self.norm(sublayer_out)\n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26bf10",
   "metadata": {},
   "source": [
    "注：上面的实现中，我对残差的链接方案进行了小小的调整，和原论文有所不同。把x从norm中拿出来，保证永远有一条“高速公路”，这样理论上会收敛的快一些，但我无法确保这样做一定是对的，请一定注意。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04340eed",
   "metadata": {},
   "source": [
    "定义好了SubLayerConnection，我们就可以实现EncoderLayer的结构了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aef53a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"EncoderLayer is made up of two sublayer: self-attn and feed forward\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size   # embedding's dimention of model, 默认512\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # attention sub layer,let self.self_attn use one arg by using lambda\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # feed forward sub layer\n",
    "        z = self.sublayer[1](x, self.feed_forward)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607b20a",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "解码器也是由N = 6 个完全相同的decoder层组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71378e44",
   "metadata": {},
   "source": [
    "### 1. 解码器整体结构\n",
    "解码器的作用：根据编码器的结果以及上一次预测的结果，输出序列的下一个结果。\n",
    "\n",
    "整体结构上，解码器也是由N个相同层堆叠而成。构造代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "848e558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用类Decoder来实现解码器\n",
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        # 初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N\n",
    "        super(Decoder, self).__init__()\n",
    "        # 首先使用clones方法克隆了N个layer，然后实例化一个规范化层，因为数据走过了所有的解码器层后最后要做规范化处理。\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，source_mask，target_mask代表源数据和目标数据的掩码张量，然后就是对每个层进行循环，当然这个循环就是变量x通过每一个层的处理，得出最后的结果，再进行一次规范化返回即可。\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bd249",
   "metadata": {},
   "source": [
    "### 2. 解码器层\n",
    "每个解码器层由三个子层连接结构组成\n",
    "- 第一个子层连接结构包括一个**多头自注意力子层**和规范化层以及一个残差连接\n",
    "- 第二个子层连接结构包括一个**多头注意力子层**和规范化层以及一个残差连接\n",
    "- 第三个子层连接结构包括一个**前馈全连接子层**和规范化层以及一个残差连接。\n",
    "![image-20211014165218933](https://gitee.com/shenhao-stu/picgo/raw/master/Others/image-20211014165218933.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57788d",
   "metadata": {},
   "source": [
    "有一个细节需要注意，第一个子层的多头注意力和编码器中完全一致，第二个子层，它的**多头注意力模块**中，**query来自上一个子层，key 和 value 来自编码器的输出**。可以这样理解，就是第二层负责，利用解码器已经预测出的信息作为query，去编码器提取的各种特征中，查找相关信息并融合到当前特征中，来完成预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "232c5233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用DecoderLayer的类实现解码器层\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        # 初始化函数的参数有5个，分别是size，代表词嵌入的维度大小，同时也代表解码器的尺寸，第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，第三个是src_attn,多头注意力对象，这里Q!=K=V，第四个是前馈全连接层对象，最后就是dropout置0比率\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        # 按照结构图使用clones函数克隆三个子层连接对象\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # forward函数中的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量memory，以及源数据掩码张量和目标数据掩码张量，将memory表示成m之后方便使用。\n",
    "        m = memory\n",
    "        # 将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q,K,V都是x，最后一个参数时目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据。\n",
    "        # 比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，同样生成第二个字符或词汇时，模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用。\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 接着进入第二个子层，这个子层中常规的注意力机制，q是输入x;k,v是编码层输出memory，同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄露，而是遮蔽掉对结果没有意义的padding。\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "\n",
    "        # 最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果，这就是我们的解码器结构\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1f8ad",
   "metadata": {},
   "source": [
    "## 模型输出\n",
    "\n",
    "输出部分就很简单了，每个时间步都过一个 线性层 + softmax层\n",
    "\n",
    "线性层的作用：通过对上一步的线性变化得到指定维度的输出，也就是转换维度的作用。转换后的维度对应着输出类别的个数，如果是翻译任务，那就对应的是文字字典的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c473170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将线性层和softmax计算层一起实现，因为二者的共同目标是生成最后的结构\n",
    "# 因此把类的名字叫做Generator，生成器类\n",
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        # 初始化函数的输入参数有两个，d_model代表词嵌入维度，vocab.size代表词表大小\n",
    "        super(Generator, self).__init__()\n",
    "        # 首先就是使用nn中的预定义线性层进行实例化，得到一个对象self.proj等待使用\n",
    "        # 这个线性层的参数有两个，就是初始化函数传进来的两个参数：d_model，vocab_size\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向逻辑函数中输入是上一层的输出张量x,在函数中，首先使用上一步得到的self.proj对x进行线性变化,然后使用F中已经实现的log_softmax进行softmax处理。\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04d907",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46e40095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "# 使用EncoderDecoder类来实现编码器-解码器结构\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. \n",
    "    Base for this and many other models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        # 初始化函数中有5个参数，分别是编码器对象，解码器对象,源数据嵌入函数，目标数据嵌入函数，以及输出部分的类别生成器对象.\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        # input embedding module(input embedding + positional encode)\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed    # ouput embedding module\n",
    "        self.generator = generator    # output generation module\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        # 在forward函数中，有四个参数，source代表源数据，target代表目标数据,source_mask和target_mask代表对应的掩码张量,在函数中，将source source_mask传入编码函数，得到结果后与source_mask target 和target_mask一同传给解码函数\n",
    "        memory = self.encode(src, src_mask)\n",
    "        res = self.decode(memory, src_mask, tgt, tgt_mask)\n",
    "        return res\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # 编码函数，以source和source_mask为参数,使用src_embed对source做处理，然后和source_mask一起传给self.encoder\n",
    "        src_embedds = self.src_embed(src)  # src_embedds.shape=[30,10,512]\n",
    "        return self.encoder(src_embedds, src_mask)  # src_mask.shape=[30,1,10]\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        # 解码函数，以memory即编码器的输出，source_mask target target_mask为参数,使用tgt_embed对target做处理，然后和source_mask,target_mask,memory一起传给self.decoder\n",
    "        target_embedds = self.tgt_embed(tgt)\n",
    "        return self.decoder(target_embedds, memory, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "# Full Model\n",
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    构建模型\n",
    "    params:\n",
    "        src_vocab:\n",
    "        tgt_vocab:\n",
    "        N: 编码器和解码器堆叠基础模块的个数\n",
    "        d_model: 模型中embedding的size，默认512\n",
    "        d_ff: FeedForward Layer层中embedding的size，默认2048\n",
    "        h: MultiHeadAttention中多头的个数，必须被d_model整除\n",
    "        dropout:\n",
    "    \"\"\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ccb5c9",
   "metadata": {},
   "source": [
    "## 实战案例\n",
    "\n",
    "下面我们用一个人造的玩具级的小任务，来实战体验下Transformer的训练，加深我们的理解，并且验证我们上面所述代码是否work。\n",
    "\n",
    "任务描述：针对数字序列进行学习，学习的最终目标是使模型学会输出与输入的序列删除第一个字符之后的相同的序列，如输入[1,2,3,4,5]，我们尝试让模型学会输出[2,3,4,5]。\n",
    "\n",
    "显然这对模型来说并不难，应该简单的若干次迭代就能学会。\n",
    "\n",
    "代码实现的基本的步骤是：\n",
    "\n",
    "第一步：构建并生成人工数据集\n",
    "\n",
    "第二步：构建Transformer模型及相关准备工作\n",
    "\n",
    "第三步：运行模型进行训练和评估\n",
    "\n",
    "第四步：使用模型进行贪婪解码\n",
    "\n",
    "训练的大致流程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5439adc",
   "metadata": {},
   "source": [
    "### 批处理和掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f149775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        # src,trg [30,10]\n",
    "        self.src = src\n",
    "        # src_mask.shape=[30,1,10]\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]  # [30,9]\n",
    "            self.trg_y = trg[:, 1:]  # [30,9]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        # tgt.shape [30,9]\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)  # [30,1,9]\n",
    "        tgt_mask = tgt_mask & subsequent_mask(\n",
    "            tgt.size(-1)).type_as(tgt_mask.data)  # subsequent_mask.shape [1,9,9]\n",
    "        # tgt_mask=[30,9,9]\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c0c7b",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ef22bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    # batch.src.shape=[30,10] batch.trg.shape=[30,9]\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg,\n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                  (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c76e5d",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "我们使用Adam优化器[(cite)](https://arxiv.org/abs/1412.6980)，其中 $\\beta_1=0.9$, $\\beta_2=0.98$并且$\\epsilon=10^{-9}$。我们根据以下公式在训练过程中改变学习率：                                         \n",
    "$$\n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot                                                                                                                                                                                                                                                                                                \n",
    "  \\min({step\\_num}^{-0.5},                                                                                                                                                                                                                                                                                                  \n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})                                                                                                                                                                                                                                                                               \n",
    "$$\n",
    "这对应于在第一次$warmup\\_steps$步中线性地增加学习速率，并且随后将其与步数的平方根成比例地减小。我们使用$warmup\\_steps=4000$。                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "089b3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "             min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "                   torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b8f2d",
   "metadata": {},
   "source": [
    "> 以下是此模型针对不同模型大小和优化超参数的曲线示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6baef6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x205cec22ee0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD4CAYAAAD2FnFTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABN40lEQVR4nO3dd3hUVfrA8e9Jh/RCeiUJJRBKCL2DIAiKgCCK2FDEsrqWXV13dV1XV1Zd28rqT10F1BVRQVEQEJiAhF5CgEAgIT0hlXRS5/z+mCEGSBmSSSblfJ4nT2buvefed4Yw75x7z32PkFKiKIqiKFczM3UAiqIoSsekEoSiKIrSIJUgFEVRlAapBKEoiqI0SCUIRVEUpUEWpg7AGNzc3GRgYKCpw1AURelUjhw5kiel7NXY+i6RIAIDAzl8+LCpw1AURelUhBApTa1Xp5gURVGUBqkEoSiKojRIJQhFURSlQQZdgxBCzADeBcyBT6SUK65aL/TrbwLKgXullEebaiuEWAC8BPQHRkgpD9fb35+ApUAt8LiUcmsrXqOiKJ1QdXU16enpVFRUmDqUTs/GxgZfX18sLS2vq12zCUIIYQ6sBKYB6cAhIcRGKWVcvc1mAqH6n5HAB8DIZtqeBOYB/3fV8cKARcAAwBvYLoToI6Wsva5XpihKp5aeno69vT2BgYHovoMqLSGlJD8/n/T0dIKCgq6rrSGnmEYACVLK81LKKmAtMOeqbeYAa6TOfsBJCOHVVFsp5WkpZXwDx5sDrJVSVkopk4AE/X4URelGKioqcHV1VcmhlYQQuLq6tqgnZkiC8AHS6j1P1y8zZBtD2rbkeAghlgkhDgshDufm5jazS0VROiOVHIyjpe+jIQmioT1fXSO8sW0MaduS4yGl/EhKGSmljOzVq9H7PJSrlFaV8s3Zb6jR1pg6FEVROjhDEkQ64FfvuS+QaeA2hrRtyfGUFlodt5qX973Mf0/819ShKEqnEBgYSHh4OEOGDCEyMhKAb775hgEDBmBmZnbFTbq//PILw4YNIzw8nGHDhrFz584m9/3mm28ihCAvL69u2WuvvUZISAh9+/Zl69bfxuccOXKE8PBwQkJCePzxx7k8l09lZSW33347ISEhjBw5kuTkZKO9dkMSxCEgVAgRJISwQncBeeNV22wE7hY6o4AiKWWWgW2vthFYJISwFkIEobvwffA6XpPShANZBwD4+MTHpBanmjgaRekcNBoNMTExdclg4MCBrF+/ngkTJlyxnZubGz/++CMnTpxg9erVLFmypNF9pqWl8csvv+Dv71+3LC4ujrVr13Lq1Cm2bNnCI488Qm2tbnzOww8/zEcffcS5c+c4d+4cW7ZsAeC///0vzs7OJCQk8OSTT/Lss88a7XU3myCklDXAY8BW4DSwTkp5SgixXAixXL/ZZuA8ugvKHwOPNNUWQAgxVwiRDowGNgkhturbnALWAXHAFuBRNYLJOPIv5ROTE8P80PlYmlnyt31/q/sWoiiK4fr370/fvn2vWT506FC8vb0BGDBgABUVFVRWVja4jyeffJLXX3/9iusDP/zwA4sWLcLa2pqgoCBCQkI4ePAgWVlZFBcXM3r0aIQQ3H333Xz//fd1be655x4AbrvtNnbs2GG0/9cG3QchpdyMLgnUX/ZhvccSeNTQtvrlG4ANjbR5FXjVkNgUw+1O341Ecnvf2xngNoCX973MhoQNzAudZ+rQFKVJf/vxFHGZxUbdZ5i3A3+9eUCz2wkhmD59OkIIHnroIZYtW2bQ/r/77juGDh2KtbU1AA888ADLly8nMjKSjRs34uPjw+DBg69ok5GRwahRo+qe+/r6kpGRgaWlJb6+vtcsv9zGz093Vt7CwgJHR0fy8/Nxc3MzKM6mdIlifYphdqbtxMvWi34u/ejr0pdN5zfx5uE3Ge8znl491YV+RWlIdHQ03t7e5OTkMG3aNPr163fNqaWrnTp1imeffZZt27bVLfvkk08AKC8v59VXX71i3WUNffMXQjS6vKk2xqASRDdxqeYS+zP3Mzd0LkIIBIKXRr/E/I3zeXn/y7w3+T01pFDpsAz5pt9WLp8ycnd3Z+7cuRw8eLDJBJGens7cuXNZs2YNwcHB16xPTEwkKSmprveQnp5OREQEBw8exNfXl7S0tCv25e3tja+vL+np6dcsB+ra+Pr6UlNTQ1FRES4uLkZ57aoWUzexL3MfFbUVTPabXLcs0DGQxyMeJyotiu8TvjdZbIrSUZWVlVFSUlL3eNu2bQwcOLDR7QsLC5k1axavvfYaY8eObXCb8PBwcnJySE5OJjk5GV9fX44ePYqnpye33HILa9eupbKykqSkJM6dO8eIESPw8vLC3t6e/fv3I6VkzZo1zJmju1/5lltuYfXq1QB8++23TJkyxWhf9lSC6Cai0qKwt7Qn0jPyiuVLwpYwwnMEKw6uIK0kreHGitJNZWdnM27cOAYPHsyIESOYNWsWM2bMYMOGDfj6+rJv3z5mzZrFjTfeCMD7779PQkICf//73xkyZAhDhgwhJycH0F2DaG7emgEDBrBw4ULCwsKYMWMGK1euxNzcHIAPPviABx54gJCQEIKDg5k5cyYAS5cuJT8/n5CQEN566y1WrFjR1CGui+gKo1giIyOlmjCocbXaWqZ8M4WRXiN5fcLr16zPKs1i/sb5hDiH8NmNn2FuZm6CKBXlSqdPn6Z///6mDqPLaOj9FEIckVJGNtJE9SC6g9i8WAoqCq44vVSfl50Xz496nmM5x/js1GftHJ2iKB2VShDdgCZVg4WZBeN8xjW6zaygWdwYeCMrj60kNje2HaNTFKWjUgmiG9CkaRjuMRx7K/tGtxFC8MKoF/Cw9eAPu/5AUWVRO0aoKEpHpBJEF3e+6DzJxclM9m/49FJ9jtaOvDHhDXIu5fCX6L+ou6wVpZtTCaKLi0qLAmj0+sPVwnuF8/Swp4lKi+LzuM/bLC5FUTo+lSC6OE2qhv4u/fG09TS4zeL+i5nqP5W3j7zN8dzjbRidoigdmUoQXVjepTyO5x43uPdwmRCCl8e+jIetB09pniK3XE3IpHRfbVHuOyYmhlGjRtXt8+DB3wpWd6Ry30gpO/3PsGHDpHKt785+JweuGihP559uUfsz+Wfk8C+Gy8WbFsvKmkojR6coTYuLizN1CFJKKQMCAmRubu4Vy+Li4uSZM2fkxIkT5aFDh+qWHz16VGZkZEgppTxx4oT09vZucJ/Tpk2TmzdvllJKuWnTJjlx4kQppZSnTp2SgwYNkhUVFfL8+fOyd+/esqamRkop5fDhw+XevXulVquVM2bMqGu/cuVK+dBDD0kppfzqq6/kwoULGzxmQ+8ncFg28dmqehBdmCZVg7etN32dry1LbIi+Ln15ddyrHM89zqsHXlUXrRVFr7XlvoUQFBfrqtMWFRXVtemU5b6Vzqe8upx9WfuYHzq/VXVZpgVMY9mgZXwU+xF9nftyZ/87jRilohjo5+fgwgnj7tMzHGY2X5aiLcp9v/POO9x4440888wzaLVa9u7dC6hy30o72Z+1n8raSoOGtzbn0SGPcrbgLK8fep0gxyBGe482QoSK0jkYu9w36Ooqvf3228yfP59169axdOlStm/frsp9K+1Dk6bB3tKeYR7DWr0vM2HGa+NfY8nPS3gq6ilWz1xNH+c+RohSUQxkwDf9tmLsct8Aq1ev5t133wVgwYIFPPDAAwCq3LfS9mq1texO380433FYmlkaZZ92VnZ8cMMH9LToySPbHyG7LNso+1WUjqwtyn2DLuns2rULgJ07dxIaGgrQ4cp9m3wEkjF+1CimKx25cEQOXDVQ/nz+Z6Pv+0z+GTnyy5Fy3g/zZEllidH3ryiXdYRRTImJiXLQoEFy0KBBMiwsTL7yyitSSinXr18vfXx8pJWVlXR3d5fTp0+XUkr597//Xfbs2VMOHjy47ic7O1tKKeXSpUvrRjz9+uuvMiIiQg4aNEiOGDFCHj58uO6Yr7zyiuzdu7fs06dP3UglKaU8dOiQHDBggOzdu7d89NFHpVarlVJKeenSJXnbbbfJ4OBgOXz4cJmYmNjga2nJKCZV7rsL+tfhf/HF6S/49fZfsbOyM/r+92bs5dEdjzLcczgrp67E0tw4vRRFqU+V+zYuVe5bQUqJJk3DCM8RbZIcAMb4jOHF0S+yL2sfz+95nlptbZscR1EU01IXqbuYpOIkUopTuKv/XW16nLmhcymqLOJfR/6FraUtfx39VzWntaJ0MSpBdDGaVA0Ak/wmtfmx7h14LyXVJXwU+xE9LXvyh8g/qCShKF2IShBdjCZNQ5hr2HUV52uNx4Y8Rll1GZ/HfY69pT0PD3m4XY6rKErbUwmiC8m7lEdsbmy7fkgLIfjj8D9SVl3Gf47/B0tzSx4If6Ddjq8oSttRCaIL2ZW2C4lkit+Udj2umTDjpdEvUa2t5t2j71KtrebhwaonoSidnRrF1IVo0nTF+Uxxl7O5mTmvjn2VW4Jv4T8x/+H9Y++r4n5Kp5eWlsbkyZPp378/AwYMqLv7+aWXXsLHx4chQ4YwZMgQNm/eXNcmNjaW0aNHM2DAAMLDw6moqGh0/2+++SZCCPLy8uqWqXLf6kY5oyurKpPDPh8mXzvwmknjqNXWyhejX5QDVw2U7xx5p+5mHkW5Xh3hRrnMzEx55MgRKaWUxcXFMjQ0VJ46dUr+9a9/lW+88cY121dXV8vw8HAZExMjpZQyLy+vrlz31VJTU+X06dOlv79/XTlxVe5baRP7svbpivNd5+RAxmYmzPjr6L+yoM8CPjnxCa8feh2t1Jo0JkVpKS8vLyIiIgCwt7enf//+dVVUG7Jt2zYGDRrE4MGDAXB1dcXc3LzBbZ988klef/31K0b+qXLfSpvQpGqwt7InwiPC1KFgJsx4YdQLWJtb88XpL7hYeZG/j/m7uuNaabF/HvwnZwrOGHWf/Vz68eyIZw3ePjk5mWPHjjFy5Eiio6N5//33WbNmDZGRkfzrX//C2dmZs2fPIoTgxhtvJDc3l0WLFvHHP/4RuLLc98aNG/Hx8alLJJd1tHLfqgfRBVwuzjfeZ7zRivO11uXRTY8PfZxN5zfxu52/o7y63NRhKUqLlJaWMn/+fN555x0cHBx4+OGHSUxMJCYmBi8vL55++mkAampq2LNnD19++SV79uxhw4YN7NixA9CV+46MjKS8vJxXX32Vl19++ZrjNPTNv8OX+xZCzADeBcyBT6SUK65aL/TrbwLKgXullEebaiuEcAG+BgKBZGChlPKiEMIS+ASI0Me3Rkr5WuteZtcWkxvDxcqLRpn7wZiEEDw46EFcbFx4ef/LPLjtQVZOXYmTjZOpQ1M6mev5pm9s1dXVzJ8/n8WLFzNv3jwAPDw86tY/+OCDzJ49G9B9s584cWLdt/ebbrqJo0ePMnXq1LrtExMTSUpKqus9pKenExERwcGDBztfuW8hhDmwEpgJhAF3CCHCrtpsJhCq/1kGfGBA2+eAHVLKUGCH/jnAAsBaShkODAMeEkIEtvQFdgeaVA0WZhaM8x5n6lAaNL/PfN6a9BZnCs6w5OclpBWnNd9IUToAKSVLly6lf//+PPXUU3XLs7Ky6h5v2LChrgT4jTfeSGxsLOXl5dTU1LBr1y7Cwq78uAwPDycnJ4fk5GSSk5Px9fXl6NGjeHp6drhy34acYhoBJEgpz0spq4C1wJyrtpmD7pu+lFLuB5yEEF7NtJ0DrNY/Xg3cqn8sAVshhAXQA6gCilv06roBqS/ON9JzZJsV5zOGqf5T+b9p/0dBRQF3br6To9lHTR2SojQrOjqazz//nJ07d14xpPWPf/wj4eHhDBo0CI1Gw9tvvw2As7MzTz31FMOHD2fIkCFEREQwa9YsQHcNormq0wMGDGDhwoWEhYUxY8YMVq5cWXeR+4MPPuCBBx4gJCSE4OBgZs6cCcDSpUvJz88nJCSEt956ixUrjDi5UlNDnPTntm5Dd2ro8vMlwPtXbfMTMK7e8x1AZFNtgcKr9nFR/9sSXSLJBcqAZY3EtQw4DBz29/dvcFhXd5B4MVEOXDVQrj291tShGCS5KFnOWj9LDl0zVG5M2GjqcJQOrCMMc+1K2mqYa0N9lauvijS2jSFtrzYCqAW8gSDgaSFE72t2IuVHUspIKWVkr169mtll17UzbScAE/0mmjgSwwQ4BPDlTV8y1H0oz+95nveOvqeGwSpKB2VIgkgH/Oo99wUyDdymqbbZ+tNQ6H/n6JffCWyRUlZLKXOAaHS9EaUB7V2czxgcrR358IYPmRc6j49PfMzTUU9TVl1m6rAURbmKIQniEBAqhAgSQlgBi4CNV22zEbhb6IwCiqSUWc203Qjco398D/CD/nEqMEW/L1tgFGDcAdBdRN6lPE7knjD5zXEtYWluyUujX+KZyGfQpGm4Y9MdnC86b+qwlA5GqnItRtHS97HZBCGlrAEeA7YCp4F1UspTQojlQojl+s02A+eBBOBj4JGm2urbrACmCSHOAdP0z0E36skOOIkuwXwmpYxt0avr4qLSopDITpkgQDcM9p4B9/Dx9I8pqizijp/u4JeUX0wdltJB2NjYkJ+fr5JEK0kpyc/Px8bG5rrbqjmpO7FHdzxKYmEiP8/7udNP1HOh7AJPRz1NbF4s9w28j8eHPo6FmbrRvzurrq4mPT29yWJ3imFsbGzw9fXF0vLKG2mbm5Na/Q/spMqry9mfuZ+FfRd2+uQA4GnryWczPmPFwRV8dvIzYnNjWTF+Rae6tqIYl6WlJUFBQaYOo1tTpTY6qX2Z+6jSVnXa00sNsTK34sXRL/KPcf8gLj+O2368rW4KVUVR2p9KEJ3UzrSd2FvZM9RjqKlDMbqbg29m3ex1eNt687jmcf5x4B9U1laaOixF6XZUguiEarQ17E7fzQTfCR2mOJ+xBToG8sVNX3BX/7v46sxXLN60mPOFapSTorQnlSA6oZicGAorC7vU6aWGWJlb8eyIZ1k5dSU55Tks+HEBq0+tplZba+rQFKVbUAmiE9KkabA0s2ScT8cszmdsE3wnsH7Oesb4jOHNw29y/9b7VcE/RWkHKkF0MlJfnG+E1whsLW1NHU67cevhxnuT3+PVca9y7uI55v84n6/PfK3GyCtKG1IJopM5X3SetJI0pvhNMXUo7U4IwS3Bt7B+znqGug/llQOv8OAvD6rehKK0EZUgOhlNmm7Y50TfzlGcry142nry4Q0f8sKoFziVd4q5G+fyyYlPqNZWmzo0RelSVILoZDSpGga4DsDD1qP5jbswIQQL+y7k+znfM95nPO8efZfbf7qd47nHTR2aonQZKkF0IrnlucTmxXb50UvXw8PWg7cnv817k9+juLKYJZuX8Mr+VyipKjF1aIrS6akE0YlEpUcBdLi5pzuCyf6T+eHWH7iz/52si1/H7A2z2XBug5prQlFaQSWITkSTqsHHzodQp1BTh9Ih2Vra8tyI5/hq9lf42fvx4t4XWbxpMbG5qhiworSEShCdRHl1OQeyDjDZb3KrivPll1ayUpNAWWWNEaPrWAa4DuDzmZ/zj3H/ILs8m8WbF/OXPX8h71KeqUNTlE5FJYhOYm/mXqq0VUzxb93w1re3n+WNrfE89PkRqmq67ukXIQQ3B9/Mj3N/5P6B97MpaROzN8zmkxOfcKnmkqnDU5ROQSWITkKTpsHByoGh7i0vzldcUc2GoxkA7EnI45lvjqPVdu0bzWwtbXly2JN8P+d7hnsM592j79Zdn1AlOxSlaSpBdAI12hp2pe9igu+EVk2is+5QGmVVtfz42Dj+cGNfNh7P5O+b4rrF3cgBDgH8e+q/WTVjFZ49PXlx74vc9uNt7Erb1S1ev6K0hEoQncCxnGMUVRa1anhrrVayel8ywwOdCfd15JFJwdw3NpDPopNZqUkwYrQd2zCPYXxx0xe8NektqrXVPLbzMe7beh8xOTGmDk1ROhyVIDqBy8X5xvqMbfE+tp/OJq3gEveN1c3QJYTghVlhzB3qw5vbzvJ/uxKNFW6HJ4RgWsA0NszZwF9G/oWkoiSW/LyE5duXcyL3hKnDU5QOQyWIDk5KSVRaFCO9RraqON9n0Un4OPVgethvd2CbmQneuG0Qswd58drPZ/h4d/eab8HSzJLb+93Oz/N+5vcRv+dU3inu3Hwnj+54lFP5p0wdnqKYnEoQHVxiYSJpJWmtOr10KrOI/ecLuHt0ABbmV/6TW5ib8c7tQ5gV7sWrm0/zya/dK0kA9LTsydLwpWyZv4UnIp7geO5xFv20iN/t+B1x+XGmDk9RTEYliA7ucnG+SX6TWryPVdHJ9LA0Z9Fw/wbXW5ib8c6iIcwc6Mkrm053u57EZbaWtjwQ/gBb5m3hsSGPcSTnCLf/dDuPbH+EI9lH1MVspdtRCaKD06RpGOg6EPee7i1qn1dayQ8xmcwf5oNjz8anJ7U0N+O9O4bW9STe2Hqm234g2lnZ8dDgh9g6fyu/G/o7TuWf4t4t93L3z3cTlRalynco3YZKEB1YTnkOJ/JOtKr20v8OpFJVq+XeMUHNbns5Sdwxwo+VmkT+8v1Jarv4fRJNsbeyZ9mgZWyZv4XnRz5PTnkOv9v5O+ZvnM+PiT+q8uJKl6cSRAcWlRYF0OLrD1U1Wj7fn8LEPr0IcbczqI25meAfc8N5eFIwXx5I5Ym1x7r0HdeG6GHRgzv63cFP837itfGvAfD8nueZtX4Wq06uoqiyyMQRKkrbUAmiA4tKi8LXzpcQp5AWtd90IpPckkruH9d876E+IQTPzujHczP78VNsFvd8epCicvVt2dLMktm9Z7P+lvWsnLoSHzsf/nXkX0z7dhqv7H+F80Xd89qN0nWpBNFB1RXn829ZcT4pJZ/uSSa4ly0TQt1aFMPyicG8tXAwh1MKmPtBNCn5ZS3aT1cjhGCC7wQ+m/EZ39z8DdMDprP+3HrmfD+H5duXsydjj7pOoXQJKkF0UNGZ0VRpq1p8eulIykVOZBRx39igVlV/nRfhyxdLR1JQVsXc/+zlcHJBi/fVFfVz6ccr417hl9t+4dEhjxJfEM/D2x/m1h9u5cvTX1JcVWzqEBWlxVSC6KA0qRocrR1bXJzv0+gkHGwsmBfh0+pYRvZ2ZcMjY3HsYcmdnxxg/dH0Vu+zq3Ht4crywcvZNn8b/xj3D2wtbFlxcAVT103lL3v+QmxubLcdFaZ0XipBdEA12hp2Z+xmgk/LivOlXyxny8kL3DHSn55WLS/uV1+Qmy3rHx5DhL8TT607zos/nOz2F68bYmluyc3BN/PV7K/4evbXzA6ezbaUbSzevJgFPy7g6zNfU1pVauowFcUgKkF0QHXF+Vo4vPXzfSkIIbh7dKBR43K2teKLpSN5YFwQa/alcMfH+8kurjDqMbqSMNcw/jr6r+xcsJMXRr2AEIJXDrzClG+m8NLel4jJiVG9CqVDMyhBCCFmCCHihRAJQojnGlgvhBDv6dfHCiEimmsrhHARQvwihDin/+1cb90gIcQ+IcQpIcQJIYRNa19oZ6JJ02BlZsVY7+svzldeVcNXB1OZMcATH6ceRo/NwtyMv8wO4993DOV0VjGz3tvDgfP5Rj9OV2JnZcfCvgtZN3sd/7vpf8wInMHmpM0s+XkJt3x/C5+c+IQLZRdMHaaiXKPZBCGEMAdWAjOBMOAOIUTYVZvNBEL1P8uADwxo+xywQ0oZCuzQP0cIYQF8ASyXUg4AJgHdZoyllBJNqoaRXiPpadnzutt/dzSD4ooa7hsbaPzg6rl5sDffPzoWexsL7vh4P+9sP0tNrTrl1BQhBOG9wnl57MvsXLCTl8e8jGsPV949+i7Tv53Osm3L2HR+k5rxTukwDOlBjAASpJTnpZRVwFpgzlXbzAHWSJ39gJMQwquZtnOA1frHq4Fb9Y+nA7FSyuMAUsp8KWW3mforoTCB9NL0Fp1e0molq6KTGOTryLAA5+YbtFIfD3s2PjaWWwZ78872c9z58QEyCtWHmyHsrOyYGzqXVTNWsXnuZh4a/BCpJak89+tzTFmnOwV16MIhNVxWMSlDEoQPkFbvebp+mSHbNNXWQ0qZBaD/fbnYUB9ACiG2CiGOCiH+2FBQQohlQojDQojDubm5BryMzuFycb6JvhOvu+3uc7kk5pZx39jAVg1tvR72Npa8s2goby0czKnMIma+s5ufT2S1y7G7Cj8HPx4d8iib523m0xs/ZYr/FDYnbeb+rfcz7dtpvHHoDU7mnVTXK5R2Z8gQl4Y+aa7+S21sG0PaNhTTOGA4UA7sEEIckVLuuGInUn4EfAQQGRnZZf7naFI1hLuFt6g432fRyfSyt2ZWuHcbRNa0eRG+RPg788TaYzz85VHmR/jy4s1hOPZovECgciUzYcZwz+EM9xzOn0f+mV3pu9ictJn/nfkfa+LW4G/vz4ygGcwMnEmIc8vurleU62FIDyId8Kv33BfINHCbptpm609Dof+dU29fu6SUeVLKcmAzEEE3kFOew8n8ky26OS4hp5RdZ3NZMioAKwvTDE4LdLPlm+VjeGxyCN/HZDD97V1ozuQ031C5Rk/LnswMmsm/p/ybqIVRvDzmZbztvPnkxCfM3TiXeRvn8XHsxyQXJZs6VKULM+ST5BAQKoQIEkJYAYuAjVdtsxG4Wz+aaRRQpD9t1FTbjcA9+sf3AD/oH28FBgkheuovWE8EusWsLa0pzrdqbxJWFmbcObLhOR/ai5WFGc/c2JcNj4zBqYcV9606xDPfHKfoUrcZZ2B0jtaOzA2dy8fTP2bHgh38acSfsLWw5b1j73Hz9zcz94e5vH/sfeIL4tVpKMWohCF/UEKIm4B3AHPgUynlq0KI5QBSyg+F7oT3+8AMdKeF7pNSHm6srX65K7AO8AdSgQVSygL9uruAP6E7HbVZStngdYjLIiMj5eHDh6/vlXdAD29/mJTiFDbN3XRd1xCKyqsZ9doOZg/y4o0Fg9swwutTWVPLv3ck8MGuRNzsrHjp5gHMGOjZbtdHuroLZRfYkbqDHak7OJJ9BK3U4mvnyw0BNzDVfyqDeg3CTKhbnZTG6U/fRza6vit84+gKCaKsuozxa8dzR787+MPwP1xX2//blchrP59h8+PjCfN2aKMIW+5EehHPfhdLXFYxk/r24uVbBuLvev1DeJXG5V/KJyotiu2p29mftZ8abQ3uPdyZ7D+ZyX6TGe45HCtzK1OHqXQwKkF0EtuSt/H0rqf59MZPGe453OB2NbVaJryuwd+1J2uXjW7DCFunplbL6n0pvLUtnhqt5LHJISyb2BtrC3NTh9bllFSVsDt9N9tTtrMnYw8VtRX0sOjBGO8xTPSdyHjf8bj1aFmFX6VraS5BGKdQj9JqmrSWFefbFpdNZlEFL90yoI0iMw4LczOWjgtiVrgXL/90in/9cpYNMRm8MDuMyX1bNp2q0jB7K3tm9Z7FrN6zqKip4OCFg+xO382u9F3sSNUNBhzoOpAJfhOY6DuR/i791Wk/pUGqB9EBVGurmfT1JCb5TeLVca9eV9vbPthLdkkFUc9Mxtys8/wnj4rP4aWNp0jOL2dCn178+ab+9PW0N3VYXZqUkrMXz9Yli9jcWCQS9x7ujPcdzxjvMYz0GomjtaOpQ1XaiepBdALHso9RXFV83aOXYtMLOZxykRdmh3Wq5AAwqa872550Y82+ZN7bcY6Z7+7mjhH+PDWtD6521qYOr0sSQtDXpS99Xfry4KAHKagoYE/GHnal7WJr8la+O/cdZsKMgW4DGeM9hrHeYxnoNrBFFYWVrkH1IDqAfx78J+vi1/Hrol+vq/7Sk1/HsO3UBfY9PxUHm857Q9rFsire2X6WLw6k0tPSnOWTgrlvbKDRSpUrzavR1nAi7wR7M/eyN3MvJ/NOopVa7CztGOk1kjHeYxjjPQZfe19Th6oYkbpI3cFJKZm5fibBTsGsnLrS4HY5xRWM/edOFo8M6PDXHwyVkFPKa5tPs+NMDm521jw6OZg7R/qrC9kmUFRZxIGsA3UJI6tMVz7F396fUV6jGO41nOEew3Ht4WriSJXWUKeYOrhzhefIKM1gafjS62r3xf4UarSSe8cEtk1gJhDibsd/7x3OkZQC3tgaz99+jOPj3ed5fGoo84f5YmmuxvS3F0drR6YHTmd64HSklCQXJ9cli01Jm1h3dh0AIU4hDPcczgjPEUR6ROJk42TawBWjUj0IE/u/4//H+zHvs3PBTnr17GVQm4rqWsau2MlQfyc+ucfwIbGdiZSS6IR83tgWz/G0QgJde/Lo5BBuHeqjEoWJ1WhriMuP4+CFgxy6cIhjOcfqSpT3de5blzCGeQ7Dwarj3Zej/EadYurgFv20CHNhzpezvjS4zbrDafzx21i+fGAkY0O69nh2KSXbT+fw1i9nOZ1VjI9TD5ZP7M2CSD9sLNWpp46guraak/knOZh1kEPZh4jJiaGythKBoJ9LP4a6D2Wox1Ai3CNaVIRSaTsqQXRg2WXZ3PDtDTwR8QQPhD9gUBspJTe9twetVrLl9+O7zfh1KSWa+Bze35nA0dRC3OyseWB8EHeNCsDOWp0p7UiqaquIzY3l0IVDHMk+QmxebF0Pw8fOhwj3iLqEEeQYpMqBmJC6BtGB7UrfBVxfcb795ws4nVXMinnh3SY5gG6I5pR+Hkzu687+8wX8JyqBFT+f4YOoRBaP9Ofu0YF4OnarmWk7LCtzKyI9I4n01H3uVGuriS+I52j2UWJyY4jOjObH8z8CumsdQ3v91sMIcw1TJUE6ENWDMKHl25eTVpzGT3N/MvjDftmawxxKLmDfn6Z2+1MsMWmFfBCVwLa4bMyFYPYgL5aO6024r7rRqyOTUpJWksbRnKMcyznG0eyjJBcnA2BpZkk/l36Eu4UT3iucwW6D8bX37VZfhtqTOsXUQbWkOF9qfjkT39TwyKRg/nBjvzaOsPNIzS9n1d5k1h1Oo7SyhhGBLtw/LpBpYZ6d7gbC7qqgooBjOceIzY0lNjeWU/mn6k5LOVs7M9BtYF3CGNhroLr4bSTqFFMHtSdjD9Xa6us6vbR6XzLmQrBkVGDbBdYJ+bv25MWbw3hyWijrDqezam8Sy784iq9zD+4Y4c+CSF/c7dXpp47MxcaFqf5Tmeo/FdCNlEosTCQ2L5YTuSc4kXeCPRl7kPoJKQMdAhnUaxDhbuEMcB1AH5c+WJurO/CNTfUgTOS5X58jOiMazUKNQaUMSitrGP2PHUzu5857d1xfQb/uplYr+SXuAp/vTyE6IZ/+5uk87n4c38hZDBg1EzM1TLZTKq0q5WT+SU7kntD1NPJiKagoAMBCWBDsFEyYa1jdTx/nPthYqC8GTVE9iA6oWlvN7vTdTPabbHCdm28Pp1FSWcP944LaOLrOz9xMMGOgFzMGenHhwDe4bP0rVhcvwS9fkr7di4yg2widvgwXT9POvqdcHzsrO0Z5jWKU1yhAdy0jsyyTuPy4uh9NmoYNCRsAMBfm1ySNvs59VdK4DipBmMCx7GOUVJUwxW+KQdtrtZJVe5MZ6u/EED+ntg2uq9BqYffreEa9Bt4RVM75iJMHtmNz4ktGnv83NR+sJMZ2FNqhdzFw4gKsrNTImc5GCIGPnQ8+dj5MC5gG6JJGVlkWp/NPcyr/FHEFcexO3833Cd8DuqTR26k3/V3608e5j654oXNfnG2cTfhKOi6VIExAk6bB2tya0d6GTfCjic8hOb+cp6f3bePIuojKEtiwHM78BIPvhNlvY21pw7BbQuGWh0mKP06m5mP6XvgRt+hHyI1+nnPuM+g1Zgkhg8YgzNQpqM5KCIG3nTfedt5MDdBdz5BSkl2erUsY+p7G3sy9bEzcWNfOvYc7fVz60Ne5b13S8Hfw7/aVbNU1iHZ2uThfiFMI709936A2iz/ZT2JOGb8+O1mVmWhOwXn46k7IOwvTX4FRD0MjQyRrqiqJ+/U75NEv6F+6HytRS4qZH9kBN+M/6R48A9RIsa4s/1I+Zy+e5ezFs8QXxBN/MZ7zReep0dYAYG1uTbBTcF3S6OPchz7OfbrUfBnqGkQHc/biWTJKMwy+czr+QgnRCfn8cUZflRyak7gTvrlP9/iu7yC46RFiFlbWDJp6J0y9k+L8HI7u/Bz7s+sZkfQfSPoPZyzDKAi+ldDJd9HLw6cdXoDSnlx7uDK6x+grevLVtdWcLzpP/MX4uqQRlRZVd10DwL2nOyFOIQQ7Bdf9DnYMxs7KzgSvom2pBNHONGkaBIJJfpMM2v6z6CRsLM24Y7i6oNooKWH/f2DbX6BXP1j0Jbj0vq5dOLi6M2rB08DTpJ2PJ/3XNXilbGTMmX9Qc3oFsdaDuRQyi9CJi3Dx8Gub16GYnKW5Zd2kSgTrlkkpybuUV5c0EgsTSShM4Jv4b6iorahr62XrdUXSCHEKobdj7+ua46WjUaeY2tntP92OhZkFX97UfHG+grIqRr+2g3kRvrw2L7wdouuEqi/Bj7+H2LXQ/2a49UOwNtI3OSlJPX2QrL1f4Z2xFT+ZiVYK4m3CKek9i8Bxi3D3CTTOsZROp1ZbS2ZpJucKz9UljcTCRJKKkqjSVtVt52Pno+tl6JNGoEMggY6BHeJmP3WKqQO5UHaBuPw4noh4wqDtvzqYSmWNlvvGBrZtYJ1VUQZ8vRgyj8HkP8P4Z8CYF5iFwD9sJP5hI5FaLYlxh7mw72u8s7bR//RrcPo1Tlv0J99/Bn6j5uIfOkiVhOhGzM3M8XPww8/Bjyn+v41IrNHWkF6SfkXSOFd4jr2Ze+uubwC49XAj0CGQIMegut9BjkF42XphbtYxyuioBNGOdqXpivMZMry1ulbLmn3JjA91o4+HfVuH1vmkHoCv74Lqclj0P+g3q00PJ8zMCB44guCBI5BSknI2hqx963BP+5lx59+G82+TKnzI8piI09BbCBl2A+YWnXcaWKXlLMwsCHTU9RIuj6QC3f1P6SXpJBclk1ScRFJREslFyWxN3kpxVXHddlZmVgQ4BhDkEESgoz5x6B/bWtq272tp16N1c5o0DQEOAQQ5Nn+z2+YTWWQXV6pTSw05sho2PQ2OvnDPRnDv366HF0IQ0HcoAX2HAq+RkxpP8t712CT/wtCsr7G68D+KfrbjnMMo6DOD4NG34uxq2GRQStdlaWZZ10uYzG8DKKSUXKy8qEscRUkkF+t+nyk4w/bU7Wiltm5b9x7uBDoGEuAQUPcT4hTSZnOFq2sQ7aS0qpTxX49ncb/FPDP8mWa3v3VlNEWXqtnx1ETMVME5ndpq2PInOPQxBE+B+f+Fni6mjuoKRYUFxEf/AGe3EFq0F2eKqZFmxFv1p8h7Am6DZxIyaCxmFuq7mdK8qtoq0krSrkgcycXJpBanUlhZCMC0gGm8NemtFu1fXYPoIPZk7qFGW8Nk/+aL8x1NvUhMWiF/u2WASg6XleXBunsgZQ+M+R1MfQnMO96fr6OTCyNm3Qez7qO2poazMbsojNmIy4U9jEn5AFI+oHCjPUkOw9H2noz/8Nn08rm+EVdK92FlblV3gftqhRWFpJSkYGXWdlUAOt7/sC5Kk6rB2dqZIb2GNLvtZ9HJ2NtYcNuwtuk2djpZsbD2TijNgbkfweDbTR2RQcwtLOgTORUideehC7LTOX9wE7XndtC7+AC9YnZCzAskm/mR5Toaq743EBI5DUenjtUrUjomJxsnnGyc2vQYKkG0g2ptNb9m/MoUvynNjk7IKrrE5hNZ3DcmEFs1lSac/A6+f1R3Kun+LeATYeqIWszFwxeXmx8CHkJbq+Vc3CHyj2/GLn03ETkbsM5dR82vZsRbhnLRfSR2fScREnkDNrZd585dpXNRn0Dt4Gj2UUqqSgw6vfT5vhSklNwzJrDtA+vItLWw8xXY8xb4jYKFa8Dew9RRGY2ZuRmh4SMJDR8JQNWlUs4c01AUtxOH7P0My/gSy8w11Ow046xVHwrcR9IjdCK9I6Zi7+Bk2uCVbkMliHZQV5zPq+nifJeqavnfwVSmhXng59J5775stYoi+O4BOLcNht0LM98Ai65dbdWqhx39xtwMY24GoLSkiFNHdlAWH4VL7kGGpX+BZcZqqjXmnLEMocA1ApveYwgYMhlXdWe30kYMShBCiBnAu4A58ImUcsVV64V+/U1AOXCvlPJoU22FEC7A10AgkAwslFJerLdPfyAOeElK+WbLX6JpSSnRpGoY5TWq2Vvuv4/JoLC8mvvHduM5H3LPwto74GIyzHoLhi81dUQmYWfvyJBJ82DSPADKS4s4e3QnZfEaHHIPM+zCt1hnfwX7IF14keM0GK3vSNwHTMQ3dAhm5h3jRiulc2s2QQghzIGVwDQgHTgkhNgopYyrt9lMIFT/MxL4ABjZTNvngB1SyhVCiOf0z5+tt8+3gZ9b+wJN7ezFs2SWZbJs0LImt5NS8ll0EmFeDowI6qYXKc9u1fUczK3g7o0QONbUEXUYPe0cGTBhLkyYC0BVxSXOxEZzMf5XrLMOEXhxLy4Xt8AJKMaWJJswSj0isQseQ2D4WBydXU38CpTOyJAexAggQUp5HkAIsRaYg+7b/WVzgDVSd1PFfiGEkxDCC13voLG2c4BJ+vargSj0CUIIcStwHihr+UvrGHam7UQgmOg3scntohPyOZtdypsLBne/cg1S6q417Pg7eIbr7ox2UqdNmmJl04N+I26AETcAoK3VkpJ4guxTuyFtPx6FxxmsH1ar3SFIMfch134A0mcYrn1G499/OBbWPUz8KpSOzpAE4QOk1Xuejq6X0Nw2Ps209ZBSZgFIKbOEEO4AQghbdIliGtDoHWVCiGXAMgB//45b6VSTqmFQr0G49XBrcrtPo5Nws7Pi5sFe7RRZB1FVBj88BqfWw8D5cMv7YNWNr7+0kJm5GQF9BhPQZzDwOwBKC3NJjf2VkvMHsMmOIahoP65FWyEOqjZYcM6yNwVOAxG+w3DrOwb/0EFYqBv4lHoM+Wto6Ovs1bdfN7aNIW2v9jfgbSllaVPfpKWUHwEfge5O6mb2aRIXyi5wuuA0v4/4fZPbJeWVsfNMDk9MDcXaohudO76YAmsXQ/ZJuOFvMPaJRif3Ua6fnVMvwibMgwm66xhSqyUjNYGMU3uoTj2C48VYBuZuxjZvPcRAqexBilVvSpzCMPMeTK/Q4fj1GYqFlbVpX4hiMoYkiHSgfn/fF8g0cBurJtpmCyG89L0HLyBHv3wkcJsQ4nXACdAKISqklIZNv9aBRKVFATQ7vHVVdBKW5oLFozpuT8jokvfAuruhtgYWfwOh00wdUZcnzMzwCeyDT2Af4H4AamtqSEmIJT9+L7XpR3AoPM2gnI30zP0GjkOVtCDRMpCLDv2QnoOwDxyGX/9IbO2dTPpalPZhSII4BIQKIYKADGARcOdV22wEHtNfYxgJFOk/+HObaLsRuAdYof/9A4CUcvzlnQohXgJKO2NygHrF+RwaH5VUdKmab46kc/Mgb9ztbdoxOhOREg59Alue003qs+grcAsxdVTdlrmFBQH9Igjo99sNiNqaGlIST5Jz9iDV6THYXYwjuGAXzgU/QRxoNwlSzbzJtu1DlWsYPXzD8QyNwMs/VM3n3cU0myCklDVCiMeAreiGqn4qpTwlhFiuX/8hsBndENcEdMNc72uqrX7XK4B1QoilQCqwwKivzMRKqko4eOEgd/W/q8mLzt8cTqO8qpb7usPQ1ppKXRXWY59Dnxkw7yOwUXcJdzRmFhYE9B1CQN8hdcukVktWeiLZ8YeoSDtGj/xT+JWdxLNUAylAtO4UVYZVIMUOfRDu/bEPGIxXn2E4uHSdGxy7G1XNtY1sSdrCH3b/gdUzVhPh0XB5iFqtZOIbGrwcbfhm+Zh2jrCdlVyAr5dA+kHdxD6T/2zcyX0UkygvLiA9/giFycfRZp/CoegsPlVJOIrfBiDm4swF6yBKHfsgPMJwCBiET/BgHJ276XDuDkRVczWRnWk7cbFxYXCvwY1u80tcNukXL/Hnm9p3PoN2l3EE1t4FFYWwYBUMmGvqiBQj6engQp/h02D4b9eQpFZLRnoyuYnHKE8/gUXeGZzLzhGa/R02OWvhhG67bFzJsfan3CEY0asvDn4D8AoehGMvXzVYoYNQCaINVGur2ZO+h6kBU5sszvdZdBI+Tj2YFtaFu+AxX8GPT4CdByzdprvPQenShJkZPv698fHvDcyvW66tqSEr9Qx552MozzyNef45HMqSCMrZhF3ut3V3VhVjS5alH8W2QdS6hGLl1R/XgAF4BvbDWo2oalcqQbSBI9lHKKkuYbJf46OXTmUWcSCpgOdv6oeFeRc81VJbA7+8CPtXQuB4WLAabNXdvN2ZmYUFXr0H4tV74BXLtbVaMjOTyDkfS1lGHGZ557ArTSKwcD+9Cn/W3TIbDZXSgmQzTwps/Ki0D0S4BWPv1Qe3wDDcfXojOsg8zl2JShBtQJOqL87n3Xhxvs+ik+lhac7tkV1waGt5AXx7H5yPghEPwY2vgrman1lpmJm5Gd5+wXj7BQNXnn4sLswjO/EExekn0WbHY1mcjHN5Kl7lh7HJqa7rdVRISy6Ye1HYw59KhwDMXIPp4dUHV/8wPLwDVW2qFlIJwsiklGjSNIz2Gk0Pi4ZLGeSWVLIxJpPbh/vh2LOLfXBmx+mK7RVn6u6Kjlhi6oiUTszByQ2HYZNh2JW9cW1tLRcyk8lLiaM0M57avERsSpJxKU/Bu/QA1lnVcFK37SVpRaa5F4XWvlTa+4NzID09euPs0wcP/1Bsetia4JV1DipBGFn8xXiyyrJYPnh5o9v870AqVbVa7h0b2H6BtYfTP8L6h8DaDu7dBH4jTB2R0kWZmZvj6ReMp18wcPMV62pqashMTyQ/7QyXLpxF5idiU5xMr4oU3MsP6noe8b9tn4sLeZZelNv6UusYgIVrILYewTj79MHNK6Bb9z5UgjAyTaoGgWCC74QG11fW1PL5/hQm9e1FcC+7do6ujWi1sOufsGsF+AyD278AB29TR6V0UxYWFngH9sU7sO8166S2lvycDHJT4ym9kEBVXhIWRSn0LM/Ap/AI7he3YZby29D/KmnBBTN3Cq28KbP1pdbRHyvXAOzcg3D16Y2bpz/mXTiBqARhZJo0DYN7DW60ON+m2CzySiu7zo1xlSWwYTmc+QkG3wmz3wbLbnBHuNIpCTNzXD39cfX0R1cP9Erl5WXkpidwMTOBipzzcDEFq5JUHCoy8C84g1NBKST9tn2VNOeCmRuFlh5c6ulFrb0fFs5+9OgViKNnIG6+wVj3sG+/F2hkKkEY0eXifE8Oe7LB9VJKPo1OIsTdjgmhTVd37RTyE3XF9vLOwowVMHK5Gr+udGo9e9rWq4p7rYqSi+RmJFCYlcSl3GS0hWlYlGRgW5GFb+ERel3chnnalTcfX8SBfPNelFh7UmHrjdbBD0sXP2x7BeDsEYCblz9WVh1zxkSVIIxIk6YBaHR46+GUi5zMKOaVWwd2/jkfEnboRioJM1iyHnpPMnVEitLmbOyd8es3HL9+wxtcX1FRQUZGMkXZSZTnJFF7MQ3zkgxsyjNxqUilV/lheuZWQuJvbWqlIEc4U2jhRqm1B9U9PcDBG0tnX3q6+eHkGYiLRwBWJriYrhKEEWlSNQQ6BBLk2PDpo8+ik3DsYcm8CJ92jsyIpIR9K+GXF6BXP93kPi5d5HSZorSSjY0N/sH9ILhfwxtISWlRPgWZCRRnp1CRn0ZtUQZmJRewuXQBl0vJuJYexj730jVNC7GnwNyVUit3Knp4UGvnjbmjN05BQ+gTMalNXo9KEEZSUlXCoexDLOnf8LDO9IvlbDl5gQcn9KanVSd926sv6e6Kjv0a+t8Ct36gG7GkKIphhMDOyQ07JzcIG9XgJlJKiosvkp+ZQlGOLonUFGZiUZaJdXk2dlU5eF86i1tBIQBHkqeAShAd256MPdRoaxqd++HzfSkIIbh7dGD7BmYsRRnw9WLIPKYrtDf+GVVsT1HagBACB0cXHBxdoP/QRrerrLxEflYqbTkHpUoQRqJJ1eBi48Igt0HXrCuvquGrg6nMGOCJj1MnnAc4db+uEmt1ue6UUr9Zpo5IUbo9a+seDQ7lNSb1FdAIqmur2ZOxh4m+Exsszvfd0QyKK2q4f1xg+wfXWkdWwarZulNJD2xXyUFRuhHVgzCCw9mHGy3Op9VKPotOYpCvIxH+ziaIroVqq3Wzvh36BIKnwG2fQo9OFL+iKK2mehBGoEnTYGNuwyjvay867T6Xy/ncMu4fG9R5hraW5sKaObrkMOZxWPytSg6K0g2pHkQrXS7ON8p7VIPF+T6NTsbd3pqbwtvyUpIRZR3X3fxWlgvzPoZBC00dkaIoJqJ6EK10puAMF8ouMMVvyjXrEnJK2H02lyWjArCy6ARv9cnv4L83gtTC/VtUclCUbk71IFpJk9Z4cb7PopOxsjDjzpEdfM4HbS3s/DvseRv8RsHtn4Odu6mjUhTFxFSCaKWotCiGuA/BtceVs6UVllex/mgGtw7xxtWuA0+TeKkQvnsAEn6BYffCzDfAomPWhVEUpX11gvMeHVdWaRanC043OHpp7aE0LlXXduyqrbln4ZOpcF4Ds96Cm99VyUFRlDqqB9EKl4vzTfKbdMXymlota/YmM7q3K/29HEwQmQHObtX1HMyt4O6NEDjW1BEpitLBqB5EK2jSGi7Ot/VUNplFFdzXEWeMkxJ2vwn/u11XZG9ZlEoOiqI0SPUgWqi4qpjDFw6zZMC1xfk+i07C36UnU/t7mCCyJlSVwQ+PwqkNMPA2uOXfYNXT1FEpitJBqQTRQnvS91Aja64Z3hqbXsjhlIu8MDsMc7MOdGPcxRTd/Q3ZJ+GGv8HYJ9TkPoqiNEkliBaKSovCxcaFcLfwK5Z/Fp2MnbUFCyN9TRNYQ5J+hXV364azLv4GQq+dalFRFOVq6hpEC1TXVvNrxq9M8pt0RXG+nOIKforN5LZhvtjbWJowQj0p4cBHurIZtm7w4E6VHBRFMZjqQbTAoexDlFaXXjO89Yv9KdRoJfeOCTRNYPXVVMKmp+HY59Bnhq5shk0HHVGlKEqHpBJEC2hSdcX5RnqNrFtWUV3LlwdSmdrPnUC39p879golF3TzN6Qf1E3sM/nPanIfRVGum0oQ10lKSVR6FKO9R19RnG/j8Uzyy6q439Q3xqUf0c38VlEEC1bBgLmmjUdRlE7LoK+VQogZQoh4IUSCEOK5BtYLIcR7+vWxQoiI5toKIVyEEL8IIc7pfzvrl08TQhwRQpzQ/762Cp4JnS44zYWyC1ecXpJS8umeJPp62DM62LWJ1m0s5iv4bCaYW8LSbSo5KIrSKs0mCCGEObASmAmEAXcIIcKu2mwmEKr/WQZ8YEDb54AdUspQYIf+OUAecLOUMhy4B/i8xa+uDWjSNJgJMyb6Taxbtv98AWculHD/uEDTzPlQWwNb/gTfLwe/EfBgFHiGN9tMURSlKYb0IEYACVLK81LKKmAtMOeqbeYAa6TOfsBJCOHVTNs5wGr949XArQBSymNSykz98lOAjRCiw1S7i0qLYkivIbjYuNQt+zQ6CeeelswZ4tP+AZUXwBfzYP9/YMRDsGQD2JqwF6MoSpdhSILwAdLqPU/XLzNkm6baekgpswD0vxuqLz0fOCalrLx6hRBimRDisBDicG5urgEvo/UySzM5U3DmitNLqfnlbD+dzeKRAdhYXjsfdZvKjoOPJ0PqPpizEm56XXd6SVEUxQgMSRANnTORBm5jSNuGDyrEAOCfwEMNrZdSfiSljJRSRvbq1cuQXbZaQ8X5Vu1NxlwIlowOaJcY6sRthE9ugOoKuHczDL2rfY+vKEqXZ8gopnTAr95zXyDTwG2smmibLYTwklJm6U9H5VzeSAjhC2wA7pZSJhryQtqDJk1DkGMQgY6BAJRUVLPucBqzBnnh4WDTPkFotbBrBez6J/gMg9u/BIdOMp2poiidiiE9iENAqBAiSAhhBSwCNl61zUbgbv1oplFAkf60UVNtN6K7CI3+9w8AQggnYBPwJylldMtfmnEVVxVz5MKRK04vfXskndLKmvab86GyBL6+S5ccBt+p6zmo5KAoShtptgchpawRQjwGbAXMgU+llKeEEMv16z8ENgM3AQlAOXBfU231u14BrBNCLAVSgQX65Y8BIcALQogX9MumSynrehim8Gv6r9TImroEodVKVu1NJsLfiSF+Tm0fQH4irL0T8s7BjBUwcrkqtqcoSpsy6EY5KeVmdEmg/rIP6z2WwKOGttUvzwemNrD8FeAVQ+JqT1FpUbjauDKo1yAAdp7JISW/nGem9237gyfsgG/vA2EGS9ZD70ltf0xFUbo9VX/BANW11ezJ2MMkv0mYCd1b9tneJLwcbZgx0LPtDiwl7P03fHkbOPjCgxqVHBRFaTeq1IYBDl3QFee7PHrpzIViohPy+eOMvliat1GOrb4EPz4BsV9D/1vg1g/A2q5tjqUoitIAlSAMsDNtJz0sejDKaxQAq6KTsbE0447h/m1zwKJ03eQ+WTG6Qnvjn1HF9hRFaXcqQTRDSklUWhSjvUZjY2FDQVkVG45lMC/CF2dbK+MfMHW/bqRSdQUs+gr63WT8YyiKohhAfS1tRlxBHNnl2Uz2141e+upgKpU1Wu4fG2j8gx1ZBatmg7U9PLBdJQdFUUxK9SCaEZUWpSvO5zuR6lota/YlMz7UjVAPe+MdpKYKtjwHh/8LwVPgtk+hh7Px9q8oitICqgfRDE2qhiG9huBs48zmE1lkF1cad86H0lz4/FZdchjzOCz+ViUHRVE6BJUgmpBRmkH8xfi6m+M+jU6mt5stE/sYqfZT1nH4aBJkHNFNCTr972DWzgX/FEVRGqESRBOi0qIAmOw/maOpFzmeVsi9YwMxMzPCHcwnvoX/3ghIuH8LDFrY+n0qiqIYkUoQTdCkaujt2JsAhwA+3ZOEvY0F8yN8W7dTbS388lf4bil4D4FlUeA91BjhKoqiGJVKEI0oqizicPZhJvtNJqvoEj+fvMCi4X7YWrfiuv6lQvjf7RD9Dgy7D+7eCHYNTYOhKIpiemoUUyP2ZOyhVtYy2X8ya/alIKXk7tGBLd9h7ln4ahEUpsCst2D4UqPFqiiK0hZUgmiEJk2Dq40rIQ5hfHVQw/QwT/xcerZsZ/FbYP2DYG4F9/wIAWOMG6yiKEobUKeYGlBVW1VXnO+HmCwKy6u5ryU3xkkJu9/U9RxcgnTXG1RyUBSlk1A9iAYcunCIsuoyJvlN4tVvkhjg7cCIIJfr20lVGXz/CMR9DwNvg1v+DVYt7IEoiqKYgOpBNECTpqGHRQ9qy0I4l1PKfWODENczOc/FFPjvdIj7Aaa9DPM/UclBUZROR/UgriKlRJOmYYz3GL7cn4WbnRU3D76OaT2TfoV1d+uGsy7+FkJvaLtgFUVR2pDqQVwlriCOnPIcBjqNZueZHBaPDMDawoC7m6WEA/8Ha+aArRs8uFMlB0VROjXVg7iKJlWDmTAjIdkPK/MiFo8yYM6HmkrY9BQc+wL6zIR5H4GNQ9sHqyiK0oZUgriKJk3DILchbNxfzOzBXrjb2zTdoOSCbv6G9EMw4Q8w6Xk1uY+iKF2C+iSrJ70knbMXz2JfO5jyqtrmq7amH9EV28s+BQtWw5S/qOSgKEqXoXoQ9Vwuzncs3ocRgS4M9HFsfOOY/8GPvwd7D1j6C3gObI8QFUVR2o36uluPJk2Dh00AWXl23D8usOGNamtgy5/g+4fBbwQ8GKWSg6IoXZLqQegVVRZxJPsITtXT8XHqwbQwz2s3Ki+Ab+6FpF0wcjlMfwXMLds9VkVRlPagEoTerxm/UitrSU0L4k9TAjG/es6H7FPw1R1QkgVzVsLQu0wTqKIoSjtRCUJPk6rBCkdqtQEsHO535cq4jbBhOVjbw72bwW+4aYJUFEVpRypBoCvO92vGHi4VDuS2Yf449tCfNtJqIeo12P06+ETC7V+Aw3XcVa0oitKJqQQBHLxwkEs15VSWhHHPmEDdwopi2PAQxG+GIYt1czhYNnNPhKIoSheiEgSwPWUnaK0Y5zOK4F52kJ8Ia++EvHMw458w8iG4nmJ9iqIoXUC3TxBaqWVb0k6qS0NZemMfSNgO394PwgyWbIDeE00doqIoikl0+/sg4vLiKKnJx1UMZXzuV/DlAnDwhQc1KjkoitKtGZQghBAzhBDxQogEIcRzDawXQoj39OtjhRARzbUVQrgIIX4RQpzT/3aut+5P+u3jhRA3tvZFNuXLkz8jpeA/VkcQv7wA/WbD0m26GeAURVG6sWYThBDCHFgJzATCgDuEEGFXbTYTCNX/LAM+MKDtc8AOKWUosEP/HP36RcAAYAbwH/1+2sSulG2EVUgGZ26FyX+BhWvA2q6tDqcoitJpGNKDGAEkSCnPSymrgLXAnKu2mQOskTr7ASchhFczbecAq/WPVwO31lu+VkpZKaVMAhL0+zG6YzE/UiIucOOlUlj0FUz8g7oYrSiKomdIgvAB0uo9T9cvM2Sbptp6SCmzAPS/3a/jeAghlgkhDgshDufm5hrwMq5VauPKmEvWDJv2IfS7qUX7UBRF6aoMGcXU0FdqaeA2hrRtyfGQUn4EfAQQGRnZ3D4bNL7fGMb3O9ySpoqiKF2eIT2IdKB+7QlfINPAbZpqm60/DYX+d851HE9RFEVpY4YkiENAqBAiSAhhhe4C8sarttkI3K0fzTQKKNKfNmqq7UbgHv3je4Af6i1fJISwFkIEobvwfbCFr09RFEVpoWZPMUkpa4QQjwFbAXPgUynlKSHEcv36D4HNwE3oLiiXA/c11Va/6xXAOiHEUiAVWKBvc0oIsQ6IA2qAR6WUtcZ6wYqiKIphhJQtOn3foURGRsrDh9W1BEVRlOshhDgipYxsbH23v5NaURRFaZhKEIqiKEqDVIJQFEVRGqQShKIoitKgLnGRWgiRC6S0YhduQJ6RwjEmFdf1UXFdHxXX9emKcQVIKXs1trJLJIjWEkIcbupKvqmouK6Piuv6qLiuT3eMS51iUhRFURqkEoSiKIrSIJUgdD4ydQCNUHFdHxXX9VFxXZ9uF5e6BqEoiqI0SPUgFEVRlAapBKEoiqI0qFsnCCHEDCFEvBAiQQjxXDscz08IoRFCnBZCnBJCPKFf/pIQIkMIEaP/ualemz/p44sXQtxYb/kwIcQJ/br3hGjdXKlCiGT9/mKEEIf1y1yEEL8IIc7pfzu3Z1xCiL713pMYIUSxEOL3pni/hBCfCiFyhBAn6y0z2vujL2//tX75ASFEYCviekMIcUYIESuE2CCEcNIvDxRCXKr3vn3YznEZ7d/NyHF9XS+mZCFEjAner8Y+G0z7Nyal7JY/6MqPJwK9ASvgOBDWxsf0AiL0j+2Bs0AY8BLwTAPbh+njsgaC9PGa69cdBEajm4HvZ2BmK2NLBtyuWvY68Jz+8XPAP9s7rqv+vS4AAaZ4v4AJQARwsi3eH+AR4EP940XA162IazpgoX/8z3pxBdbf7qr9tEdcRvt3M2ZcV63/F/CiCd6vxj4bTPo31p17ECOABCnleSllFbAWmNOWB5RSZkkpj+oflwCnaWC+7XrmAGullJVSyiR0822MELoZ+ByklPuk7l97DXBrG4Q8B1itf7y63jFMEddUIFFK2dQd820Wl5RyN1DQwPGM9f7U39e3wFRDejkNxSWl3CalrNE/3Y9uVsZGtVdcTTDp+3WZvv1C4Kum9tFGcTX22WDSv7HunCB8gLR6z9Np+sPaqPTdu6HAAf2ix/SnBD6t141sLEYf/eOrl7eGBLYJIY4IIZbpl3lI3cyA6H+7myCuyxZx5X9cU79fYNz3p66N/sO9CHA1Qoz3o/sWeVmQEOKYEGKXEGJ8vWO3V1zG+ndri/drPJAtpTxXb1m7v19XfTaY9G+sOyeIhjJnu4z5FULYAd8Bv5dSFgMfAMHAECALXTe3qRjbIvaxUsoIYCbwqBBiQhPbtmdcCN10tbcA3+gXdYT3qykticPoMQoh/oxuVsYv9YuyAH8p5VDgKeB/QgiHdozLmP9ubfFvegdXfglp9/ergc+GRjdt5DhGja07J4h0wK/ec18gs60PKoSwRPcH8KWUcj2AlDJbSlkrpdQCH6M7/dVUjOlcedqg1bFLKTP1v3OADfoYsvVd1svd6pz2jktvJnBUSpmtj9Hk75eeMd+fujZCCAvAEcNP0VxDCHEPMBtYrD/VgP50RL7+8RF05637tFdcRv53M/b7ZQHMA76uF2+7vl8NfTZg4r+x7pwgDgGhQogg/TfURcDGtjyg/nzff4HTUsq36i33qrfZXODyCIuNwCL96IMgIBQ4qO9qlgghRun3eTfwQyvishVC2F9+jO4i50n98e/Rb3ZPvWO0S1z1XPHNztTvVz3GfH/q7+s2YOflD/brJYSYATwL3CKlLK+3vJcQwlz/uLc+rvPtGJcx/92MFpfeDcAZKWXd6Zn2fL8a+2zA1H9jzV3F7so/wE3oRgskAn9uh+ONQ9eliwVi9D83AZ8DJ/TLNwJe9dr8WR9fPPVG3gCR6P6DJQLvo78rvoVx9UY3IuI4cOrye4Hu/OQO4Jz+t0t7xqXfX08gH3Cst6zd3y90CSoLqEb3TWypMd8fwAbdKbQEdKNQercirgR055ov/41dHrkyX//vexw4CtzcznEZ7d/NmHHpl68Cll+1bXu+X419Npj0b0yV2lAURVEa1J1PMSmKoihNUAlCURRFaZBKEIqiKEqDVIJQFEVRGqQShKIoitIglSAURVGUBqkEoSiKojTo/wFGXsvR0y146wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None),\n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i)\n",
    "                                for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c41149",
   "metadata": {},
   "source": [
    "### 正则化\n",
    "### 标签平滑\n",
    "\n",
    "在训练过程中，我们使用的label平滑的值为$\\epsilon_{ls}=0.1$ [(cite)](https://arxiv.org/abs/1512.00567)。虽然对label进行平滑会让模型困惑，但提高了准确性和BLEU得分。\n",
    "\n",
    "> 我们使用KL div损失实现标签平滑。我们没有使用one-hot独热分布，而是创建了一个分布，该分布设定目标分布为1-smoothing，将剩余概率分配给词表中的其他单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1689b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.requires_grad_(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499c467",
   "metadata": {},
   "source": [
    "下面我们看一个例子，看看平滑后的真实概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2703a9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\56550\\AppData\\Local\\Temp/ipykernel_7752/410927135.py:19: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  mask = torch.nonzero(target.data == self.padding_idx)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x205cfce9130>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of label smoothing.\n",
    "crit = LabelSmoothing(5, 0, 0.4)\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "v = crit(predict.log(),\n",
    "         torch.LongTensor([2, 1, 0]))\n",
    "\n",
    "# Show the target distributions expected by the system.\n",
    "plt.imshow(crit.true_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "111b18b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
      "        [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(crit.true_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ca252",
   "metadata": {},
   "source": [
    "由于标签平滑的存在，如果模型对于某个单词特别有信心，输出特别大的概率，会被惩罚。如下代码所示，随着输入x的增大，x/d会越来越大，1/d会越来越小，但是loss并不是一直降低的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21f1002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x205cfd34b50>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "crit = LabelSmoothing(5, 0, 0.1)\n",
    "\n",
    "\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    # print(predict)\n",
    "    return crit(predict.log(),\n",
    "                torch.LongTensor([1])).item()\n",
    "\n",
    "\n",
    "y = [loss(x) for x in range(1, 100)]\n",
    "x = np.arange(1, 100)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2d9d4",
   "metadata": {},
   "source": [
    "## 实例\n",
    "### 合成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f2fc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = data.long().requires_grad_(False)\n",
    "        tgt = data.long().requires_grad_(False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab556f75",
   "metadata": {},
   "source": [
    "### 损失函数计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f8e28b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.item() * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ecfab8",
   "metadata": {},
   "source": [
    "### 贪婪解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9baac41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1 Loss: 2.964034 Tokens per Sec: 387.010254\n",
      "Epoch Step: 1 Loss: 1.944704 Tokens per Sec: 591.981201\n",
      "tensor(1.9819)\n",
      "Epoch Step: 1 Loss: 2.119860 Tokens per Sec: 405.909332\n",
      "Epoch Step: 1 Loss: 1.777609 Tokens per Sec: 599.199463\n",
      "tensor(1.7378)\n",
      "Epoch Step: 1 Loss: 1.928171 Tokens per Sec: 407.761536\n",
      "Epoch Step: 1 Loss: 1.672072 Tokens per Sec: 614.896240\n",
      "tensor(1.6674)\n",
      "Epoch Step: 1 Loss: 1.980943 Tokens per Sec: 414.652527\n",
      "Epoch Step: 1 Loss: 1.387953 Tokens per Sec: 598.535339\n",
      "tensor(1.4324)\n",
      "Epoch Step: 1 Loss: 1.557681 Tokens per Sec: 396.967346\n",
      "Epoch Step: 1 Loss: 1.232811 Tokens per Sec: 616.299866\n",
      "tensor(1.1763)\n",
      "Epoch Step: 1 Loss: 1.439810 Tokens per Sec: 401.095856\n",
      "Epoch Step: 1 Loss: 0.728257 Tokens per Sec: 603.893311\n",
      "tensor(0.7543)\n",
      "Epoch Step: 1 Loss: 1.051911 Tokens per Sec: 413.381775\n",
      "Epoch Step: 1 Loss: 0.540547 Tokens per Sec: 625.584290\n",
      "tensor(0.5310)\n",
      "Epoch Step: 1 Loss: 0.941762 Tokens per Sec: 417.215820\n",
      "Epoch Step: 1 Loss: 0.402128 Tokens per Sec: 632.918701\n",
      "tensor(0.4327)\n",
      "Epoch Step: 1 Loss: 0.792281 Tokens per Sec: 406.225067\n",
      "Epoch Step: 1 Loss: 0.345573 Tokens per Sec: 627.766785\n",
      "tensor(0.3450)\n",
      "Epoch Step: 1 Loss: 0.470235 Tokens per Sec: 419.160828\n",
      "Epoch Step: 1 Loss: 0.455055 Tokens per Sec: 631.438232\n",
      "tensor(0.3832)\n"
     ]
    }
   ],
   "source": [
    "# Train the simple copy task.\n",
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    run_epoch(data_gen(V, 30, 20), model, \n",
    "              SimpleLossCompute(model.generator, criterion, model_opt))\n",
    "    model.eval()\n",
    "    print(run_epoch(data_gen(V, 30, 5), model, \n",
    "                    SimpleLossCompute(model.generator, criterion, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08a674d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask,\n",
    "                           ys,\n",
    "                           subsequent_mask(ys.size(1)).type_as(src.data))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "model.eval()\n",
    "src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "src_mask = torch.ones(1, 1, 10)\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
