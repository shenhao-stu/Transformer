<p align="center">
  <a href="https://github.com/shenhao-stu/Transformer-In-CV">
  	<img src="output_img/logo.png" alt="Transformerâ€”In-CV" height=40% width=40%/>
  </a>
</p>

<p align="center">Python | shenhao0223@163.sufe.edu.cn | ä¸Šæµ·è´¢ç»å¤§å­¦ </p>

<h2 align="center">ğŸ³ğŸ³Transformer In CVğŸ³ğŸ³</h2>

- **Learner** : shenhao
- **Date** : 2021.10.14
- **Contact** : [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/421792538)

Transformer æ¨¡å‹åœ¨ 2017 å¹´è¢« google æå‡ºï¼Œç›´æ¥åŸºäº Self-Attention ç»“æ„ï¼Œå–ä»£äº†ä¹‹å‰ NLP ä»»åŠ¡ä¸­å¸¸ç”¨çš„ RNN ç¥ç»ç½‘ç»œç»“æ„ï¼Œå¹¶åœ¨ WMT2014 Englishto-German å’Œ WMT2014 English-to-French ä¸¤ä¸ªæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†å½“æ—¶çš„ SOTAã€‚

ä¸ RNN è¿™ç±»ç¥ç»ç½‘ç»œç»“æ„ç›¸æ¯”ï¼ŒTransformer ä¸€ä¸ªå·¨å¤§çš„ä¼˜ç‚¹æ˜¯ï¼š**æ¨¡å‹åœ¨å¤„ç†åºåˆ—è¾“å…¥æ—¶ï¼Œå¯ä»¥å¯¹æ•´ä¸ªåºåˆ—è¾“å…¥è¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œä¸éœ€è¦æŒ‰ç…§æ—¶é—´æ­¥å¾ªç¯é€’å½’å¤„ç†è¾“å…¥åºåˆ—ã€‚**

ä¸‹å›¾å…ˆä¾¿æ˜¯ Transformer æ•´ä½“ç»“æ„å›¾ï¼ŒTransformer æ¨¡å‹ç»“æ„ä¸­çš„å·¦åŠéƒ¨åˆ†ä¸º**ç¼–ç å™¨ï¼ˆencoderï¼‰**ï¼Œå³åŠéƒ¨åˆ†ä¸º**è§£ç å™¨ï¼ˆdecoderï¼‰**ï¼Œä¸‹é¢æˆ‘ä»¬æ¥ä¸€æ­¥æ­¥æ‹†è§£ Transformerã€‚

<img src="output_img/0-1-transformer-arc.png" alt="png" style="zoom:50%;" />

## å¯¼å…¥éœ€è¦çš„åº“

```python
import math, copy, time
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
```

```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Using {} device'.format(device))
```

    Using cpu device

## Embeddings

å’Œå¸¸è§çš„ NLP ä»»åŠ¡ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆä¼šä½¿ç”¨è¯åµŒå…¥ç®—æ³•ï¼ˆembedding algorithmï¼‰ï¼Œå°†è¾“å…¥æ–‡æœ¬åºåˆ—çš„æ¯ä¸ªè¯è½¬æ¢ä¸ºä¸€ä¸ªè¯å‘é‡ã€‚å®é™…åº”ç”¨ä¸­çš„å‘é‡ä¸€èˆ¬æ˜¯ 256 æˆ–è€… 512 ç»´ã€‚ä½†ä¸ºäº†ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬è¿™é‡Œä½¿ç”¨ 4 ç»´çš„è¯å‘é‡æ¥è¿›è¡Œè®²è§£ã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå‡è®¾æˆ‘ä»¬çš„è¾“å…¥æ–‡æœ¬æ˜¯åºåˆ—åŒ…å«äº† 3 ä¸ªè¯ï¼Œé‚£ä¹ˆæ¯ä¸ªè¯å¯ä»¥é€šè¿‡è¯åµŒå…¥ç®—æ³•å¾—åˆ°ä¸€ä¸ª 4 ç»´å‘é‡ï¼Œäºæ˜¯æ•´ä¸ªè¾“å…¥è¢«è½¬åŒ–æˆä¸ºä¸€ä¸ªå‘é‡åºåˆ—ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåŒæ—¶ç»™æ¨¡å‹è¾“å…¥å¤šä¸ªå¥å­ï¼Œå¦‚æœæ¯ä¸ªå¥å­çš„é•¿åº¦ä¸ä¸€æ ·ï¼Œæˆ‘ä»¬ä¼šé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„é•¿åº¦ï¼Œä½œä¸ºè¾“å…¥æ–‡æœ¬åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼šå¦‚æœä¸€ä¸ªå¥å­è¾¾ä¸åˆ°è¿™ä¸ªé•¿åº¦ï¼Œé‚£ä¹ˆå°±å¡«å……å…ˆå¡«å……ä¸€ä¸ªç‰¹æ®Šçš„â€œpaddingâ€è¯ï¼›å¦‚æœå¥å­è¶…å‡ºè¿™ä¸ªé•¿åº¦ï¼Œåˆ™åšæˆªæ–­ã€‚æœ€å¤§åºåˆ—é•¿åº¦æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œé€šå¸¸å¸Œæœ›è¶Šå¤§è¶Šå¥½ï¼Œä½†æ˜¯æ›´é•¿çš„åºåˆ—å¾€å¾€ä¼šå ç”¨æ›´å¤§çš„è®­ç»ƒæ˜¾å­˜/å†…å­˜ï¼Œå› æ­¤éœ€è¦åœ¨æ¨¡å‹è®­ç»ƒæ—¶å€™è§†æƒ…å†µè¿›è¡Œå†³å®šã€‚

![3ä¸ªè¯å’Œå¯¹åº”çš„è¯å‘é‡](output_img/2-x.png)

> è¾“å…¥åºåˆ—æ¯ä¸ªå•è¯è¢«è½¬æ¢æˆ**è¯å‘é‡**è¡¨ç¤ºè¿˜å°†åŠ ä¸Š**ä½ç½®å‘é‡**æ¥å¾—åˆ°è¯¥è¯çš„æœ€ç»ˆå‘é‡è¡¨ç¤ºã€‚

ä¸å…¶ä»– seq2seq æ¨¡å‹ç±»ä¼¼ï¼Œæˆ‘ä»¬ä½¿ç”¨å­¦ä¹ åˆ°çš„ embedding å°†è¾“å…¥ token å’Œè¾“å‡º token è½¬æ¢ä¸º$d_{\text{model}}$ç»´çš„å‘é‡ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æ™®é€šçš„çº¿æ€§å˜æ¢å’Œ softmax å‡½æ•°å°†è§£ç å™¨è¾“å‡ºè½¬æ¢ä¸ºé¢„æµ‹çš„ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡ åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œä¸¤ä¸ªåµŒå…¥å±‚ä¹‹é—´å’Œ pre-softmax çº¿æ€§å˜æ¢å…±äº«ç›¸åŒçš„æƒé‡çŸ©é˜µï¼Œç±»ä¼¼äº[(cite)](https://arxiv.org/abs/1608.05859)ã€‚åœ¨ embedding å±‚ä¸­ï¼Œæˆ‘ä»¬å°†è¿™äº›æƒé‡ä¹˜ä»¥$\sqrt{d_{\text{model}}}$ã€‚

```python
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        """
        ç±»çš„åˆå§‹åŒ–å‡½æ•°
        d_modelï¼šæŒ‡è¯åµŒå…¥çš„ç»´åº¦
        vocab:æŒ‡è¯è¡¨çš„å¤§å°
        """
        super(Embeddings, self).__init__()
        # ä¹‹åå°±æ˜¯è°ƒç”¨nnä¸­çš„é¢„å®šä¹‰å±‚Embeddingï¼Œè·å¾—ä¸€ä¸ªè¯åµŒå…¥å¯¹è±¡self.lut
        self.lut = nn.Embedding(vocab, d_model)
        # æœ€åå°±æ˜¯å°†d_modelä¼ å…¥ç±»ä¸­
        self.d_model = d_model

    def forward(self, x):
        """
        Embeddingå±‚çš„å‰å‘ä¼ æ’­é€»è¾‘
        å‚æ•°xï¼šè¿™é‡Œä»£è¡¨è¾“å…¥ç»™æ¨¡å‹çš„å•è¯æ–‡æœ¬é€šè¿‡è¯è¡¨æ˜ å°„åçš„one-hotå‘é‡
        å°†xä¼ ç»™self.lutå¹¶ä¸æ ¹å·ä¸‹self.d_modelç›¸ä¹˜ä½œä¸ºç»“æœè¿”å›
        """
        embedds = self.lut(x)
        return embedds * math.sqrt(self.d_model)
```

### æœ‰å…³ Embeddings å‡½æ•°çš„æµ‹è¯•

```python
# embedding_size=16, input_vocab_size=10
embedding_fc = Embeddings(16, 10).to(device)
print(embedding_fc)
```

    Embeddings(
      (lut): Embedding(10, 16)
    )

```python
# sentence or batch_size=1,words=5
input_X = torch.randint(0, 10, (1, 5))
input_X, input_X.shape
```

    (tensor([[4, 6, 5, 1, 2]]), torch.Size([1, 5]))

```python
embeds_x = embedding_fc(input_X)
embeds_x.shape
```

    torch.Size([1, 5, 16])

## Positional Encodding

Positional Encodding ä½ç½®ç¼–ç çš„ä½œç”¨æ˜¯ä¸ºæ¨¡å‹æä¾›å½“å‰æ—¶é—´æ­¥çš„å‰åå‡ºç°é¡ºåºçš„ä¿¡æ¯.å› ä¸º Transformer ä¸åƒ RNN é‚£æ ·çš„å¾ªç¯ç»“æ„æœ‰å‰åä¸åŒæ—¶é—´æ­¥è¾“å…¥é—´å¤©ç„¶çš„å…ˆåé¡ºåº,æ‰€æœ‰çš„æ—¶é—´æ­¥æ˜¯åŒæ—¶è¾“å…¥,å¹¶è¡Œæ¨ç†çš„,å› æ­¤åœ¨æ—¶é—´æ­¥çš„ç‰¹å¾ä¸­èåˆè¿›ä½ç½®ç¼–ç çš„ä¿¡æ¯æ˜¯åˆç†çš„.

ä½ç½®ç¼–ç å¯ä»¥æœ‰å¾ˆå¤šé€‰æ‹©,å¯ä»¥æ˜¯å›ºå®šçš„,ä¹Ÿå¯ä»¥è®¾ç½®æˆå¯å­¦ä¹ çš„å‚æ•°.

è¿™é‡Œ,æˆ‘ä»¬ä½¿ç”¨å›ºå®šçš„ä½ç½®ç¼–ç .å…·ä½“åœ°,ä½¿ç”¨ä¸åŒé¢‘ç‡çš„ sin å’Œ cos å‡½æ•°æ¥è¿›è¡Œä½ç½®ç¼–ç ,å¦‚ä¸‹æ‰€ç¤º:
$$PE_{pos,2i}=sin(pos/10000^{2i/d_{model}})$$  
$$PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}})$$

å…¶ä¸­ pos ä»£è¡¨æ—¶é—´æ­¥çš„ä¸‹æ ‡ç´¢å¼•,å‘é‡$PE_{pos}$ä¹Ÿå°±æ˜¯ç¬¬ pos ä¸ªæ—¶é—´æ­¥çš„ä½ç½®ç¼–ç ,ç¼–ç é•¿åº¦åŒ Embedding å±‚.

ä¸‹å›¾ä¸º$pos=1$ï¼Œ$d_{model}=128$çš„ $sin$ å’Œ $cos$ çš„å‡½æ•°å›¾åƒï¼Œå¯ä»¥çœ‹åˆ°å½“ $x$ å¢å¤§çš„æ—¶å€™ï¼Œcos->1,sin->0ã€‚ç»´åº¦ä¸Šéšç€ç»´åº¦åºå·å¢å¤§ï¼Œå‘¨æœŸå˜åŒ–ä¼šè¶Šæ¥è¶Šæ…¢ï¼Œè€Œäº§ç”Ÿä¸€ç§åŒ…å«ä½ç½®ä¿¡æ¯çš„çº¹ç†ã€‚

![fc](output_img/fc.png)

**æˆ‘ä»¬å¯ä»¥ç”¨ä»£ç ç®€å•çœ‹ä¸€ä¸‹æ•ˆæœ**

```python
# å¯¼å…¥ä¾èµ–åº“
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math


def get_positional_encoding(max_seq_len, embed_dim):
    # åˆå§‹åŒ–ä¸€ä¸ªpositional encoding
    # embed_dim: å­—åµŒå…¥çš„ç»´åº¦
    # max_seq_len: æœ€å¤§çš„åºåˆ—é•¿åº¦
    positional_encoding = np.array([
        [pos / np.power(10000, 2 * i / embed_dim) for i in range(embed_dim)]
        if pos != 0 else np.zeros(embed_dim) for pos in range(max_seq_len)])
    positional_encoding[1:, 0::2] = np.sin(
        positional_encoding[1:, 0::2])  # dim 2i å¶æ•°
    positional_encoding[1:, 1::2] = np.cos(
        positional_encoding[1:, 1::2])  # dim 2i+1 å¥‡æ•°
    # å½’ä¸€åŒ–, ç”¨ä½ç½®åµŒå…¥çš„æ¯ä¸€è¡Œé™¤ä»¥å®ƒçš„æ¨¡é•¿
    # denominator = np.sqrt(np.sum(position_enc**2, axis=1, keepdims=True))
    # position_enc = position_enc / (denominator + 1e-8)
    return positional_encoding

positional_encoding = get_positional_encoding(max_seq_len=100, embed_dim=16)
plt.figure(figsize=(10, 10))
sns.heatmap(positional_encoding)
plt.title("Sinusoidal Function")
plt.xlabel("hidden dimension")
plt.ylabel("sequence length")
```

<img src="output_img/pos_enc.png" alt="png" style="zoom:50%;" />

å…·ä½“çš„æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![png](output_img/2-position2.png)

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        """
        ä½ç½®ç¼–ç å™¨ç±»çš„åˆå§‹åŒ–å‡½æ•°

        å…±æœ‰ä¸‰ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯
        d_modelï¼šè¯åµŒå…¥ç»´åº¦
        dropout: dropoutè§¦å‘æ¯”ç‡
        max_lenï¼šæ¯ä¸ªå¥å­çš„æœ€å¤§é•¿åº¦
        """
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Compute the positional encodings
        # æ³¨æ„ä¸‹é¢ä»£ç çš„è®¡ç®—æ–¹å¼ä¸å…¬å¼ä¸­ç»™å‡ºçš„æ˜¯ä¸åŒçš„ï¼Œä½†æ˜¯æ˜¯ç­‰ä»·çš„ï¼Œä½ å¯ä»¥å°è¯•ç®€å•æ¨å¯¼è¯æ˜ä¸€ä¸‹ã€‚
        # è¿™æ ·è®¡ç®—æ˜¯ä¸ºäº†é¿å…ä¸­é—´çš„æ•°å€¼è®¡ç®—ç»“æœè¶…å‡ºfloatçš„èŒƒå›´ï¼Œ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)].requires_grad_(False)
        return self.dropout(x)
```

```python
# embedding_size=15,dropout=0.1
position_encoding_layer = PositionalEncoding(16, 0.1)
```

```python
position_enc_x = position_encoding_layer(embeds_x)
position_enc_x.shape
```

    torch.Size([1, 5, 16])

## æ©ç åŠå…¶ä½œç”¨

**æ©ç ï¼š** æ©ä»£è¡¨é®æ©ï¼Œç å°±æ˜¯æˆ‘ä»¬å¼ é‡ä¸­çš„æ•°å€¼ï¼Œå®ƒçš„å°ºå¯¸ä¸å®šï¼Œé‡Œé¢ä¸€èˆ¬åªæœ‰ 0 å’Œ 1ï¼›ä»£è¡¨ä½ç½®è¢«é®æ©æˆ–è€…ä¸è¢«é®æ©ã€‚

æ©ç çš„ä½œç”¨ï¼šåœ¨ transformer ä¸­ï¼Œæ©ç ä¸»è¦çš„ä½œç”¨æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯å±è”½æ‰æ— æ•ˆçš„ padding åŒºåŸŸï¼Œä¸€ä¸ªæ˜¯å±è”½æ‰æ¥è‡ªâ€œæœªæ¥â€çš„ä¿¡æ¯ã€‚Encoder ä¸­çš„æ©ç ä¸»è¦æ˜¯èµ·åˆ°ç¬¬ä¸€ä¸ªä½œç”¨ï¼ŒDecoder ä¸­çš„æ©ç åˆ™åŒæ—¶å‘æŒ¥ç€ä¸¤ç§ä½œç”¨ã€‚

å±è”½æ‰æ— æ•ˆçš„ padding åŒºåŸŸï¼šæˆ‘ä»¬è®­ç»ƒéœ€è¦ç»„ batch è¿›è¡Œï¼Œå°±ä»¥æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸ºä¾‹ï¼Œä¸€ä¸ª batch ä¸­ä¸åŒæ ·æœ¬çš„è¾“å…¥é•¿åº¦å¾ˆå¯èƒ½æ˜¯ä¸ä¸€æ ·çš„ï¼Œæ­¤æ—¶æˆ‘ä»¬è¦è®¾ç½®ä¸€ä¸ªæœ€å¤§å¥å­é•¿åº¦ï¼Œç„¶åå¯¹ç©ºç™½åŒºåŸŸè¿›è¡Œ padding å¡«å……ï¼Œè€Œå¡«å……çš„åŒºåŸŸæ— è®ºåœ¨ Encoder è¿˜æ˜¯ Decoder çš„è®¡ç®—ä¸­éƒ½æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œå› æ­¤éœ€è¦ç”¨ mask è¿›è¡Œæ ‡è¯†ï¼Œå±è”½æ‰å¯¹åº”åŒºåŸŸçš„å“åº”ã€‚

å±è”½æ‰æ¥è‡ªæœªæ¥çš„ä¿¡æ¯ï¼šæˆ‘ä»¬å·²ç»å­¦ä¹ äº† attention çš„è®¡ç®—æµç¨‹ï¼Œå®ƒæ˜¯ä¼šç»¼åˆæ‰€æœ‰æ—¶é—´æ­¥çš„è®¡ç®—çš„ï¼Œé‚£ä¹ˆåœ¨è§£ç çš„æ—¶å€™ï¼Œå°±æœ‰å¯èƒ½è·å–åˆ°æœªæ¥çš„ä¿¡æ¯ï¼Œè¿™æ˜¯ä¸è¡Œçš„ã€‚å› æ­¤ï¼Œè¿™ç§æƒ…å†µä¹Ÿéœ€è¦æˆ‘ä»¬ä½¿ç”¨ mask è¿›è¡Œå±è”½ã€‚ç°åœ¨è¿˜æ²¡ä»‹ç»åˆ° Decoderï¼Œå¦‚æœæ²¡å®Œå…¨ç†è§£ï¼Œå¯ä»¥ä¹‹åå†å›è¿‡å¤´æ¥æ€è€ƒä¸‹ã€‚

mask çš„æ„é€ ä»£ç å¦‚ä¸‹ï¼š

```python
def subsequent_mask(size):
    # ç”Ÿæˆå‘åé®æ©çš„æ©ç å¼ é‡ï¼Œå‚æ•°sizeæ˜¯æ©ç å¼ é‡æœ€åä¸¤ä¸ªç»´åº¦çš„å¤§å°ï¼Œå®ƒæœ€åä¸¤ç»´å½¢æˆä¸€ä¸ªæ–¹é˜µ
    "Mask out subsequent positions."
    attn_shape = (1, size, size)

    # ç„¶åä½¿ç”¨np.onesæ–¹æ³•å‘è¿™ä¸ªå½¢çŠ¶ä¸­æ·»åŠ 1å…ƒç´ ï¼Œå½¢æˆä¸Šä¸‰è§’é˜µ
    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')

    # æœ€åå°†numpyç±»å‹è½¬åŒ–ä¸ºtorchä¸­çš„tensorï¼Œå†…éƒ¨åšä¸€ä¸ª1- çš„æ“ä½œã€‚è¿™ä¸ªå…¶å®æ˜¯åšäº†ä¸€ä¸ªä¸‰è§’é˜µçš„åè½¬ï¼Œsubsequent_maskä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½ä¼šè¢«1å‡ã€‚
    # å¦‚æœæ˜¯0ï¼Œsubsequent_maskä¸­çš„è¯¥ä½ç½®ç”±0å˜æˆ1
    # å¦‚æœæ˜¯1ï¼Œsubsequect_maskä¸­çš„è¯¥ä½ç½®ç”±1å˜æˆ0
    return torch.from_numpy(subsequent_mask) == 0
```

```python
attn_shape = (1, 10, 10)
# np.triu(m, k=0)
# kæ˜¯æŒ‡ä»ä¸»å¯¹è§’çº¿å¼€å§‹ä¿ç•™
# k=0
# [[1. 1. 1. 1. 1.]
#  [0. 1. 1. 1. 1.]
#  [0. 0. 1. 1. 1.]
#  [0. 0. 0. 1. 1.]
#  [0. 0. 0. 0. 1.]]
# k=1
# [[0. 1. 1. 1. 1.]
#  [0. 0. 1. 1. 1.]
#  [0. 0. 0. 1. 1.]
#  [0. 0. 0. 0. 1.]
#  [0. 0. 0. 0. 0.]]
# k=2
# [[0. 0. 1. 1. 1.]
#  [0. 0. 0. 1. 1.]
#  [0. 0. 0. 0. 1.]
#  [0. 0. 0. 0. 0.]
#  [0. 0. 0. 0. 0.]]
subseq_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')
subseq_mask
```

    array([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],
            [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
            [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
            [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
            [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
            [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],
            [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
            [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=uint8)

```python
torch.from_numpy(subseq_mask) == 0
```

    tensor([[[ True, False, False, False, False, False, False, False, False, False],
             [ True,  True, False, False, False, False, False, False, False, False],
             [ True,  True,  True, False, False, False, False, False, False, False],
             [ True,  True,  True,  True, False, False, False, False, False, False],
             [ True,  True,  True,  True,  True, False, False, False, False, False],
             [ True,  True,  True,  True,  True,  True, False, False, False, False],
             [ True,  True,  True,  True,  True,  True,  True, False, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
             [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],
             [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])

```python
import matplotlib.pyplot as plt
plt.figure(figsize=(5,5))
plt.imshow(subsequent_mask(20)[0])
```

![png](output_img/output_16_1.png)
â€‹

## è§„èŒƒåŒ–å±‚

è§„èŒƒåŒ–å±‚çš„ä½œç”¨ï¼šå®ƒæ˜¯æ‰€æœ‰æ·±å±‚ç½‘ç»œæ¨¡å‹éƒ½éœ€è¦çš„æ ‡å‡†ç½‘ç»œå±‚ï¼Œå› ä¸ºéšç€ç½‘ç»œå±‚æ•°çš„å¢åŠ ï¼Œé€šè¿‡å¤šå±‚çš„è®¡ç®—åè¾“å‡ºå¯èƒ½å¼€å§‹å‡ºç°è¿‡å¤§æˆ–è¿‡å°çš„æƒ…å†µï¼Œè¿™æ ·å¯èƒ½ä¼šå¯¼è‡´å­¦ä¹ è¿‡ç¨‹å‡ºç°å¼‚å¸¸ï¼Œæ¨¡å‹å¯èƒ½æ”¶æ•›éå¸¸æ…¢ã€‚å› æ­¤éƒ½ä¼šåœ¨ä¸€å®šå±‚åæ¥è§„èŒƒåŒ–å±‚è¿›è¡Œæ•°å€¼çš„è§„èŒƒåŒ–ï¼Œä½¿å…¶ç‰¹å¾æ•°å€¼åœ¨åˆç†èŒƒå›´å†…ã€‚

Transformer ä¸­ä½¿ç”¨çš„ normalization æ‰‹æ®µæ˜¯ layer normï¼Œå®ç°ä»£ç å¾ˆç®€å•ï¼Œå¦‚ä¸‹ï¼š

```python
class LayerNorm(nn.Module):
    "Construct a layernorm module (See citation for details)."

    def __init__(self, feature_size, eps=1e-6):
        # åˆå§‹åŒ–å‡½æ•°æœ‰ä¸¤ä¸ªå‚æ•°ï¼Œä¸€ä¸ªæ˜¯features,è¡¨ç¤ºè¯åµŒå…¥çš„ç»´åº¦,å¦ä¸€ä¸ªæ˜¯epså®ƒæ˜¯ä¸€ä¸ªè¶³å¤Ÿå°çš„æ•°ï¼Œåœ¨è§„èŒƒåŒ–å…¬å¼çš„åˆ†æ¯ä¸­å‡ºç°,é˜²æ­¢åˆ†æ¯ä¸º0ï¼Œé»˜è®¤æ˜¯1e-6ã€‚
        super(LayerNorm, self).__init__()
        # æ ¹æ®featuresçš„å½¢çŠ¶åˆå§‹åŒ–ä¸¤ä¸ªå‚æ•°å¼ é‡a2ï¼Œå’Œb2ï¼Œç¬¬ä¸€åˆå§‹åŒ–ä¸º1å¼ é‡ï¼Œä¹Ÿå°±æ˜¯é‡Œé¢çš„å…ƒç´ éƒ½æ˜¯1ï¼Œç¬¬äºŒä¸ªåˆå§‹åŒ–ä¸º0å¼ é‡ï¼Œä¹Ÿå°±æ˜¯é‡Œé¢çš„å…ƒç´ éƒ½æ˜¯0ï¼Œè¿™ä¸¤ä¸ªå¼ é‡å°±æ˜¯è§„èŒƒåŒ–å±‚çš„å‚æ•°ã€‚å› ä¸ºç›´æ¥å¯¹ä¸Šä¸€å±‚å¾—åˆ°çš„ç»“æœåšè§„èŒƒåŒ–å…¬å¼è®¡ç®—ï¼Œå°†æ”¹å˜ç»“æœçš„æ­£å¸¸è¡¨å¾ï¼Œå› æ­¤å°±éœ€è¦æœ‰å‚æ•°ä½œä¸ºè°ƒèŠ‚å› å­ï¼Œä½¿å…¶å³èƒ½æ»¡è¶³è§„èŒƒåŒ–è¦æ±‚ï¼Œåˆèƒ½ä¸æ”¹å˜é’ˆå¯¹ç›®æ ‡çš„è¡¨å¾ï¼Œæœ€åä½¿ç”¨nn.parameterå°è£…ï¼Œä»£è¡¨ä»–ä»¬æ˜¯æ¨¡å‹çš„å‚æ•°
        self.a_2 = nn.Parameter(torch.ones(feature_size))
        self.b_2 = nn.Parameter(torch.zeros(feature_size))
        # æŠŠepsä¼ åˆ°ç±»ä¸­
        self.eps = eps

    def forward(self, x):
        # è¾“å…¥å‚æ•°xä»£è¡¨æ¥è‡ªä¸Šä¸€å±‚çš„è¾“å‡ºï¼Œåœ¨å‡½æ•°ä¸­ï¼Œé¦–å…ˆå¯¹è¾“å…¥å˜é‡xæ±‚å…¶æœ€åä¸€ä¸ªç»´åº¦çš„å‡å€¼ï¼Œå¹¶ä¿æŒè¾“å‡ºç»´åº¦ä¸è¾“å…¥ç»´åº¦ä¸€è‡´ï¼Œæ¥ç€å†æ±‚æœ€åä¸€ä¸ªç»´åº¦çš„æ ‡å‡†å·®ï¼Œç„¶åå°±æ˜¯æ ¹æ®è§„èŒƒåŒ–å…¬å¼ï¼Œç”¨xå‡å»å‡å€¼é™¤ä»¥æ ‡å‡†å·®è·å¾—è§„èŒƒåŒ–çš„ç»“æœã€‚
        # æœ€åå¯¹ç»“æœä¹˜ä»¥æˆ‘ä»¬çš„ç¼©æ”¾å‚æ•°ï¼Œå³a2,*å·ä»£è¡¨åŒå‹ç‚¹ä¹˜ï¼Œå³å¯¹åº”ä½ç½®è¿›è¡Œä¹˜æ³•æ“ä½œï¼ŒåŠ ä¸Šä½ç§»å‚b2ï¼Œè¿”å›å³å¯
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
```

```python
# embedding_size=16
layer_norm = LayerNorm(16)
norm_x = layer_norm(position_enc_x)
position_enc_x, norm_x
```

    (tensor([[[-0.0000e+00, -3.6160e+00, -3.0979e-01, -0.0000e+00, -6.0106e+00,
               -7.8812e+00,  1.1963e+01,  3.3401e-01, -5.2540e+00, -3.0309e+00,
                4.7534e+00,  8.1180e-01,  6.2579e-01,  7.4930e+00,  4.5049e+00,
                1.8610e+00],
              [-5.2908e-01, -0.0000e+00, -2.6326e-01, -1.1667e+00, -4.5975e+00,
               -7.8027e+00, -1.0195e-02,  2.9557e+00,  5.6879e-01,  4.5661e+00,
               -1.3931e+00, -3.0531e+00,  2.2437e+00,  2.2772e+00,  2.5196e+00,
               -2.9457e-01],
              [-1.9345e+00, -6.6504e+00, -4.5080e-01, -7.0320e-01,  1.5061e+00,
                1.0495e+01, -0.0000e+00,  1.7928e+00,  2.1457e+00, -0.0000e+00,
               -0.0000e+00, -7.5229e+00, -2.6902e+00,  1.7529e+00, -2.2068e+00,
               -2.5872e+00],
              [-1.1563e+00, -7.1341e+00,  3.6549e+00,  1.6422e+00, -8.7020e+00,
                4.2414e+00,  2.2792e-01, -4.1304e+00, -2.0449e-01, -1.1205e+00,
                1.9175e-01,  1.4125e+00,  1.6505e+00,  3.3348e+00,  5.8560e+00,
               -2.7107e+00],
              [-6.3058e-01, -0.0000e+00, -6.4573e-01, -3.2826e+00,  8.2511e+00,
               -6.4506e-01, -6.7896e-01,  5.1568e+00, -2.6388e+00,  0.0000e+00,
               -3.2014e+00,  0.0000e+00, -1.5666e+00, -2.1916e+00,  0.0000e+00,
                1.7595e+00]]], grad_fn=<MulBackward0>),
     tensor([[[-7.6292e-02, -7.8315e-01, -1.3685e-01, -7.6292e-02, -1.2513e+00,
               -1.6169e+00,  2.2622e+00, -1.0999e-02, -1.1033e+00, -6.6877e-01,
                8.5291e-01,  8.2399e-02,  4.6039e-02,  1.3885e+00,  8.0433e-01,
                2.8750e-01],
              [-9.1594e-02,  8.1241e-02, -4.7592e-03, -2.9987e-01, -1.4206e+00,
               -2.4676e+00,  7.7910e-02,  1.0468e+00,  2.6705e-01,  1.5728e+00,
               -3.7384e-01, -9.1610e-01,  8.1417e-01,  8.2512e-01,  9.0432e-01,
               -1.4987e-02],
              [-3.7014e-01, -1.5388e+00, -2.4632e-03, -6.5011e-02,  4.8250e-01,
                2.7100e+00,  1.0925e-01,  5.5353e-01,  6.4100e-01,  1.0925e-01,
                1.0925e-01, -1.7550e+00, -5.5743e-01,  5.4364e-01, -4.3763e-01,
               -5.3188e-01],
              [-2.4436e-01, -1.7470e+00,  9.6504e-01,  4.5909e-01, -2.1411e+00,
                1.1125e+00,  1.0358e-01, -9.9197e-01, -5.1123e-03, -2.3538e-01,
                9.4490e-02,  4.0134e-01,  4.6118e-01,  8.8458e-01,  1.5183e+00,
               -6.3511e-01],
              [-2.0388e-01,  6.5466e-03, -2.0894e-01, -1.0889e+00,  2.7600e+00,
               -2.0871e-01, -2.2002e-01,  1.7274e+00, -8.7403e-01,  6.5466e-03,
               -1.0618e+00,  6.5466e-03, -5.1622e-01, -7.2481e-01,  6.5466e-03,
                5.9370e-01]]], grad_fn=<AddBackward0>))

## Attention

Attention åŠŸèƒ½å¯ä»¥æè¿°ä¸ºå°† query å’Œä¸€ç»„ key-value æ˜ å°„åˆ°è¾“å‡ºï¼Œå…¶ä¸­ queryã€keyã€value å’Œè¾“å‡ºéƒ½æ˜¯å‘é‡ã€‚è¾“å‡ºä¸º value çš„åŠ æƒå’Œï¼Œå…¶ä¸­æ¯ä¸ª value çš„æƒé‡é€šè¿‡ query ä¸ç›¸åº” key çš„è®¡ç®—å¾—åˆ°ã€‚  
æˆ‘ä»¬å°† particular attention ç§°ä¹‹ä¸ºâ€œç¼©æ”¾çš„ç‚¹ç§¯ Attentionâ€(Scaled Dot-Product Attention")ã€‚å…¶è¾“å…¥ä¸º queryã€key(ç»´åº¦æ˜¯$d_k$)ä»¥åŠ values(ç»´åº¦æ˜¯$d_v$)ã€‚æˆ‘ä»¬è®¡ç®— query å’Œæ‰€æœ‰ key çš„ç‚¹ç§¯ï¼Œç„¶åå¯¹æ¯ä¸ªé™¤ä»¥ $\sqrt{d_k}$, æœ€åç”¨ softmax å‡½æ•°è·å¾— value çš„æƒé‡ã€‚

```python
def attention(query, key, value, mask=None, dropout=None):
    "Compute 'Scaled Dot Product Attention'"
    # query,key,valueå‡ä¸º[batch_size,sentence_len,embedding_size]
    # é¦–å…ˆå–queryçš„æœ€åä¸€ç»´çš„å¤§å°ï¼Œå¯¹åº”è¯åµŒå…¥ç»´åº¦
    d_k = query.size(-1)
    # æŒ‰ç…§æ³¨æ„åŠ›å…¬å¼ï¼Œå°†queryä¸keyçš„è½¬ç½®ç›¸ä¹˜ï¼Œè¿™é‡Œé¢keyæ˜¯å°†æœ€åä¸¤ä¸ªç»´åº¦è¿›è¡Œè½¬ç½®ï¼Œå†é™¤ä»¥ç¼©æ”¾ç³»æ•°å¾—åˆ°æ³¨æ„åŠ›å¾—åˆ†å¼ é‡scores
    # src:scores.shape=[30,8,10,10] tgt:[30,8,9,9]
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)

    # æ¥ç€åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ©ç å¼ é‡
    if mask is not None:
        # ä½¿ç”¨tensorçš„masked_fillæ–¹æ³•ï¼Œå°†æ©ç å¼ é‡å’Œscoreså¼ é‡æ¯ä¸ªä½ç½®ä¸€ä¸€æ¯”è¾ƒï¼Œå¦‚æœæ©ç å¼ é‡åˆ™å¯¹åº”çš„scoreså¼ é‡ç”¨-1e9è¿™ä¸ªç½®æ¥æ›¿æ¢
        scores = scores.masked_fill(mask == 0, -1e9)

    # å¯¹scoresçš„æœ€åä¸€ç»´è¿›è¡Œsoftmaxæ“ä½œï¼Œä½¿ç”¨F.softmaxæ–¹æ³•ï¼Œè¿™æ ·è·å¾—æœ€ç»ˆçš„æ³¨æ„åŠ›å¼ é‡
    p_attn = F.softmax(scores, dim=-1)

    # ä¹‹ååˆ¤æ–­æ˜¯å¦ä½¿ç”¨dropoutè¿›è¡Œéšæœºç½®0
    if dropout is not None:
        p_attn = dropout(p_attn)

    # æœ€åï¼Œæ ¹æ®å…¬å¼å°†p_attnä¸valueå¼ é‡ç›¸ä¹˜è·å¾—æœ€ç»ˆçš„queryæ³¨æ„åŠ›è¡¨ç¤ºï¼ŒåŒæ—¶è¿”å›æ³¨æ„åŠ›å¼ é‡
    return torch.matmul(p_attn, value), p_attn
```

```python
position_enc_x.shape
```

    torch.Size([1, 5, 16])

```python
# queryçš„æœ€åä¸€ç»´çš„å¤§å°ä¸ºembedding_size,ç†è®ºä¸Šq,k,vé€šè¿‡input_xçº¿æ€§è½¬æ¢åå¾—æ¥çš„.
query, key, value = [torch.randn((1, 5, 16)) for i in range(3)]
output_x = attention(query,key,value)
output_x[0].shape,output_x[1].shape
# p_attn
# [[a11,a12,a13,a14,a15],
#  [a21,a22,a23,a24,a25],
#  [a31,a32,a33,a34,a35],
#  [a41,a42,a43,a44,a45],
#  [a51,a52,a53,a54,a55]]
# å…¶ä¸­a(i,j)è¡¨ç¤ºç¬¬iä¸ªå•è¯å¯¹ç¬¬jä¸ªå•è¯çš„æ³¨æ„åŠ›ç¨‹åº¦.
```

    (torch.Size([1, 5, 16]), torch.Size([1, 5, 5]))

## å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

Transformer çš„è®ºæ–‡é€šè¿‡å¢åŠ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¸€ç»„æ³¨æ„åŠ›ç§°ä¸ºä¸€ä¸ª attention headï¼‰ï¼Œè¿›ä¸€æ­¥å®Œå–„äº† Self-Attentionã€‚è¿™ç§æœºåˆ¶ä»å¦‚ä¸‹ä¸¤ä¸ªæ–¹é¢å¢å¼ºäº† attention å±‚çš„èƒ½åŠ›ï¼š

- **å®ƒæ‰©å±•äº†æ¨¡å‹å…³æ³¨ä¸åŒä½ç½®çš„èƒ½åŠ›**ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œç¬¬ä¸€ä¸ªä½ç½®çš„è¾“å‡º $z_1$ åŒ…å«äº†å¥å­ä¸­å…¶ä»–æ¯ä¸ªä½ç½®çš„å¾ˆå°ä¸€éƒ¨åˆ†ä¿¡æ¯ï¼Œä½†$z_1$â€‹ ä»…ä»…æ˜¯å•ä¸ªå‘é‡ï¼Œæ‰€ä»¥å¯èƒ½ä»…ç”±ç¬¬ 1 ä¸ªä½ç½®çš„ä¿¡æ¯ä¸»å¯¼äº†ã€‚è€Œå½“æˆ‘ä»¬ç¿»è¯‘å¥å­ï¼š`The animal didnâ€™t cross the street because it was too tired`æ—¶ï¼Œæˆ‘ä»¬ä¸ä»…å¸Œæœ›æ¨¡å‹å…³æ³¨åˆ°"it"æœ¬èº«ï¼Œè¿˜å¸Œæœ›æ¨¡å‹å…³æ³¨åˆ°"The"å’Œ"animal"ï¼Œç”šè‡³å…³æ³¨åˆ°"tired"ã€‚è¿™æ—¶ï¼Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¼šæœ‰å¸®åŠ©ã€‚
- **å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶èµ‹äºˆ attention å±‚å¤šä¸ªâ€œå­è¡¨ç¤ºç©ºé—´â€**ã€‚ä¸‹é¢æˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¼šæœ‰å¤šç»„ $W^Q, W^K W^V$ çš„æƒé‡çŸ©é˜µï¼ˆåœ¨ Transformer çš„è®ºæ–‡ä¸­ï¼Œä½¿ç”¨äº† 8 ç»„æ³¨æ„åŠ›),ï¼Œå› æ­¤å¯ä»¥å°† $X$ å˜æ¢åˆ°æ›´å¤šç§å­ç©ºé—´è¿›è¡Œè¡¨ç¤ºã€‚æ¥ä¸‹æ¥æˆ‘ä»¬ä¹Ÿä½¿ç”¨ 8 ç»„æ³¨æ„åŠ›å¤´ï¼ˆattention headsï¼‰ï¼‰ã€‚æ¯ä¸€ç»„æ³¨æ„åŠ›çš„æƒé‡çŸ©é˜µéƒ½æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œä½†ç»è¿‡è®­ç»ƒä¹‹åï¼Œæ¯ä¸€ç»„æ³¨æ„åŠ›çš„æƒé‡ $W^Q, W^K W^V$ å¯ä»¥æŠŠè¾“å…¥çš„å‘é‡æ˜ å°„åˆ°ä¸€ä¸ªå¯¹åº”çš„"å­è¡¨ç¤ºç©ºé—´"ã€‚

![å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶](output_img/2-multi-head.png)

åœ¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæˆ‘ä»¬ä¸ºæ¯ç»„æ³¨æ„åŠ›è®¾å®šå•ç‹¬çš„ $W_Q$, $W_K$, $W_V$ å‚æ•°çŸ©é˜µã€‚å°†è¾“å…¥ $X$ å’Œæ¯ç»„æ³¨æ„åŠ›çš„ $W_Q$, $W_K$, $W_V$ ç›¸ä¹˜ï¼Œå¾—åˆ° 8 ç»„ $Q$, $K$, $V$ çŸ©é˜µã€‚

æ¥ç€ï¼Œæˆ‘ä»¬æŠŠæ¯ç»„ $K$, $Q$, $V$ è®¡ç®—å¾—åˆ°æ¯ç»„çš„ $Z$ çŸ©é˜µï¼Œå°±å¾—åˆ° 8 ä¸ª $Z$ çŸ©é˜µã€‚

<img src="output_img/2-8z.png" alt="png" style="zoom:33%;" />

ç”±äºå‰é¦ˆç¥ç»ç½‘ç»œå±‚æ¥æ”¶çš„æ˜¯ 1 ä¸ªçŸ©é˜µï¼ˆå…¶ä¸­æ¯è¡Œçš„å‘é‡è¡¨ç¤ºä¸€ä¸ªè¯ï¼‰ï¼Œè€Œä¸æ˜¯ 8 ä¸ªçŸ©é˜µï¼Œæ‰€ä»¥æˆ‘ä»¬ç›´æ¥æŠŠ 8 ä¸ªå­çŸ©é˜µæ‹¼æ¥èµ·æ¥å¾—åˆ°ä¸€ä¸ªå¤§çš„çŸ©é˜µï¼Œç„¶åå’Œå¦ä¸€ä¸ªæƒé‡çŸ©é˜µ$W^O$ç›¸ä¹˜åšä¸€æ¬¡å˜æ¢ï¼Œæ˜ å°„åˆ°å‰é¦ˆç¥ç»ç½‘ç»œå±‚æ‰€éœ€è¦çš„ç»´åº¦ã€‚

<img src="output_img/2-to1.png" alt="png" style="zoom:33%;" />

### MutiHeadAttention å°ç»“ï¼š

- æŠŠ 8 ä¸ªçŸ©é˜µ {$Z_0$,$Z_1$...,$Z_7$} æ‹¼æ¥èµ·æ¥

- æŠŠæ‹¼æ¥åçš„çŸ©é˜µå’Œ$W_O$æƒé‡çŸ©é˜µç›¸ä¹˜

- å¾—åˆ°æœ€ç»ˆçš„çŸ©é˜µ Zï¼Œè¿™ä¸ªçŸ©é˜µåŒ…å«äº†æ‰€æœ‰ attention headsï¼ˆæ³¨æ„åŠ›å¤´ï¼‰ çš„ä¿¡æ¯ã€‚è¿™ä¸ªçŸ©é˜µä¼šè¾“å…¥åˆ° FFNN (Feed Forward Neural Network)å±‚ã€‚

ä»¥ä¸Šå°±æ˜¯å¤šå¤´æ³¨æ„åŠ›çš„å…¨éƒ¨å†…å®¹ã€‚æœ€åå°†æ‰€æœ‰å†…å®¹æ”¾åˆ°ä¸€å¼ å›¾ä¸­ï¼š

<img src="output_img/2-put-together.png" alt="png" style="zoom: 33%;" />

å­¦ä¹ äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©æˆ‘ä»¬å†æ¥çœ‹ä¸‹å½“æˆ‘ä»¬å‰é¢æåˆ°çš„ it ä¾‹å­ï¼Œä¸åŒçš„ attention heads ï¼ˆæ³¨æ„åŠ›å¤´ï¼‰å¯¹åº”çš„â€œitâ€attention äº†å“ªäº›å†…å®¹ã€‚ä¸‹å›¾ä¸­çš„ç»¿è‰²å’Œæ©™è‰²çº¿æ¡åˆ†åˆ«è¡¨ç¤º 2 ç»„ä¸åŒçš„ attentin headsï¼š

<img src="output_img/it-attention.png" style="zoom: 50%;" />

> å½“æˆ‘ä»¬ç¼–ç å•è¯"it"æ—¶ï¼Œå…¶ä¸­ä¸€ä¸ª attention head ï¼ˆæ©™è‰²æ³¨æ„åŠ›å¤´ï¼‰æœ€å…³æ³¨çš„æ˜¯"the animal"ï¼Œå¦å¤–ä¸€ä¸ªç»¿è‰² attention head å…³æ³¨çš„æ˜¯"tired"ã€‚å› æ­¤åœ¨æŸç§æ„ä¹‰ä¸Šï¼Œ"it"åœ¨æ¨¡å‹ä¸­çš„è¡¨ç¤ºï¼Œèåˆäº†"animal"å’Œ"tire"çš„éƒ¨åˆ†è¡¨è¾¾ã€‚

### MultiHeadAttention ä»£ç å®ä¾‹

```python
class MultiheadAttention(nn.Module):
    # n_headsï¼šå¤šå¤´æ³¨æ„åŠ›çš„æ•°é‡
    # hid_dimï¼šæ¯ä¸ªè¯è¾“å‡ºçš„å‘é‡ç»´åº¦
    def __init__(self, hid_dim, n_heads, dropout):
        super(MultiheadAttention, self).__init__()
        self.hid_dim = hid_dim
        self.n_heads = n_heads

        # å¼ºåˆ¶ hid_dim å¿…é¡»æ•´é™¤ h
        assert hid_dim % n_heads == 0
        # å®šä¹‰ W_q çŸ©é˜µ
        self.w_q = nn.Linear(hid_dim, hid_dim)
        # å®šä¹‰ W_k çŸ©é˜µ
        self.w_k = nn.Linear(hid_dim, hid_dim)
        # å®šä¹‰ W_v çŸ©é˜µ
        self.w_v = nn.Linear(hid_dim, hid_dim)

        self.fc = nn.Linear(hid_dim, hid_dim)
        self.do = nn.Dropout(dropout)
        # ç¼©æ”¾
        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))

    def forward(self, query, key, value, mask=None):
        # æ³¨æ„ Qï¼ŒKï¼ŒVçš„åœ¨å¥å­é•¿åº¦è¿™ä¸€ä¸ªç»´åº¦çš„æ•°å€¼å¯ä»¥ä¸€æ ·ï¼Œå¯ä»¥ä¸ä¸€æ ·ã€‚
        # K: [64,10,300], å‡è®¾batch_size ä¸º 64ï¼Œæœ‰ 10 ä¸ªè¯ï¼Œæ¯ä¸ªè¯çš„ Query å‘é‡æ˜¯ 300 ç»´
        # V: [64,10,300], å‡è®¾batch_size ä¸º 64ï¼Œæœ‰ 10 ä¸ªè¯ï¼Œæ¯ä¸ªè¯çš„ Query å‘é‡æ˜¯ 300 ç»´
        # Q: [64,12,300], å‡è®¾batch_size ä¸º 64ï¼Œæœ‰ 12 ä¸ªè¯ï¼Œæ¯ä¸ªè¯çš„ Query å‘é‡æ˜¯ 300 ç»´
        bsz = query.shape[0]
        Q = self.w_q(query)
        K = self.w_k(key)
        V = self.w_v(value)
        # è¿™é‡ŒæŠŠ K Q V çŸ©é˜µæ‹†åˆ†ä¸ºå¤šç»„æ³¨æ„åŠ›
        # æœ€åä¸€ç»´å°±æ˜¯æ˜¯ç”¨ self.hid_dim // self.n_heads æ¥å¾—åˆ°çš„ï¼Œè¡¨ç¤ºæ¯ç»„æ³¨æ„åŠ›çš„å‘é‡é•¿åº¦, æ¯ä¸ª head çš„å‘é‡é•¿åº¦æ˜¯ï¼š300/6=50
        # 64 è¡¨ç¤º batch sizeï¼Œ6 è¡¨ç¤ºæœ‰ 6ç»„æ³¨æ„åŠ›ï¼Œ10 è¡¨ç¤ºæœ‰ 10 è¯ï¼Œ50 è¡¨ç¤ºæ¯ç»„æ³¨æ„åŠ›çš„è¯çš„å‘é‡é•¿åº¦
        # K: [64,10,300] æ‹†åˆ†å¤šç»„æ³¨æ„åŠ› -> [64,10,6,50] è½¬ç½®å¾—åˆ° -> [64,6,10,50]
        # V: [64,10,300] æ‹†åˆ†å¤šç»„æ³¨æ„åŠ› -> [64,10,6,50] è½¬ç½®å¾—åˆ° -> [64,6,10,50]
        # Q: [64,12,300] æ‹†åˆ†å¤šç»„æ³¨æ„åŠ› -> [64,12,6,50] è½¬ç½®å¾—åˆ° -> [64,6,12,50]
        # è½¬ç½®æ˜¯ä¸ºäº†æŠŠæ³¨æ„åŠ›çš„æ•°é‡ 6 æ”¾åˆ°å‰é¢ï¼ŒæŠŠ 10 å’Œ 50 æ”¾åˆ°åé¢ï¼Œæ–¹ä¾¿ä¸‹é¢è®¡ç®—
        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim //
                   self.n_heads).permute(0, 2, 1, 3)
        K = K.view(bsz, -1, self.n_heads, self.hid_dim //
                   self.n_heads).permute(0, 2, 1, 3)
        V = V.view(bsz, -1, self.n_heads, self.hid_dim //
                   self.n_heads).permute(0, 2, 1, 3)

        # ç¬¬ 1 æ­¥ï¼šQ ä¹˜ä»¥ Kçš„è½¬ç½®ï¼Œé™¤ä»¥scale
        # [64,6,12,50] * [64,6,50,10] = [64,6,12,10]
        # attentionï¼š[64,6,12,10]
        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale

        # å¦‚æœ mask ä¸ä¸ºç©ºï¼Œé‚£ä¹ˆå°±æŠŠ mask ä¸º 0 çš„ä½ç½®çš„ attention åˆ†æ•°è®¾ç½®ä¸º -1e10ï¼Œè¿™é‡Œç”¨â€œ0â€æ¥æŒ‡ç¤ºå“ªäº›ä½ç½®çš„è¯å‘é‡ä¸èƒ½è¢«attentionåˆ°ï¼Œæ¯”å¦‚paddingä½ç½®ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ç”¨â€œ1â€æˆ–è€…å…¶ä»–æ•°å­—æ¥æŒ‡ç¤ºï¼Œä¸»è¦è®¾è®¡ä¸‹é¢2è¡Œä»£ç çš„æ”¹åŠ¨ã€‚
        if mask is not None:
            attention = attention.masked_fill(mask == 0, -1e10)

        # ç¬¬ 2 æ­¥ï¼šè®¡ç®—ä¸Šä¸€æ­¥ç»“æœçš„ softmaxï¼Œå†ç»è¿‡ dropoutï¼Œå¾—åˆ° attentionã€‚
        # æ³¨æ„ï¼Œè¿™é‡Œæ˜¯å¯¹æœ€åä¸€ç»´åš softmaxï¼Œä¹Ÿå°±æ˜¯åœ¨è¾“å…¥åºåˆ—çš„ç»´åº¦åš softmax
        # attention: [64,6,12,10]
        attention = self.do(torch.softmax(attention, dim=-1))

        # ç¬¬ä¸‰æ­¥ï¼Œattentionç»“æœä¸Vç›¸ä¹˜ï¼Œå¾—åˆ°å¤šå¤´æ³¨æ„åŠ›çš„ç»“æœ
        # [64,6,12,10] * [64,6,10,50] = [64,6,12,50]
        # x: [64,6,12,50]
        x = torch.matmul(attention, V)

        # å› ä¸º query æœ‰ 12 ä¸ªè¯ï¼Œæ‰€ä»¥æŠŠ 12 æ”¾åˆ°å‰é¢ï¼ŒæŠŠ 50 å’Œ 6 æ”¾åˆ°åé¢ï¼Œæ–¹ä¾¿ä¸‹é¢æ‹¼æ¥å¤šç»„çš„ç»“æœ
        # x: [64,6,12,50] è½¬ç½®-> [64,12,6,50]
        x = x.permute(0, 2, 1, 3).contiguous()
        # è¿™é‡Œçš„çŸ©é˜µè½¬æ¢å°±æ˜¯ï¼šæŠŠå¤šç»„æ³¨æ„åŠ›çš„ç»“æœæ‹¼æ¥èµ·æ¥
        # æœ€ç»ˆç»“æœå°±æ˜¯ [64,12,300]
        # x: [64,12,6,50] -> [64,12,300]
        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))
        x = self.fc(x)
        return x


# batch_size ä¸º 64ï¼Œæœ‰ 12 ä¸ªè¯ï¼Œæ¯ä¸ªè¯çš„ Query å‘é‡æ˜¯ 300 ç»´
query = torch.rand(64, 12, 300)
# batch_size ä¸º 64ï¼Œæœ‰ 12 ä¸ªè¯ï¼Œæ¯ä¸ªè¯çš„ Key å‘é‡æ˜¯ 300 ç»´
key = torch.rand(64, 12, 300)
# batch_size ä¸º 64ï¼Œæœ‰ 12 ä¸ªè¯ï¼Œæ¯ä¸ªè¯çš„ Value å‘é‡æ˜¯ 300 ç»´
value = torch.rand(64, 12, 300)
attention_fn = MultiheadAttention(hid_dim=300, n_heads=6, dropout=0.1)
output = attention_fn(query, key, value)
## output: torch.Size([64, 12, 300])
print(output.shape)
```

    torch.Size([64, 12, 300])

### ç®€åŒ–ä»£ç 

```python
# å®šä¹‰ä¸€ä¸ªcloneså‡½æ•°ï¼Œæ¥æ›´æ–¹ä¾¿çš„å°†æŸä¸ªç»“æ„å¤åˆ¶è‹¥å¹²ä»½
def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])
```

```python
class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        # åœ¨ç±»çš„åˆå§‹åŒ–æ—¶ï¼Œä¼šä¼ å…¥ä¸‰ä¸ªå‚æ•°ï¼Œhä»£è¡¨å¤´æ•°ï¼Œd_modelä»£è¡¨è¯åµŒå…¥çš„ç»´åº¦ï¼Œdropoutä»£è¡¨è¿›è¡Œdropoutæ“ä½œæ—¶ç½®0æ¯”ç‡ï¼Œé»˜è®¤æ˜¯0.1
        super(MultiHeadedAttention, self).__init__()
        # åœ¨å‡½æ•°ä¸­ï¼Œé¦–å…ˆä½¿ç”¨äº†ä¸€ä¸ªæµ‹è¯•ä¸­å¸¸ç”¨çš„assertè¯­å¥ï¼Œåˆ¤æ–­hæ˜¯å¦èƒ½è¢«d_modelæ•´é™¤ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬ä¹‹åè¦ç»™æ¯ä¸ªå¤´åˆ†é…ç­‰é‡çš„è¯ç‰¹å¾ï¼Œä¹Ÿå°±æ˜¯embedding_dim/headä¸ª
        assert d_model % h == 0
        # å¾—åˆ°æ¯ä¸ªå¤´è·å¾—çš„åˆ†å‰²è¯å‘é‡ç»´åº¦d_k
        self.d_k = d_model // h
        # ä¼ å…¥å¤´æ•°h
        self.h = h

        # åˆ›å»ºlinearå±‚ï¼Œé€šè¿‡nnçš„Linearå®ä¾‹åŒ–ï¼Œå®ƒçš„å†…éƒ¨å˜æ¢çŸ©é˜µæ˜¯embedding_dim x embedding_dimï¼Œç„¶åä½¿ç”¨ï¼Œä¸ºä»€ä¹ˆæ˜¯å››ä¸ªå‘¢ï¼Œè¿™æ˜¯å› ä¸ºåœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­ï¼ŒQ,K,Vå„éœ€è¦ä¸€ä¸ªï¼Œæœ€åæ‹¼æ¥çš„çŸ©é˜µè¿˜éœ€è¦ä¸€ä¸ªï¼Œå› æ­¤ä¸€å…±æ˜¯å››ä¸ª
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        # self.attnä¸ºNoneï¼Œå®ƒä»£è¡¨æœ€åå¾—åˆ°çš„æ³¨æ„åŠ›å¼ é‡ï¼Œç°åœ¨è¿˜æ²¡æœ‰ç»“æœæ‰€ä»¥ä¸ºNone
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        # å‰å‘é€»è¾‘å‡½æ•°ï¼Œå®ƒè¾“å…¥å‚æ•°æœ‰å››ä¸ªï¼Œå‰ä¸‰ä¸ªå°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶éœ€è¦çš„Q,K,Vï¼Œæœ€åä¸€ä¸ªæ˜¯æ³¨æ„åŠ›æœºåˆ¶ä¸­å¯èƒ½éœ€è¦çš„maskæ©ç å¼ é‡ï¼Œé»˜è®¤æ˜¯None
        if mask is not None:
            # Same mask applied to all h heads.
            # ä½¿ç”¨unsqueezeæ‰©å±•ç»´åº¦ï¼Œä»£è¡¨å¤šå¤´ä¸­çš„ç¬¬nå¤´
            mask = mask.unsqueeze(1)
        # æ¥ç€ï¼Œæˆ‘ä»¬è·å¾—ä¸€ä¸ªbatch_sizeçš„å˜é‡ï¼Œä»–æ˜¯queryå°ºå¯¸çš„ç¬¬1ä¸ªæ•°å­—ï¼Œä»£è¡¨æœ‰å¤šå°‘æ¡æ ·æœ¬
        nbatches = query.size(0)

        # 1) Do all the linear projections in batch from d_model => h x d_k
        # é¦–å…ˆåˆ©ç”¨zipå°†è¾“å…¥QKVä¸ä¸‰ä¸ªçº¿æ€§å±‚ç»„åˆ°ä¸€èµ·ï¼Œç„¶ååˆ©ç”¨forå¾ªç¯ï¼Œå°†è¾“å…¥QKVåˆ†åˆ«ä¼ åˆ°çº¿æ€§å±‚ä¸­ï¼Œåšå®Œçº¿æ€§å˜æ¢åï¼Œå¼€å§‹ä¸ºæ¯ä¸ªå¤´åˆ†å‰²è¾“å…¥ï¼Œè¿™é‡Œä½¿ç”¨viewæ–¹æ³•å¯¹çº¿æ€§å˜æ¢çš„ç»“æ„è¿›è¡Œç»´åº¦é‡å¡‘ï¼Œå¤šåŠ äº†ä¸€ä¸ªç»´åº¦hä»£è¡¨å¤´ï¼Œè¿™æ ·å°±æ„å‘³ç€æ¯ä¸ªå¤´å¯ä»¥è·å¾—ä¸€éƒ¨åˆ†è¯ç‰¹å¾ç»„æˆçš„å¥å­ï¼Œå…¶ä¸­çš„-1ä»£è¡¨è‡ªé€‚åº”ç»´åº¦ï¼Œè®¡ç®—æœºä¼šæ ¹æ®è¿™ç§å˜æ¢è‡ªåŠ¨è®¡ç®—è¿™é‡Œçš„å€¼ï¼Œç„¶åå¯¹ç¬¬äºŒç»´å’Œç¬¬ä¸‰ç»´è¿›è¡Œè½¬ç½®æ“ä½œï¼Œä¸ºäº†è®©ä»£è¡¨å¥å­é•¿åº¦ç»´åº¦å’Œè¯å‘é‡ç»´åº¦èƒ½å¤Ÿç›¸é‚»ï¼Œè¿™æ ·æ³¨æ„åŠ›æœºåˆ¶æ‰èƒ½æ‰¾åˆ°è¯ä¹‰ä¸å¥å­ä½ç½®çš„å…³ç³»ï¼Œä»attentionå‡½æ•°ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåˆ©ç”¨çš„æ˜¯åŸå§‹è¾“å…¥çš„å€’æ•°ç¬¬ä¸€å’Œç¬¬äºŒç»´ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†æ¯ä¸ªå¤´çš„è¾“å…¥
        # src:query.shape=[30,10,512] tgt:query.shape=[30,9,512]
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]

        # 2) Apply attention on all the projected vectors in batch.
        # å¾—åˆ°æ¯ä¸ªå¤´çš„è¾“å…¥åï¼Œæ¥ä¸‹æ¥å°±æ˜¯å°†ä»–ä»¬ä¼ å…¥åˆ°attentionä¸­ï¼Œè¿™é‡Œç›´æ¥è°ƒç”¨æˆ‘ä»¬ä¹‹å‰å®ç°çš„attentionå‡½æ•°ï¼ŒåŒæ—¶ä¹Ÿå°†maskå’Œdropoutä¼ å…¥å…¶ä¸­
        # src:query.shape=[30,8,10,64] tgt:query.shape=[30,8,9,64]
        # 8*64 = 512
        x, self.attn = attention(
            query, key, value, mask=mask, dropout=self.dropout)

        # 3) "Concat" using a view and apply a final linear.
        # é€šè¿‡å¤šå¤´æ³¨æ„åŠ›è®¡ç®—åï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†æ¯ä¸ªå¤´è®¡ç®—ç»“æœç»„æˆçš„4ç»´å¼ é‡ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºè¾“å…¥çš„å½¢çŠ¶ä»¥æ–¹ä¾¿åç»­çš„è®¡ç®—ï¼Œå› æ­¤è¿™é‡Œå¼€å§‹è¿›è¡Œç¬¬ä¸€æ­¥å¤„ç†ç¯èŠ‚çš„é€†æ“ä½œï¼Œå…ˆå¯¹ç¬¬äºŒå’Œç¬¬ä¸‰ç»´è¿›è¡Œè½¬ç½®ï¼Œç„¶åä½¿ç”¨contiguousæ–¹æ³•ã€‚è¿™ä¸ªæ–¹æ³•çš„ä½œç”¨å°±æ˜¯èƒ½å¤Ÿè®©è½¬ç½®åçš„å¼ é‡åº”ç”¨viewæ–¹æ³•ï¼Œå¦åˆ™å°†æ— æ³•ç›´æ¥ä½¿ç”¨ï¼Œæ‰€ä»¥ï¼Œä¸‹ä¸€æ­¥å°±æ˜¯ä½¿ç”¨viewé‡å¡‘å½¢çŠ¶ï¼Œå˜æˆå’Œè¾“å…¥å½¢çŠ¶ç›¸åŒã€‚
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d_k)
        # æœ€åä½¿ç”¨çº¿æ€§å±‚åˆ—è¡¨ä¸­çš„æœ€åä¸€ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆçš„å¤šå¤´æ³¨æ„åŠ›ç»“æ„çš„è¾“å‡º
        # src:return.shape=[30,10,512] tgt:return.shape=[30,9,512]
        return self.linears[-1](x)
```

## å‰é¦ˆå…¨è¿æ¥å±‚

é™¤äº† attention å­å±‚ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„ç¼–ç å™¨å’Œè§£ç å™¨ä¸­çš„æ¯ä¸ªå±‚éƒ½åŒ…å«ä¸€ä¸ªå…¨è¿æ¥çš„å‰é¦ˆç½‘ç»œï¼Œè¯¥ç½‘ç»œåœ¨æ¯ä¸ªå±‚çš„ä½ç½®ç›¸åŒï¼ˆéƒ½åœ¨æ¯ä¸ª encoder-layer æˆ–è€… decoder-layer çš„æœ€åï¼‰ã€‚è¯¥å‰é¦ˆç½‘ç»œåŒ…æ‹¬ä¸¤ä¸ªçº¿æ€§å˜æ¢ï¼Œå¹¶åœ¨ä¸¤ä¸ªçº¿æ€§å˜æ¢ä¸­é—´æœ‰ä¸€ä¸ª ReLU æ¿€æ´»å‡½æ•°ã€‚

$$\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2$$

å°½ç®¡ä¸¤å±‚éƒ½æ˜¯çº¿æ€§å˜æ¢ï¼Œä½†å®ƒä»¬åœ¨å±‚ä¸å±‚ä¹‹é—´ä½¿ç”¨ä¸åŒçš„å‚æ•°ã€‚å¦ä¸€ç§æè¿°æ–¹å¼æ˜¯ä¸¤ä¸ªå†…æ ¸å¤§å°ä¸º 1 çš„å·ç§¯ã€‚ è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦éƒ½æ˜¯ $d_{\text{model}}=512$, å†…å±‚ç»´åº¦æ˜¯$d_{ff}=2048$ã€‚ï¼ˆä¹Ÿå°±æ˜¯ç¬¬ä¸€å±‚è¾“å…¥ 512 ç»´,è¾“å‡º 2048 ç»´ï¼›ç¬¬äºŒå±‚è¾“å…¥ 2048 ç»´ï¼Œè¾“å‡º 512 ç»´ï¼‰

```python
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        #åˆå§‹åŒ–å‡½æ•°æœ‰ä¸‰ä¸ªè¾“å…¥å‚æ•°åˆ†åˆ«æ˜¯d_modelï¼Œd_ffï¼Œå’Œdropout=0.1ï¼Œç¬¬ä¸€ä¸ªæ˜¯çº¿æ€§å±‚çš„è¾“å…¥ç»´åº¦ä¹Ÿæ˜¯ç¬¬äºŒä¸ªçº¿æ€§å±‚çš„è¾“å‡ºç»´åº¦ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›è¾“å…¥é€šè¿‡å‰é¦ˆå…¨è¿æ¥å±‚åè¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦ä¸å˜ï¼Œç¬¬äºŒä¸ªå‚æ•°d_ffå°±æ˜¯ç¬¬äºŒä¸ªçº¿æ€§å±‚çš„è¾“å…¥ç»´åº¦å’Œç¬¬ä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºï¼Œæœ€åä¸€ä¸ªæ˜¯dropoutç½®0æ¯”ç‡ã€‚
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        #è¾“å…¥å‚æ•°ä¸ºxï¼Œä»£è¡¨æ¥è‡ªä¸Šä¸€å±‚çš„è¾“å‡ºï¼Œé¦–å…ˆç»è¿‡ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç„¶åä½¿ç”¨Fä¸­çš„reluå‡½æ•°è¿›è¡Œæ¿€æ´»ï¼Œä¹‹åå†ä½¿ç”¨dropoutè¿›è¡Œéšæœºç½®0ï¼Œæœ€åé€šè¿‡ç¬¬äºŒä¸ªçº¿æ€§å±‚w2ï¼Œè¿”å›æœ€ç»ˆç»“æœ
        return self.w_2(self.dropout(F.relu(self.w_1(x))))
```

## Encoder

ç¼–ç å™¨ç”± N = 6 ä¸ªå®Œå…¨ç›¸åŒçš„å±‚ç»„æˆã€‚
ç¼–ç å™¨çš„æ¯å±‚ encoder åŒ…å« Self Attention å­å±‚å’Œ FFN å­å±‚ï¼Œæ¯ä¸ªå­å±‚éƒ½ä½¿ç”¨äº†æ®‹å·®è¿æ¥[(cite)](https://arxiv.org/abs/1512.03385)ï¼Œå’Œå±‚æ ‡å‡†åŒ–ï¼ˆlayer-normalizationï¼‰ [(cite)](https://arxiv.org/abs/1607.06450)ã€‚

æˆ‘ä»¬ç§°å‘¼å­å±‚ä¸ºï¼š$\mathrm{Sublayer}(x)$ï¼Œæ¯ä¸ªå­å±‚çš„æœ€ç»ˆè¾“å‡ºæ˜¯$\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$ã€‚ dropout [(cite)](http://jmlr.org/papers/v15/srivastava14a.html)è¢«åŠ åœ¨ Sublayer ä¸Šã€‚

ä¸ºäº†ä¾¿äºè¿›è¡Œæ®‹å·®è¿æ¥ï¼Œæ¨¡å‹ä¸­çš„æ‰€æœ‰å­å±‚ä»¥åŠ embedding å±‚äº§ç”Ÿçš„è¾“å‡ºçš„ç»´åº¦éƒ½ä¸º $d_{\text{model}}=512$ã€‚

å°† Self-Attention å±‚çš„å±‚æ ‡å‡†åŒ–ï¼ˆlayer-normalizationï¼‰å’Œæ¶‰åŠçš„å‘é‡è®¡ç®—ç»†èŠ‚éƒ½è¿›è¡Œå¯è§†åŒ–ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

<img src="output_img/resnet_norm.png" alt="png" style="zoom: 33%;" />

```python
# å®šä¹‰ä¸€ä¸ªcloneså‡½æ•°ï¼Œæ¥æ›´æ–¹ä¾¿çš„å°†æŸä¸ªç»“æ„å¤åˆ¶è‹¥å¹²ä»½
def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


class Encoder(nn.Module):
    """
    Encoder
    The encoder is composed of a stack of N=6 identical layers.
    """

    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        # è°ƒç”¨æ—¶ä¼šå°†ç¼–ç å™¨å±‚ä¼ è¿›æ¥ï¼Œæˆ‘ä»¬ç®€å•å…‹éš†Nåˆ†ï¼Œå åŠ åœ¨ä¸€èµ·ï¼Œç»„æˆå®Œæ•´çš„Encoder
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, mask):
        "Pass the input (and mask) through each layer in turn."
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
```

ç¬¬ä¸€ä¸ªå­å±‚åŒ…æ‹¬ä¸€ä¸ª**å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚**å’Œ**è§„èŒƒåŒ–å±‚**ä»¥åŠä¸€ä¸ª**æ®‹å·®è¿æ¥**

ç¬¬äºŒä¸ªå­å±‚åŒ…æ‹¬ä¸€ä¸ª**å‰é¦ˆå…¨è¿æ¥å±‚**å’Œ**è§„èŒƒåŒ–å±‚**ä»¥åŠä¸€ä¸ª**æ®‹å·®è¿æ¥**

å¯ä»¥çœ‹åˆ°ï¼Œä¸¤ä¸ªå­å±‚çš„ç»“æ„å…¶å®æ˜¯ä¸€è‡´çš„ï¼Œåªæ˜¯ä¸­é—´æ ¸å¿ƒå±‚çš„å®ç°ä¸åŒ.

![png](output_img/encoder.png)

ä¸‹é¢çš„**SublayerConnection**ç±»ç”¨æ¥å¤„ç†å•ä¸ª Sublayer çš„è¾“å‡ºï¼Œè¯¥è¾“å‡ºå°†ç»§ç»­è¢«è¾“å…¥ä¸‹ä¸€ä¸ª Sublayerï¼š

```python
class SublayerConnection(nn.Module):
    """
    å®ç°å­å±‚è¿æ¥ç»“æ„çš„ç±»
    """

    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):

        # åŸpaperçš„æ–¹æ¡ˆ
        #sublayer_out = sublayer(x)
        #x_norm = self.norm(x + self.dropout(sublayer_out))

        # ç¨åŠ è°ƒæ•´çš„ç‰ˆæœ¬
        sublayer_out = sublayer(x)
        sublayer_out = self.dropout(sublayer_out)
        x_norm = x + self.norm(sublayer_out)
        return x_norm
```

æ³¨ï¼šä¸Šé¢çš„å®ç°ä¸­ï¼Œæˆ‘å¯¹æ®‹å·®çš„é“¾æ¥æ–¹æ¡ˆè¿›è¡Œäº†å°å°çš„è°ƒæ•´ï¼Œå’ŒåŸè®ºæ–‡æœ‰æ‰€ä¸åŒã€‚æŠŠ x ä» norm ä¸­æ‹¿å‡ºæ¥ï¼Œä¿è¯æ°¸è¿œæœ‰ä¸€æ¡â€œé«˜é€Ÿå…¬è·¯â€ï¼Œè¿™æ ·ç†è®ºä¸Šä¼šæ”¶æ•›çš„å¿«ä¸€äº›ï¼Œä½†æˆ‘æ— æ³•ç¡®ä¿è¿™æ ·åšä¸€å®šæ˜¯å¯¹çš„ï¼Œè¯·ä¸€å®šæ³¨æ„ã€‚

å®šä¹‰å¥½äº† SubLayerConnectionï¼Œæˆ‘ä»¬å°±å¯ä»¥å®ç° EncoderLayer çš„ç»“æ„äº†

```python
class EncoderLayer(nn.Module):
    "EncoderLayer is made up of two sublayer: self-attn and feed forward"

    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size   # embedding's dimention of model, é»˜è®¤512

    def forward(self, x, mask):
        # attention sub layer,let self.self_attn use one arg by using lambda
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        # feed forward sub layer
        z = self.sublayer[1](x, self.feed_forward)
        return z
```

## Decoder

è§£ç å™¨ä¹Ÿæ˜¯ç”± N = 6 ä¸ªå®Œå…¨ç›¸åŒçš„ decoder å±‚ç»„æˆã€‚

### 1. è§£ç å™¨æ•´ä½“ç»“æ„

è§£ç å™¨çš„ä½œç”¨ï¼šæ ¹æ®ç¼–ç å™¨çš„ç»“æœä»¥åŠä¸Šä¸€æ¬¡é¢„æµ‹çš„ç»“æœï¼Œè¾“å‡ºåºåˆ—çš„ä¸‹ä¸€ä¸ªç»“æœã€‚

æ•´ä½“ç»“æ„ä¸Šï¼Œè§£ç å™¨ä¹Ÿæ˜¯ç”± N ä¸ªç›¸åŒå±‚å †å è€Œæˆã€‚æ„é€ ä»£ç å¦‚ä¸‹ï¼š

```python
# ä½¿ç”¨ç±»Decoderæ¥å®ç°è§£ç å™¨
class Decoder(nn.Module):
    "Generic N layer decoder with masking."

    def __init__(self, layer, N):
        # åˆå§‹åŒ–å‡½æ•°çš„å‚æ•°æœ‰ä¸¤ä¸ªï¼Œç¬¬ä¸€ä¸ªå°±æ˜¯è§£ç å™¨å±‚layerï¼Œç¬¬äºŒä¸ªæ˜¯è§£ç å™¨å±‚çš„ä¸ªæ•°N
        super(Decoder, self).__init__()
        # é¦–å…ˆä½¿ç”¨clonesæ–¹æ³•å…‹éš†äº†Nä¸ªlayerï¼Œç„¶åå®ä¾‹åŒ–ä¸€ä¸ªè§„èŒƒåŒ–å±‚ï¼Œå› ä¸ºæ•°æ®èµ°è¿‡äº†æ‰€æœ‰çš„è§£ç å™¨å±‚åæœ€åè¦åšè§„èŒƒåŒ–å¤„ç†ã€‚
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, memory, src_mask, tgt_mask):
        # forwardå‡½æ•°ä¸­çš„å‚æ•°æœ‰4ä¸ªï¼Œxä»£è¡¨ç›®æ ‡æ•°æ®çš„åµŒå…¥è¡¨ç¤ºï¼Œmemoryæ˜¯ç¼–ç å™¨å±‚çš„è¾“å‡ºï¼Œsource_maskï¼Œtarget_maskä»£è¡¨æºæ•°æ®å’Œç›®æ ‡æ•°æ®çš„æ©ç å¼ é‡ï¼Œç„¶åå°±æ˜¯å¯¹æ¯ä¸ªå±‚è¿›è¡Œå¾ªç¯ï¼Œå½“ç„¶è¿™ä¸ªå¾ªç¯å°±æ˜¯å˜é‡xé€šè¿‡æ¯ä¸€ä¸ªå±‚çš„å¤„ç†ï¼Œå¾—å‡ºæœ€åçš„ç»“æœï¼Œå†è¿›è¡Œä¸€æ¬¡è§„èŒƒåŒ–è¿”å›å³å¯ã€‚
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
```

### 2. è§£ç å™¨å±‚

æ¯ä¸ªè§£ç å™¨å±‚ç”±ä¸‰ä¸ªå­å±‚è¿æ¥ç»“æ„ç»„æˆ

- ç¬¬ä¸€ä¸ªå­å±‚è¿æ¥ç»“æ„åŒ…æ‹¬ä¸€ä¸ª**å¤šå¤´è‡ªæ³¨æ„åŠ›å­å±‚**å’Œè§„èŒƒåŒ–å±‚ä»¥åŠä¸€ä¸ªæ®‹å·®è¿æ¥
- ç¬¬äºŒä¸ªå­å±‚è¿æ¥ç»“æ„åŒ…æ‹¬ä¸€ä¸ª**å¤šå¤´æ³¨æ„åŠ›å­å±‚**å’Œè§„èŒƒåŒ–å±‚ä»¥åŠä¸€ä¸ªæ®‹å·®è¿æ¥
- ç¬¬ä¸‰ä¸ªå­å±‚è¿æ¥ç»“æ„åŒ…æ‹¬ä¸€ä¸ª**å‰é¦ˆå…¨è¿æ¥å­å±‚**å’Œè§„èŒƒåŒ–å±‚ä»¥åŠä¸€ä¸ªæ®‹å·®è¿æ¥ã€‚
  <img src="output_img/decoder.png" alt="png" style="zoom:33%;" />

æœ‰ä¸€ä¸ªç»†èŠ‚éœ€è¦æ³¨æ„ï¼Œç¬¬ä¸€ä¸ªå­å±‚çš„å¤šå¤´æ³¨æ„åŠ›å’Œç¼–ç å™¨ä¸­å®Œå…¨ä¸€è‡´ï¼Œç¬¬äºŒä¸ªå­å±‚ï¼Œå®ƒçš„**å¤šå¤´æ³¨æ„åŠ›æ¨¡å—**ä¸­ï¼Œ**query æ¥è‡ªä¸Šä¸€ä¸ªå­å±‚ï¼Œkey å’Œ value æ¥è‡ªç¼–ç å™¨çš„è¾“å‡º**ã€‚å¯ä»¥è¿™æ ·ç†è§£ï¼Œå°±æ˜¯ç¬¬äºŒå±‚è´Ÿè´£ï¼Œåˆ©ç”¨è§£ç å™¨å·²ç»é¢„æµ‹å‡ºçš„ä¿¡æ¯ä½œä¸º queryï¼Œå»ç¼–ç å™¨æå–çš„å„ç§ç‰¹å¾ä¸­ï¼ŒæŸ¥æ‰¾ç›¸å…³ä¿¡æ¯å¹¶èåˆåˆ°å½“å‰ç‰¹å¾ä¸­ï¼Œæ¥å®Œæˆé¢„æµ‹ã€‚

![gif](output_img/transformer_decoding_1.gif)

è§£ç ï¼ˆdecoding ï¼‰é˜¶æ®µçš„æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½è¾“å‡ºä¸€ä¸ªç¿»è¯‘åçš„å•è¯ï¼ˆè¿™é‡Œçš„ä¾‹å­æ˜¯è‹±è¯­ç¿»è¯‘ï¼‰ï¼Œè§£ç å™¨å½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºåˆé‡æ–°ä½œä¸ºè¾“å…¥ Q å’Œç¼–ç å™¨çš„è¾“å‡º Kã€V å…±åŒä½œä¸ºä¸‹ä¸€ä¸ªæ—¶é—´æ­¥è§£ç å™¨çš„è¾“å…¥ã€‚ç„¶åé‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°è¾“å‡ºä¸€ä¸ªç»“æŸç¬¦ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![gif](output_img/2-encoder-decoder.gif)

```python
# ä½¿ç”¨DecoderLayerçš„ç±»å®ç°è§£ç å™¨å±‚
class DecoderLayer(nn.Module):
    "Decoder is made of self-attn, src-attn, and feed forward (defined below)"

    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        # åˆå§‹åŒ–å‡½æ•°çš„å‚æ•°æœ‰5ä¸ªï¼Œåˆ†åˆ«æ˜¯sizeï¼Œä»£è¡¨è¯åµŒå…¥çš„ç»´åº¦å¤§å°ï¼ŒåŒæ—¶ä¹Ÿä»£è¡¨è§£ç å™¨çš„å°ºå¯¸ï¼Œç¬¬äºŒä¸ªæ˜¯self_attnï¼Œå¤šå¤´è‡ªæ³¨æ„åŠ›å¯¹è±¡ï¼Œä¹Ÿå°±æ˜¯è¯´è¿™ä¸ªæ³¨æ„åŠ›æœºåˆ¶éœ€è¦Q=K=Vï¼Œç¬¬ä¸‰ä¸ªæ˜¯src_attn,å¤šå¤´æ³¨æ„åŠ›å¯¹è±¡ï¼Œè¿™é‡ŒQ!=K=Vï¼Œç¬¬å››ä¸ªæ˜¯å‰é¦ˆå…¨è¿æ¥å±‚å¯¹è±¡ï¼Œæœ€åå°±æ˜¯dropoutç½®0æ¯”ç‡
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        # æŒ‰ç…§ç»“æ„å›¾ä½¿ç”¨cloneså‡½æ•°å…‹éš†ä¸‰ä¸ªå­å±‚è¿æ¥å¯¹è±¡
        self.sublayer = clones(SublayerConnection(size, dropout), 3)

    def forward(self, x, memory, src_mask, tgt_mask):
        # forwardå‡½æ•°ä¸­çš„å‚æ•°æœ‰4ä¸ªï¼Œåˆ†åˆ«æ˜¯æ¥è‡ªä¸Šä¸€å±‚çš„è¾“å…¥xï¼Œæ¥è‡ªç¼–ç å™¨å±‚çš„è¯­ä¹‰å­˜å‚¨å˜é‡memoryï¼Œä»¥åŠæºæ•°æ®æ©ç å¼ é‡å’Œç›®æ ‡æ•°æ®æ©ç å¼ é‡ï¼Œå°†memoryè¡¨ç¤ºæˆmä¹‹åæ–¹ä¾¿ä½¿ç”¨ã€‚
        m = memory
        # å°†xä¼ å…¥ç¬¬ä¸€ä¸ªå­å±‚ç»“æ„ï¼Œç¬¬ä¸€ä¸ªå­å±‚ç»“æ„çš„è¾“å…¥åˆ†åˆ«æ˜¯xå’Œself-attnå‡½æ•°ï¼Œå› ä¸ºæ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‰€ä»¥Q,K,Véƒ½æ˜¯xï¼Œæœ€åä¸€ä¸ªå‚æ•°æ—¶ç›®æ ‡æ•°æ®æ©ç å¼ é‡ï¼Œè¿™æ—¶è¦å¯¹ç›®æ ‡æ•°æ®è¿›è¡Œé®æ©ï¼Œå› ä¸ºæ­¤æ—¶æ¨¡å‹å¯èƒ½è¿˜æ²¡æœ‰ç”Ÿæˆä»»ä½•ç›®æ ‡æ•°æ®ã€‚
        # æ¯”å¦‚åœ¨è§£ç å™¨å‡†å¤‡ç”Ÿæˆç¬¬ä¸€ä¸ªå­—ç¬¦æˆ–è¯æ±‡æ—¶ï¼Œæˆ‘ä»¬å…¶å®å·²ç»ä¼ å…¥äº†ç¬¬ä¸€ä¸ªå­—ç¬¦ä»¥ä¾¿è®¡ç®—æŸå¤±ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸å¸Œæœ›åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªå­—ç¬¦æ—¶æ¨¡å‹èƒ½åˆ©ç”¨è¿™ä¸ªä¿¡æ¯ï¼Œå› æ­¤æˆ‘ä»¬ä¼šå°†å…¶é®æ©ï¼ŒåŒæ ·ç”Ÿæˆç¬¬äºŒä¸ªå­—ç¬¦æˆ–è¯æ±‡æ—¶ï¼Œæ¨¡å‹åªèƒ½ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—ç¬¦æˆ–è¯æ±‡ä¿¡æ¯ï¼Œç¬¬äºŒä¸ªå­—ç¬¦ä»¥åŠä¹‹åçš„ä¿¡æ¯éƒ½ä¸å…è®¸è¢«æ¨¡å‹ä½¿ç”¨ã€‚
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        # æ¥ç€è¿›å…¥ç¬¬äºŒä¸ªå­å±‚ï¼Œè¿™ä¸ªå­å±‚ä¸­å¸¸è§„çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œqæ˜¯è¾“å…¥x;k,væ˜¯ç¼–ç å±‚è¾“å‡ºmemoryï¼ŒåŒæ ·ä¹Ÿä¼ å…¥source_maskï¼Œä½†æ˜¯è¿›è¡Œæºæ•°æ®é®æ©çš„åŸå› å¹¶éæ˜¯æŠ‘åˆ¶ä¿¡æ¯æ³„éœ²ï¼Œè€Œæ˜¯é®è”½æ‰å¯¹ç»“æœæ²¡æœ‰æ„ä¹‰çš„paddingã€‚
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))

        # æœ€åä¸€ä¸ªå­å±‚å°±æ˜¯å‰é¦ˆå…¨è¿æ¥å­å±‚ï¼Œç»è¿‡å®ƒçš„å¤„ç†åå°±å¯ä»¥è¿”å›ç»“æœï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„è§£ç å™¨ç»“æ„
        return self.sublayer[2](x, self.feed_forward)
```

## æ¨¡å‹è¾“å‡º

è¾“å‡ºéƒ¨åˆ†å°±å¾ˆç®€å•äº†ï¼Œæ¯ä¸ªæ—¶é—´æ­¥éƒ½è¿‡ä¸€ä¸ª çº¿æ€§å±‚ + softmax å±‚

çº¿æ€§å±‚çš„ä½œç”¨ï¼šé€šè¿‡å¯¹ä¸Šä¸€æ­¥çš„çº¿æ€§å˜åŒ–å¾—åˆ°æŒ‡å®šç»´åº¦çš„è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯è½¬æ¢ç»´åº¦çš„ä½œç”¨ã€‚è½¬æ¢åçš„ç»´åº¦å¯¹åº”ç€è¾“å‡ºç±»åˆ«çš„ä¸ªæ•°ï¼Œå¦‚æœæ˜¯ç¿»è¯‘ä»»åŠ¡ï¼Œé‚£å°±å¯¹åº”çš„æ˜¯æ–‡å­—å­—å…¸çš„å¤§å°ã€‚

```python
# å°†çº¿æ€§å±‚å’Œsoftmaxè®¡ç®—å±‚ä¸€èµ·å®ç°ï¼Œå› ä¸ºäºŒè€…çš„å…±åŒç›®æ ‡æ˜¯ç”Ÿæˆæœ€åçš„ç»“æ„
# å› æ­¤æŠŠç±»çš„åå­—å«åšGeneratorï¼Œç”Ÿæˆå™¨ç±»
class Generator(nn.Module):
    "Define standard linear + softmax generation step."

    def __init__(self, d_model, vocab):
        # åˆå§‹åŒ–å‡½æ•°çš„è¾“å…¥å‚æ•°æœ‰ä¸¤ä¸ªï¼Œd_modelä»£è¡¨è¯åµŒå…¥ç»´åº¦ï¼Œvocab.sizeä»£è¡¨è¯è¡¨å¤§å°
        super(Generator, self).__init__()
        # é¦–å…ˆå°±æ˜¯ä½¿ç”¨nnä¸­çš„é¢„å®šä¹‰çº¿æ€§å±‚è¿›è¡Œå®ä¾‹åŒ–ï¼Œå¾—åˆ°ä¸€ä¸ªå¯¹è±¡self.projç­‰å¾…ä½¿ç”¨
        # è¿™ä¸ªçº¿æ€§å±‚çš„å‚æ•°æœ‰ä¸¤ä¸ªï¼Œå°±æ˜¯åˆå§‹åŒ–å‡½æ•°ä¼ è¿›æ¥çš„ä¸¤ä¸ªå‚æ•°ï¼šd_modelï¼Œvocab_size
        self.proj = nn.Linear(d_model, vocab)

    def forward(self, x):
        # å‰å‘é€»è¾‘å‡½æ•°ä¸­è¾“å…¥æ˜¯ä¸Šä¸€å±‚çš„è¾“å‡ºå¼ é‡x,åœ¨å‡½æ•°ä¸­ï¼Œé¦–å…ˆä½¿ç”¨ä¸Šä¸€æ­¥å¾—åˆ°çš„self.projå¯¹xè¿›è¡Œçº¿æ€§å˜åŒ–,ç„¶åä½¿ç”¨Fä¸­å·²ç»å®ç°çš„log_softmaxè¿›è¡Œsoftmaxå¤„ç†ã€‚
        return F.log_softmax(self.proj(x), dim=-1)
```

## æ¨¡å‹æ„å»º

ç¼–ç å™¨å’Œå’Œè§£ç å™¨çš„å­å±‚é‡Œé¢éƒ½æœ‰å±‚æ ‡å‡†åŒ–ï¼ˆlayer-normalizationï¼‰ã€‚å‡è®¾ä¸€ä¸ª Transformer æ˜¯ç”± 2 å±‚ç¼–ç å™¨å’Œä¸¤å±‚è§£ç å™¨ç»„æˆçš„ï¼Œå°†å…¨éƒ¨å†…éƒ¨ç»†èŠ‚å±•ç¤ºèµ·æ¥å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

<img src="output_img/transformer_2.png" alt="png" style="zoom:38%;" />

```python
# Model Architecture
# ä½¿ç”¨EncoderDecoderç±»æ¥å®ç°ç¼–ç å™¨-è§£ç å™¨ç»“æ„
class EncoderDecoder(nn.Module):
    """
    A standard Encoder-Decoder architecture.
    Base for this and many other models.
    """

    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        # åˆå§‹åŒ–å‡½æ•°ä¸­æœ‰5ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯ç¼–ç å™¨å¯¹è±¡ï¼Œè§£ç å™¨å¯¹è±¡,æºæ•°æ®åµŒå…¥å‡½æ•°ï¼Œç›®æ ‡æ•°æ®åµŒå…¥å‡½æ•°ï¼Œä»¥åŠè¾“å‡ºéƒ¨åˆ†çš„ç±»åˆ«ç”Ÿæˆå™¨å¯¹è±¡.
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        # input embedding module(input embedding + positional encode)
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed    # ouput embedding module
        self.generator = generator    # output generation module

    def forward(self, src, tgt, src_mask, tgt_mask):
        "Take in and process masked src and target sequences."
        # åœ¨forwardå‡½æ•°ä¸­ï¼Œæœ‰å››ä¸ªå‚æ•°ï¼Œsourceä»£è¡¨æºæ•°æ®ï¼Œtargetä»£è¡¨ç›®æ ‡æ•°æ®,source_maskå’Œtarget_maskä»£è¡¨å¯¹åº”çš„æ©ç å¼ é‡,åœ¨å‡½æ•°ä¸­ï¼Œå°†source source_maskä¼ å…¥ç¼–ç å‡½æ•°ï¼Œå¾—åˆ°ç»“æœåä¸source_mask target å’Œtarget_maskä¸€åŒä¼ ç»™è§£ç å‡½æ•°
        memory = self.encode(src, src_mask)
        res = self.decode(memory, src_mask, tgt, tgt_mask)
        return res

    def encode(self, src, src_mask):
        # ç¼–ç å‡½æ•°ï¼Œä»¥sourceå’Œsource_maskä¸ºå‚æ•°,ä½¿ç”¨src_embedå¯¹sourceåšå¤„ç†ï¼Œç„¶åå’Œsource_maskä¸€èµ·ä¼ ç»™self.encoder
        src_embedds = self.src_embed(src)  # src_embedds.shape=[30,10,512]
        return self.encoder(src_embedds, src_mask)  # src_mask.shape=[30,1,10]

    def decode(self, memory, src_mask, tgt, tgt_mask):
        # è§£ç å‡½æ•°ï¼Œä»¥memoryå³ç¼–ç å™¨çš„è¾“å‡ºï¼Œsource_mask target target_maskä¸ºå‚æ•°,ä½¿ç”¨tgt_embedå¯¹targetåšå¤„ç†ï¼Œç„¶åå’Œsource_mask,target_mask,memoryä¸€èµ·ä¼ ç»™self.decoder
        target_embedds = self.tgt_embed(tgt)
        return self.decoder(target_embedds, memory, src_mask, tgt_mask)


# Full Model
def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):
    """
    æ„å»ºæ¨¡å‹
    params:
        src_vocab:
        tgt_vocab:
        N: ç¼–ç å™¨å’Œè§£ç å™¨å †å åŸºç¡€æ¨¡å—çš„ä¸ªæ•°
        d_model: æ¨¡å‹ä¸­embeddingçš„sizeï¼Œé»˜è®¤512
        d_ff: FeedForward Layerå±‚ä¸­embeddingçš„sizeï¼Œé»˜è®¤2048
        h: MultiHeadAttentionä¸­å¤šå¤´çš„ä¸ªæ•°ï¼Œå¿…é¡»è¢«d_modelæ•´é™¤
        dropout:
    """
    c = copy.deepcopy
    attn = MultiHeadedAttention(h, d_model, dropout)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),
        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab))

    # This was important from their code.
    # Initialize parameters with Glorot / fan_avg.
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    return model
```

## å®æˆ˜æ¡ˆä¾‹

ä¸‹é¢æˆ‘ä»¬ç”¨ä¸€ä¸ªäººé€ çš„ç©å…·çº§çš„å°ä»»åŠ¡ï¼Œæ¥å®æˆ˜ä½“éªŒä¸‹ Transformer çš„è®­ç»ƒï¼ŒåŠ æ·±æˆ‘ä»¬çš„ç†è§£ï¼Œå¹¶ä¸”éªŒè¯æˆ‘ä»¬ä¸Šé¢æ‰€è¿°ä»£ç æ˜¯å¦ workã€‚

ä»»åŠ¡æè¿°ï¼šé’ˆå¯¹æ•°å­—åºåˆ—è¿›è¡Œå­¦ä¹ ï¼Œå­¦ä¹ çš„æœ€ç»ˆç›®æ ‡æ˜¯ä½¿æ¨¡å‹å­¦ä¼šè¾“å‡ºä¸è¾“å…¥çš„åºåˆ—åˆ é™¤ç¬¬ä¸€ä¸ªå­—ç¬¦ä¹‹åçš„ç›¸åŒçš„åºåˆ—ï¼Œå¦‚è¾“å…¥[1,2,3,4,5]ï¼Œæˆ‘ä»¬å°è¯•è®©æ¨¡å‹å­¦ä¼šè¾“å‡º[2,3,4,5]ã€‚

æ˜¾ç„¶è¿™å¯¹æ¨¡å‹æ¥è¯´å¹¶ä¸éš¾ï¼Œåº”è¯¥ç®€å•çš„è‹¥å¹²æ¬¡è¿­ä»£å°±èƒ½å­¦ä¼šã€‚

ä»£ç å®ç°çš„åŸºæœ¬çš„æ­¥éª¤æ˜¯ï¼š

ç¬¬ä¸€æ­¥ï¼šæ„å»ºå¹¶ç”Ÿæˆäººå·¥æ•°æ®é›†

ç¬¬äºŒæ­¥ï¼šæ„å»º Transformer æ¨¡å‹åŠç›¸å…³å‡†å¤‡å·¥ä½œ

ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œæ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°

ç¬¬å››æ­¥ï¼šä½¿ç”¨æ¨¡å‹è¿›è¡Œè´ªå©ªè§£ç 

è®­ç»ƒçš„å¤§è‡´æµç¨‹å¦‚ä¸‹ï¼š

### æ‰¹å¤„ç†å’Œæ©ç 

```python
class Batch:
    "Object for holding a batch of data with mask during training."

    def __init__(self, src, trg=None, pad=0):
        # src,trg [30,10]
        self.src = src
        # src_mask.shape=[30,1,10]
        self.src_mask = (src != pad).unsqueeze(-2)
        if trg is not None:
            self.trg = trg[:, :-1]  # [30,9]
            self.trg_y = trg[:, 1:]  # [30,9]
            self.trg_mask = \
                self.make_std_mask(self.trg, pad)
            self.ntokens = (self.trg_y != pad).data.sum()

    @staticmethod
    def make_std_mask(tgt, pad):
        # tgt.shape [30,9]
        "Create a mask to hide padding and future words."
        tgt_mask = (tgt != pad).unsqueeze(-2)  # [30,1,9]
        tgt_mask = tgt_mask & subsequent_mask(
            tgt.size(-1)).type_as(tgt_mask.data)  # subsequent_mask.shape [1,9,9]
        # tgt_mask=[30,9,9]
        return tgt_mask
```

### Training Loop

```python
def run_epoch(data_iter, model, loss_compute):
    "Standard Training and Logging Function"
    start = time.time()
    total_tokens = 0
    total_loss = 0
    tokens = 0
    # batch.src.shape=[30,10] batch.trg.shape=[30,9]
    for i, batch in enumerate(data_iter):
        out = model.forward(batch.src, batch.trg,
                            batch.src_mask, batch.trg_mask)
        loss = loss_compute(out, batch.trg_y, batch.ntokens)
        total_loss += loss
        total_tokens += batch.ntokens
        tokens += batch.ntokens
        if i % 50 == 1:
            elapsed = time.time() - start
            print("Epoch Step: %d Loss: %f Tokens per Sec: %f" %
                  (i, loss / batch.ntokens, tokens / elapsed))
            start = time.time()
            tokens = 0
    return total_loss / total_tokens
```

### Optimizer

æˆ‘ä»¬ä½¿ç”¨ Adam ä¼˜åŒ–å™¨[(cite)](https://arxiv.org/abs/1412.6980)ï¼Œå…¶ä¸­ $\beta_1=0.9$, $\beta_2=0.98$å¹¶ä¸”$\epsilon=10^{-9}$ã€‚æˆ‘ä»¬æ ¹æ®ä»¥ä¸‹å…¬å¼åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ”¹å˜å­¦ä¹ ç‡ï¼š

$$
lrate = d_{\text{model}}^{-0.5} \cdot
  \min({step\_num}^{-0.5},
    {step\_num} \cdot {warmup\_steps}^{-1.5})
$$

è¿™å¯¹åº”äºåœ¨ç¬¬ä¸€æ¬¡$warmup\_steps$æ­¥ä¸­çº¿æ€§åœ°å¢åŠ å­¦ä¹ é€Ÿç‡ï¼Œå¹¶ä¸”éšåå°†å…¶ä¸æ­¥æ•°çš„å¹³æ–¹æ ¹æˆæ¯”ä¾‹åœ°å‡å°ã€‚æˆ‘ä»¬ä½¿ç”¨$warmup\_steps=4000$ã€‚

```python
class NoamOpt:
    "Optim wrapper that implements rate."

    def __init__(self, model_size, factor, warmup, optimizer):
        self.optimizer = optimizer
        self._step = 0
        self.warmup = warmup
        self.factor = factor
        self.model_size = model_size
        self._rate = 0

    def step(self):
        "Update parameters and rate"
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()

    def rate(self, step=None):
        "Implement `lrate` above"
        if step is None:
            step = self._step
        return self.factor * \
            (self.model_size ** (-0.5) *
             min(step ** (-0.5), step * self.warmup ** (-1.5)))


def get_std_opt(model):
    return NoamOpt(model.src_embed[0].d_model, 2, 4000,
                   torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))
```

> ä»¥ä¸‹æ˜¯æ­¤æ¨¡å‹é’ˆå¯¹ä¸åŒæ¨¡å‹å¤§å°å’Œä¼˜åŒ–è¶…å‚æ•°çš„æ›²çº¿ç¤ºä¾‹ã€‚

```python
# Three settings of the lrate hyperparameters.
opts = [NoamOpt(512, 1, 4000, None),
        NoamOpt(512, 1, 8000, None),
        NoamOpt(256, 1, 4000, None)]
plt.plot(np.arange(1, 20000), [[opt.rate(i)
                                for opt in opts] for i in range(1, 20000)])
plt.legend(["512:4000", "512:8000", "256:4000"])
```

![png](output_img/output_56_1.png)
â€‹

### æ­£åˆ™åŒ–

### æ ‡ç­¾å¹³æ»‘

$$
q_{i}= \begin{cases}1-\varepsilon & \text { if } i=y \\ \varepsilon /(K-1) & \text { otherwise }\end{cases}
$$

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„ label å¹³æ»‘çš„å€¼ä¸º$\epsilon_{ls}=0.1$ [(cite)](https://arxiv.org/abs/1512.00567)ã€‚è™½ç„¶å¯¹ label è¿›è¡Œå¹³æ»‘ä¼šè®©æ¨¡å‹å›°æƒ‘ï¼Œä½†æé«˜äº†å‡†ç¡®æ€§å’Œ BLEU å¾—åˆ†ã€‚

> æˆ‘ä»¬ä½¿ç”¨ KL div æŸå¤±å®ç°æ ‡ç­¾å¹³æ»‘ã€‚æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ one-hot ç‹¬çƒ­åˆ†å¸ƒï¼Œè€Œæ˜¯åˆ›å»ºäº†ä¸€ä¸ªåˆ†å¸ƒï¼Œè¯¥åˆ†å¸ƒè®¾å®šç›®æ ‡åˆ†å¸ƒä¸º 1-smoothingï¼Œå°†å‰©ä½™æ¦‚ç‡åˆ†é…ç»™è¯è¡¨ä¸­çš„å…¶ä»–å•è¯ã€‚

```python
class LabelSmoothing(nn.Module):
    "Implement label smoothing."

    def __init__(self, size, padding_idx, smoothing=0.0):
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(reduction='sum')
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None

    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2)) # if i!=y, smoothing/(size-2)
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) # if i=y, 1-smoothing
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        return self.criterion(x, true_dist.requires_grad_(False))
```

ä¸‹é¢æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼Œçœ‹çœ‹å¹³æ»‘åçš„çœŸå®æ¦‚ç‡åˆ†å¸ƒã€‚

```python
# Example of label smoothing.
crit = LabelSmoothing(5, 0, 0.4)
predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],
                             [0, 0.2, 0.7, 0.1, 0],
                             [0, 0.2, 0.7, 0.1, 0]])
v = crit(predict.log(),
         torch.LongTensor([2, 1, 0]))

# Show the target distributions expected by the system.
plt.imshow(crit.true_dist)
```

![png](output_img/output_60_2.png)
â€‹

```python
print(crit.true_dist)
```

    tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],
            [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],
            [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])

ç”±äºæ ‡ç­¾å¹³æ»‘çš„å­˜åœ¨ï¼Œå¦‚æœæ¨¡å‹å¯¹äºæŸä¸ªå•è¯ç‰¹åˆ«æœ‰ä¿¡å¿ƒï¼Œè¾“å‡ºç‰¹åˆ«å¤§çš„æ¦‚ç‡ï¼Œä¼šè¢«æƒ©ç½šã€‚å¦‚ä¸‹ä»£ç æ‰€ç¤ºï¼Œéšç€è¾“å…¥ x çš„å¢å¤§ï¼Œx/d ä¼šè¶Šæ¥è¶Šå¤§ï¼Œ1/d ä¼šè¶Šæ¥è¶Šå°ï¼Œä½†æ˜¯ loss å¹¶ä¸æ˜¯ä¸€ç›´é™ä½çš„ã€‚

```python
crit = LabelSmoothing(5, 0, 0.1)


def loss(x):
    d = x + 3 * 1
    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])
    # print(predict)
    return crit(predict.log(),
                torch.LongTensor([1])).item()


y = [loss(x) for x in range(1, 100)]
x = np.arange(1, 100)
plt.plot(x, y)
```

![png](output_img/output_63_1.png)
â€‹

#### æ€è€ƒï¼š

1. ä¸ºä»€ä¹ˆä½¿ç”¨KLDivLossè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚ï¼Ÿ

é¦–å…ˆKLDivLossï¼ŒKLæ•£åº¦ç”¨äºè¿ç»­åˆ†å¸ƒçš„è·ç¦»åº¦é‡ï¼›å¹¶ä¸”å¯¹ç¦»æ•£é‡‡ç”¨çš„è¿ç»­è¾“å‡ºç©ºé—´åˆ†å¸ƒè¿›è¡Œå›å½’é€šå¸¸å¾ˆæœ‰ç”¨ã€‚

$l(x,y)=L=\{l_1,...,l_N\},l_n=y_n(logy_n-x_n)$

> æ³¨æ„:å¦‚æœ$y_i$ä¸º0,$l_i$ä¹Ÿä¸º0ã€‚å¯ä»¥ç†è§£ä¸ºä¸å½±å“lossã€‚

2. ä¸ºä»€ä¹ˆæ˜¯$size-2$è€Œä¸æ˜¯$size-1$ï¼Ÿ

å› ä¸ºåœ¨vocabularyä¸­ index=0 æ˜¯ 0å¡«å……ç¬¦å·pad ï¼Œä¸“é—¨ä¸ºäº†å¡«å……é•¿åº¦æ²¡æœ‰è¾¾åˆ°max_lençš„å¥å­ã€‚è€Œç”±äºend of sentenceç¬¦å·çš„å­˜åœ¨ï¼Œ0å¡«å……ç¬¦å·çš„é¢„æµ‹å’Œè®¡ç®—æŸå¤±æ˜¯æ²¡æœ‰æ„ä¹‰ã€‚

å¦‚æœè®©å…¶true_distçš„æ¯ä¸€ä¸ªtokenï¼ˆå³true_dist[i]ï¼‰å¯¹åº”çš„ç¬¬0ä¸ªç´¢å¼•ï¼ˆindex=0ï¼‰çš„å€¼ä¸º0å³èƒ½å®ç°è¿™ä¸€ç›®çš„ï¼›è€Œtokené¢„æµ‹æ¦‚ç‡çš„ç¬¬0ä¸ªç´¢å¼•ä»£è¡¨çš„å°±æ˜¯å¡«å……ç¬¦å·çš„æ¦‚ç‡ã€‚å› æ­¤æˆ‘ä»¬å¸Œæœ›true_dist[i]ä¸­ç¬¬0ä¸ªç´¢å¼•çš„å€¼å§‹ç»ˆä¸º0ï¼Œè¿™æ ·ç”±äº$l_0$å§‹ç»ˆä¸º0ï¼Œpadä¸å‚ä¸lossè®¡ç®—ï¼Œæ‰€ä»¥true_dist[:,0]=0ã€‚

æ‰€ä»¥$size-1$å†$-1$ã€‚ç›¸å½“äºåªæœ‰ $size-1$ ä¸ªæ¦‚ç‡éœ€è¦è¢«å¹³æ»‘ï¼Œè€Œ $i=y$ æ˜¯ç”¨ $1-smoothing$ å¡«å……ï¼Œ$i \neq y$ ç”¨ $smoothing/(size-2)$

3. maskçš„ä½œç”¨ï¼šå°±æ˜¯å°†targetä¸­æ‰€æœ‰é¢„æµ‹ä¸º0çš„tokençš„ä½ç½®æå–å‡ºï¼Œå°†true_distå¯¹åº”ä½ç½®å…¨éƒ¨å¡«å……0ï¼Œä»¥æ­¤ä¸å½±å“lossã€‚

æ¯”å¦‚target=[25,21,0]ï¼Œé‚£ä¹ˆtrue_dist[2]å…¨ä¸º0ï¼Œ[0,....,0]

## å®ä¾‹

### åˆæˆæ•°æ®

```python
def data_gen(V, batch, nbatches):
    "Generate random data for a src-tgt copy task."
    for i in range(nbatches):
        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))
        data[:, 0] = 1
        src = data.long().requires_grad_(False)
        tgt = data.long().requires_grad_(False)
        yield Batch(src, tgt, 0)
```

### æŸå¤±å‡½æ•°è®¡ç®—

```python
class SimpleLossCompute:
    "A simple loss compute and train function."

    def __init__(self, generator, criterion, opt=None):
        self.generator = generator
        self.criterion = criterion
        self.opt = opt

    def __call__(self, x, y, norm):
        x = self.generator(x)
        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),
                              y.contiguous().view(-1)) / norm
        loss.backward()
        if self.opt is not None:
            self.opt.step()
            self.opt.optimizer.zero_grad()
        return loss.item() * norm
```

### è´ªå©ªè§£ç 

```python
# Train the simple copy task.
V = 11
criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)
model = make_model(V, V, N=2)
model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,
        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))

for epoch in range(10):
    model.train()
    run_epoch(data_gen(V, 30, 20), model,
              SimpleLossCompute(model.generator, criterion, model_opt))
    model.eval()
    print(run_epoch(data_gen(V, 30, 5), model,
                    SimpleLossCompute(model.generator, criterion, None)))
```

    Epoch Step: 1 Loss: 2.964034 Tokens per Sec: 387.010254
    Epoch Step: 1 Loss: 1.944704 Tokens per Sec: 591.981201
    tensor(1.9819)
    Epoch Step: 1 Loss: 2.119860 Tokens per Sec: 405.909332
    Epoch Step: 1 Loss: 1.777609 Tokens per Sec: 599.199463
    tensor(1.7378)
    Epoch Step: 1 Loss: 1.928171 Tokens per Sec: 407.761536
    Epoch Step: 1 Loss: 1.672072 Tokens per Sec: 614.896240
    tensor(1.6674)
    Epoch Step: 1 Loss: 1.980943 Tokens per Sec: 414.652527
    Epoch Step: 1 Loss: 1.387953 Tokens per Sec: 598.535339
    tensor(1.4324)
    Epoch Step: 1 Loss: 1.557681 Tokens per Sec: 396.967346
    Epoch Step: 1 Loss: 1.232811 Tokens per Sec: 616.299866
    tensor(1.1763)
    Epoch Step: 1 Loss: 1.439810 Tokens per Sec: 401.095856
    Epoch Step: 1 Loss: 0.728257 Tokens per Sec: 603.893311
    tensor(0.7543)
    Epoch Step: 1 Loss: 1.051911 Tokens per Sec: 413.381775
    Epoch Step: 1 Loss: 0.540547 Tokens per Sec: 625.584290
    tensor(0.5310)
    Epoch Step: 1 Loss: 0.941762 Tokens per Sec: 417.215820
    Epoch Step: 1 Loss: 0.402128 Tokens per Sec: 632.918701
    tensor(0.4327)
    Epoch Step: 1 Loss: 0.792281 Tokens per Sec: 406.225067
    Epoch Step: 1 Loss: 0.345573 Tokens per Sec: 627.766785
    tensor(0.3450)
    Epoch Step: 1 Loss: 0.470235 Tokens per Sec: 419.160828
    Epoch Step: 1 Loss: 0.455055 Tokens per Sec: 631.438232
    tensor(0.3832)

```python
def greedy_decode(model, src, src_mask, max_len, start_symbol):
    memory = model.encode(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)
    for i in range(max_len-1):
        out = model.decode(memory, src_mask,
                           ys,
                           subsequent_mask(ys.size(1)).type_as(src.data))
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]
        ys = torch.cat([ys,
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
    return ys


model.eval()
src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])
src_mask = torch.ones(1, 1, 10)
print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))
```

    tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])
